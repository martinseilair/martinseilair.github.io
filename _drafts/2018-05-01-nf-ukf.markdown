---
layout: post
title:  "Nonlinear filtering: Unscented Kalman filter"
date:   2018-10-12 18:04:07 +0900
categories: jekyll update
excerpt_separator: <!--more-->
---
Intro
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/particle_filter.js"></script>
<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/race_car.js"></script>
<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/race_track.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/util.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/plot.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/scene.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>


<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />


<link rel="stylesheet" type="text/css" href="{{ base.url | prepend: site.url }}/assets/css/nonlinear_filter/style.css">






<script type="text/javascript">


// mit keys oder button steuerbar
// strips ein und ausblendbar
// weights ein und ausblendbar
// update resample predict manuell oder langsam automatisch (weiter button)
// update resample predict button (hier macht input keinen sinn, außer man hat 3 button für predict)
// mit maus car position festlegen (geringster abstand)


// herangehensweise

// 1. auto fährt 
// 2. Vorstellung der system und beobachtungsfunktion (plot)
// 3. mit maus car position festlegen, entsprechende verteilung innen und außen anzeigen
// 3a. Bayes filter approximierung außen posterior innen beobachtung (update prediction weiter)
// 4. standbild: particle anzeigen
// 5. standbild: update step (5 sek vorher 5 sek nachher) (prob strip innen anzeigen)
// 6. standbild: resampling (5 sek vorher 5 sek nachher)
// 7. standbild: predict (5 sek vorher 5 sek nachher)
// 8. update resample predict manuell (weiter button)
// 9. update resample predict automatisch (geschwindigkeit einstellbar) (steuerung über pfeiltasten)
// 10. zwei trees

	// SITE NOT LOADED!!!

	// input modes
	// 0: Automatisch langsam; sequential
	// 1: Set input per  A = backward, S = no movement, D = forward; one step
	// 2: Set input per  A = backward, S = no movement, D = forward; sequential
	// 3: Mouse exploring
	// 4: No input

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>


## Unscented Kalman filter



The unscented Kalman filter is another approximation of the Bayes filter with Gaussian belief states. 






When we are doing the **prediction step**

$$ p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t} $$

with 

$$  \begin{align}
p(x_{t+1}|x_{t}, u_{t})  &= \mathcal{N}(x_{t+1}|f(x_t, u_t), Q_t) \\
p(y_t|x_t) &=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align}$$


with a Gaussian belief distribution xxx we will receive a non-Gaussian function as a result. The first idea of the unscented Kalman filter is, that we want a Gaussian posterior in the end. In other words we are only intrested in the mean and variance of the true posterior. We try to approximate the real posterior with a Gaussian. In this scenario, we have two problems:

1. The integral is intractable, because of the non-linear sytem dynamic \\(f(x_t,u_t)\\). 
2. It is intractable to calculate the mean and covariance of the resulting posterior

How can we solve this problem?

One solution is sampling. We could simply obtain the empricial distribution by sampling from our Gaussian. Then we replace our belief with the emprical distribution in the predict step. The integral will become a sum. Please be aware, that the resulting function will still be a continuous function. Nonetheless, we know that this function is a mixture of gaussians, where each component is a Gaussian with mean \\(f(\xi^i_t,u_t)\\) and variance \\(Q_t\\). When we are forming our empirical distrobution our samples are weighted by \\(w^i  = \frac{1}{N}\\). 

By assuming an arbitray weighting of the samples the resulting mean is

$$ \hat{\mu} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}[x_{t+1}] $$

$$ \hat{\mu} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}dx_{t+1} $$

$$ \hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i p(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}dx_{t+1} $$

$$ \hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1} $$

$$ \hat{\mu} = \sum_{i=1}^N w^i\int_{x_{t+1}}  w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1} $$

$$ \hat{\mu} = \sum_{i=1}^N w^i f(\xi^i_t, u_t) $$



and the variance

$$ \hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[(x_{t+1}-\hat{\mu})(x_{t+1}-\hat{\mu})^T\right] $$

$$ \hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T - x_{t+1}\hat{\mu}^T - \hat{\mu}x_{t+1}^T + \hat{\mu}\hat{\mu}^T\right] $$

$$ \hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}\hat{\mu}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}x_{t+1}^T\right] + \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}\hat{\mu}^T\right] $$

$$ \hat{\Sigma} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T - \hat{\mu}\hat{\mu}^T + \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^ip(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T $$


$$ \hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T $$

$$ \hat{\Sigma} = Q_t + \sum_{i=1}^N w^i f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T - \hat{\mu}\hat{\mu}^T $$



Because of this we can find the true mean and variance of the continuous posterior. In the limit with inifinitley many samples this procedure will procduce the correct mean and variance.

Great, we have a nice method! But there is another problem. Sampling can be very inefficient. Maybe there is a smarter way.

The second big idea of the unscented Kalman filter is, that we are replacing the our belief distribution with a surrogate distribution, which is distributoin containing weighted samples.

We choose this surrogate distribution in such a way, that it has the same mean and variance as the Gaussian belief distribution. This distribution is not unique.

For our samples we use

$$ \xi_t^0 = \mu_t $$

$$ \xi_t^i = \mu_t + \sqrt{n+\lambda}L i=1,..,n $$

$$ \xi_t^i = \mu_t - \sqrt{n+\lambda}L i=n+1, ..., 2n$$

with weights

$$ w_c^0 = \frac{\lambda}{n + \lambda} $$

$$ w_m^0 = \frac{\lambda}{n + \lambda} + (1-\alpha^2+\beta)$$

$$ w_m^i =   \frac{\lambda}{2(n + \lambda)} $$

Why do they choose another Wc0?????????

$$ D_{KL}(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})||\mathcal{N}(x_{t+1}|\hat{\mu},\hat{\Sigma})) $$

and the **update step**

$$ p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .$$














## Unscented Kalman filter

The unscented Kalman filter was born by intuition. It seems that it is not derived from the general Bayes filter but from the Kalman filter

Bayes filter -> Linear Gaussian models -> Kalman filter

Kalman filter -> Non-linear models -> Unscented Kalman filter

In this post I will try to derive the UKF from first principles. There is one catch, to obtain the true UKF you have to do some very nasty approximations, will incorporate in the derivation afterwards.

The UKF is based around the unscented transform. If you want to transform a Gaussian distribution

$$ p(x) = \mathcal{N}(y|\mu, \Sigma) $$

 thorugh a nonlinear function, it will give you an estimate of the resulting mean and covariance.

For developing the equations I would state the UT in another form. The nonlinear equations is in our case a Gaussian distribution with constant variance and nonlinear mean:


$$ p(y|x) = \mathcal{N}(y|f(x),B) $$


. Now, the big difference is that we dont want to estimate the distribution of the output \\(p(x)\\), but the distribution over the joint probability.

The first step is to replace the Gaussian distribution \\( p(x) = \mathcal{N}(y\|\mu, \Sigma) \\) with a surrogate distribution. This surrogate distribution is a weighted dirac distribution.

For our samples we use

$$ \chi^0 = \mu $$

$$ \chi^i = \mu + a_iL_i  i=1,..,n $$

$$ \chi^i = \mu - a_iL_{i-n}  i=n+1, ..., 2n$$


With the property

$$ \mu= \sum_{i} w^i \chi^i$$

$$ \Sigma = \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T$$

where 

$$ \sum_{i} w^i = 1 $$

$$ a_i = a_{i+n} i=1,..,n$$ 

$$ w_i = w_{i+n} i=1,..,n$$ 


$$
\begin{align}
 \Sigma &= \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
&= w^0 (\chi^0-\mu)(\chi^0-\mu)^T + \sum_{i=1}^{n} w^i (\chi^i-\mu)(\chi^i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
 &= w^0 (\mu-\mu)(\mu-\mu)^T + \sum_{i=1}^{n} w^i (\mu + a_iL_i-\mu)(\mu + a_iL_i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\mu - a_iL_{i-n}-\mu)(\mu - a_iL_{i-n}-\mu)^T \\
&= \sum_{i=1}^{n} w^i (a_iL_i)(a_iL_i)^T + \sum_{i=n+1}^{2n} w^i (- a_iL_{i-n})(- a_iL_{i-n})^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=n+1}^{2n} w^i a_i^2 L_{i-n}L_{i-n}^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^{i+n} a_{i+n}^2 L_{i}L_{i}^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^i a_i^2 L_{i}L_{i}^T \\
&= \sum_{i=1}^{n} 2w^i a_i^2 L_iL_i^T \\
&\stackrel{!}{=} \sum_{i=1}^{n} L_iL_i^T = LL^T = \Sigma \\
\end{align}
$$

It follows that \\(w^i = \frac{1}{2a_i^2} \\) has to hold for \\(i=1,..,n\\).

From \\(\sum_{i} w^i = 1\\) follows that

$$ w^0  = 1 - \sum_{i=1}^{n}\frac{1}{2a_i^2}$$


Given the deviations \\(a_i\\) we can uniquely determine the corresponding weights.




Because of the symmetry of the points and the fact that the weights sum to 1, the first property is always fulfilled. You have to choose the weights based on \\(\{a_i\}_{1:2n}\\) to fulfill the second property.


In total our surrogate distribution is defined as:

$$ \hat{p}(x) = \sum_{i} w^i \delta_{\chi^i}(x). $$


Now we can write down the joint probability based on \\(\hat{p}(x)\\) and  \\(p(y\|x)\\)

$$ \hat{p}(x,y) = p(y|x)\hat{p}(x) = \mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x). $$

This joint probability will look like slices of the \\(p(y\|x) \\).

Now comes the crucial part. Now we are calculating the mean and covariance of this surrogate joint probability. lets start with the mean

$$
\begin{align}
\hat{\mu} &= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy \\ 
&= \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy \\ 
&= \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy \\ 
&= \sum_{i} w^i \int\limits_{x}\mathcal{N}(y|f(\chi^i),B)\begin{pmatrix}
    \chi^i \\
    y \\
    \end{pmatrix} \,dy \\ 
&= \sum_{i} w^i \begin{pmatrix}
    \chi^i \\
    f(\chi^i) \\
    \end{pmatrix} =  \begin{pmatrix}
    \sum_{i} w^i \chi^i \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}\\
&=  \begin{pmatrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}. 
\end{align}
$$

Lets calculate the covariance

$$
\begin{align}
\hat{\Sigma} &= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x-\hat{\mu}_x  \\
    y-\hat{\mu}_y \\
    \end{pmatrix}\begin{pmatrix}
    x-\mu  \\
    y-\hat{\mu} \\
    \end{pmatrix}^T \,dx\,dy \\ 
&= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy \\ 
&= \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy \\ 
&= \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy \\ 
&= \sum_{i} w^i \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T & (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dy \\ 
&= \sum_{i} w^i \begin{pmatrix}
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T \,dy& \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T\,dy\\
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T\,dy & \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T\,dy \\
    \end{pmatrix}  \\ 

&= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& (\chi^i-\hat{\mu}_x)\left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right]^T\\
    \left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right](\chi^i-\hat{\mu}_x)^T &  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& (\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    (f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&=  \begin{pmatrix}
    \sum_{i} w^i(\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&=  \begin{pmatrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
\end{align}
$$    


In total we get for the estimated surrogate joint probability 

$$ \hat{p}(x,y) = \mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{matrix},\begin{matrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{matrix}\right) $$


This is the result of this variant of the UT transform. Lets look how this goes into the Bayes filter and with the **prediction step**:

$$ p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}. $$

This is equation is nothing else but marginalization

$$ p(y) = \int_x p(y|x)p(x)dx  $$

we can also write it as 

$$ p(y) = \int_x p(x,y)dx  $$

and have identified our joint probability distribution, that we can approximate with our variant of the UT transform.

It is very important to notice, that in this case you dont have to calculate the whole joint Gaussian probability, because you are marginalizing afterwards.

$$ \int\limits_{x}\mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    a  \\
    b \\
    \end{matrix}, \begin{matrix}
    A & C \\
    C^T & B\\
    \end{matrix} \right) \,dx = \mathcal{N}(y|b,B) $$




Now lets look at the **update step**

$$ p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\,dx_t} .$$

We see that we have a joint probability in the numerator and a marginalization of the same joint probability in the denominator.


$$ p(y|x) = \frac{p(x,y)}{\int_{x}p(x,y)\,dx} .$$


We can calculate this analytically with our estimate of the surrogate function and obtain directly a Gaussian posterior.

We can calculate the joint probability for the Linear Gaussian case and then do a coefficient comparison. To obtain the corresponding filtering equations.

$$ \mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}\left(\begin{matrix}x_t \\y_t\end{matrix}\middle|\begin{matrix}\hat x_{t|t-1}\\C_t\hat x_{t|t-1} \end{matrix},\begin{matrix}P_{t|t-1} & P_{t|t-1}^TC_t^T\\C_tP_{t|t-1} & R_t + C_tP_{t|t-1}^TC_t^T\end{matrix}\right) .$$

and 

$$ \hat{p}(x,y) = \mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{matrix},\begin{matrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{matrix}\right) $$



To obtain the known equations of the UKF you are choosing specific values \\(a_i\\) and when you calculate the update function you are cheating by using the sigma points from the last time step instead from the current timestep. But you still take the covariance of the current timestep for the calculation.



<a href='https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm'></a>


<div id="rad_to_s" style="width:100px"></div>
<div id="div1"></div>
<div id="div2"></div>
<!-- <div id="system_dist_approx"  style="width: 600px; height: 600px;"></div> -->
<!--<div id="output_dist_approx"  style="width: 600px; height: 600px;"></div>-->









