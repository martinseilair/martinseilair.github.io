---
layout: post
title:  "Optimal control in Markov decision processes"
date:   2018-10-12 18:04:07 +0900
categories: jekyll update
comments: true
excerpt_separator: <!--more-->
---

<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>


What is a Markov Decision process?

In the standard Reinforcement Learning setting an agent is operating in an environment defined by a [Markov decision process](https://en.wikipedia.org/wiki/Markov_decision_process) (MDP), which is defined by a tuple \\((\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \rho_0)\\). The current state of the environment is defined by \\(s_t \in \mathcal{S}\\). At every time step the agent takes an action \\(a_t \in \mathcal{A}\\). Given the state \\(s_t\\) and action \\(a_t\\) the next state \\(s_{t+1}\\) is distributed according to the transition dynamics \\(s_{t+1} \sim \mathcal{P}(s_{t+1}\|s_t,a_t)\\). The initial state \\(s_0\\) is distributed by \\(s_0 \sim \rho_0(s_0)\\). The reward \\(r_{t} \sim \mathcal{R}(r_{t}\|s_{t}, a_t)\\) evaluates the current state and action. 

The state \\(s_t\\) of the MDP is by definition a sufficient statistic of the environment. Therefore, it is sufficient that the policy \\(\pi\\), which defines the behavior of the agent, solely depends on the current state \\(s_t\\).

The objective of the agent is to maximize the expected return 

$$ J = \mathbb{E}_{p(\tau)}\left[\sum_{t=0}^T\gamma^{t}r_t\right] $$

with distribution 

$$p(\tau) = \rho_0(s_0)\prod_{t=0}^T \pi(a_t|s_t)\mathcal{P}(s_{t+1}|s_t, a_t) $$

over trajectories \\(\tau = (s_0, a_0,...,a_{T-1},s_T)\\) and discount factor \\(\gamma \in [0,1]\\).


What is the goal of optimal control?

https://en.wikipedia.org/wiki/Expected_utility_hypothesis

https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem

[Optimal control](https://en.wikipedia.org/wiki/Optimal_control) tries to find a controller that is maximizing cost functional. 



