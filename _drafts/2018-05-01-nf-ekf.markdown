---
layout: post
title:  "Nonlinear filtering: Extended Kalman filter"
date:   2018-10-12 18:04:07 +0900
categories: jekyll update
excerpt_separator: <!--more-->
---
Intro
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>

  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/particle_filter.js"></script>
<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/race_car.js"></script>
<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/race_track.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/util.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/plot.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/scene.js"></script>

<script src="{{ base.url | prepend: site.url }}/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>


<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />


<link rel="stylesheet" type="text/css" href="{{ base.url | prepend: site.url }}/assets/css/nonlinear_filter/style.css">




<script type="text/javascript">


// mit keys oder button steuerbar
// strips ein und ausblendbar
// weights ein und ausblendbar
// update resample predict manuell oder langsam automatisch (weiter button)
// update resample predict button (hier macht input keinen sinn, außer man hat 3 button für predict)
// mit maus car position festlegen (geringster abstand)


// herangehensweise

// 1. auto fährt 
// 2. Vorstellung der system und beobachtungsfunktion (plot)
// 3. mit maus car position festlegen, entsprechende verteilung innen und außen anzeigen
// 3a. Bayes filter approximierung außen posterior innen beobachtung (update prediction weiter)
// 4. standbild: particle anzeigen
// 5. standbild: update step (5 sek vorher 5 sek nachher) (prob strip innen anzeigen)
// 6. standbild: resampling (5 sek vorher 5 sek nachher)
// 7. standbild: predict (5 sek vorher 5 sek nachher)
// 8. update resample predict manuell (weiter button)
// 9. update resample predict automatisch (geschwindigkeit einstellbar) (steuerung über pfeiltasten)
// 10. zwei trees

	// SITE NOT LOADED!!!

	// input modes
	// 0: Automatisch langsam; sequential
	// 1: Set input per  A = backward, S = no movement, D = forward; one step
	// 2: Set input per  A = backward, S = no movement, D = forward; sequential
	// 3: Mouse exploring
	// 4: No input

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>


## Derivation

We will start the derivation directly from the recursive equations of the Bayes filter with the **prediction step**

$$ p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t} $$

and the **update step**

$$ p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .$$




$$
\begin{align}
 x_{t+1} &= f(x_t, u_t, w_t) \\
 y_t &= h(x_k, v_k)
 \end{align}
 $$



 What does the probability distribution of 

 $$ p(x_{t+1}|x_t,u_t) $$ 

 looks like?

 We can express our function \\(x_{t+1} = f(x_t,u_t,w_t)\\) as a probability distribution

 $$ p(x_{t+1}|x_t,u_t,v_t) = \delta(x_{t+1} - f(x_t, u_t, w_t)). $$

 We can express this also as 

 $$ \delta(x_{t+1} - f(x_t, u_t, w_t)) = \sum_p \frac{\delta(x_t - a_i)\delta(u_t - a_i)\delta(v_t - a_i)}{|\nabla g(a_i)|} ,$$

 where the sum is over all points that satisfy x_{t+1} = f(x_t, u_t, w_t).




 To calculate \\(p(x_{t+1}\|x_t,u_t)\\) we have to marginalize \\(v_t)\\ out:

  $$ p(x_{t+1}|x_t,u_t) = \int_{v_t}p(x_{t+1}|x_t,u_t,v_t)p(v_t) \,dv_t $$

 $$ p(x_{t+1}|x_t,u_t) = \int_{v_t}\delta(x_{t+1} - f(x_{t+1}, x_t, u_t, w_t))\,dv_t  $$

W





 with Gaussian process noise \\( w_t \sim \mathcal{N}(w_t\|0, Q_t) \\) and Gaussian observation noise \\( v_t \sim \mathcal{N}(v_t\|0, R_t) \\).

 First order Taylor expansion around the current state \\(x_t = x\\), current input \\(u_t=u\\) and zero process noise \\(w_t=0\\)

 $$ x_{t+1} \approx f(x_t, u_t, w_t)|_{x_t=x,u_t=u,w_t=0} + \nabla_{x_t} f(x_t, u_t, w_t)^T|_{x_t=x,u_t=u,w_t=0}(x_t - x)+ \nabla_{u_t} f(x_t, u_t, w_t)^T|_{x_t=x,u_t=u,w_t=0}(u_t - u) + \nabla_{w_t} f(x_t, u_t, w_t)^T|_{x_t=x,u_t=u,w_t=0}w_t$$

  $$ x_{t+1} \approx f(x, u, 0) + A_t(x_t - x)+ B_t(u_t - u) + L_tw_t$$

  $$ \hat{p}(x_{t+1}|x_t,u_t) = \mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) + A_t(x_t - x)+ B_t(u_t - u),L_t^TQ_tL_t\right)  $$

$$ \hat{p}(x_{t+1}|x_t,u_t) = \mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) - A_tx - B_tu + A_tx_t + B_tu_t,L_t^TQ_tL_t\right)  $$


   First order Taylor expansion around the current state \\(x_t = x\\) and zero measurement noise \\(v_t=0\\)

 $$ y_t \approx h(x_t, v_t)|_{x_t=x,v_t=0} + \nabla_{x_t} h(x_t, v_t)^T|_{x_t=x,v_t=0}(x_t - x) + \nabla_{v_t} h(x_t, v_t)^T|_{x_t=x,v_t=0}v_t$$

  $$ y_t \approx h(x, 0) + C_t(x_t - x) + M_tv_t$$

$$ \hat{p}(y_{t}|x_t) = \mathcal{N}\left(y_{t}\middle| h(x, 0) + C_t(x_t - x),M_t^TR_tM_t\right)  $$

$$ \hat{p}(y_{t}|x_t) = \mathcal{N}\left(y_{t}\middle| h(x, 0) - C_tx + C_tx_t ,M_t^TR_tM_t\right)  $$

$$  \begin{align}
p(x_t|y_{0:t},u_{0:t-1}) &:= \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t}) \\
p(x_t|y_{0:t-1},u_{0:t-1}) &:= \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) .
 \end{align}$$


**Prediction step**

$$ \mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) - A_tx - B_tu + A_tx_t + B_tu_t,L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.$$

**Update step**

$$ \mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(x, 0) - C_tx + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(x, 0) - C_tx + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}   $$ 



Let's try to simplify these equations!

### Prediction step

We will start with the prediction step

$$ \mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) - A_tx - B_tu + A_tx_t + B_tu_t,L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.$$

In order to find a closed form solution of this integral, we could simply plug in the corresponding expressions of the Gaussian distributions and solve the integral. Fortunately, Marc Toussaint already gathered the most important [Gaussian identities](https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf), which will lighten our workload a lot.  To find an expression for our prediction step we can simply use the *propagation* formula (Formula 37, Toussaint)

$$ \int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ). $$

By comparison with our expression, we see that

$$ \hat x_{t+1|t} = f(x_{t|t}, u_t, 0) -A_t \hat x_{t|t} - B_tu_t + A_t \hat x_{t|t} + B_tu_t = f(x_{t|t}, u_t, 0) $$


$$ P_{t+1|t} = L_t^TQ_tL_t + A_t P_{t|t} A_t^T  .$$

### Update step

We will start to simplify the update step 

$$ \mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(x, 0) - C_tx + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(x, 0) - C_tx + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}   $$ 


by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, Toussaint)

$$ \mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) .$$

Then again, this joint distribution can be rewritten as 

$$ \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D & F\\F^T & E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) .$$

We can combine the two previous equations to the following expression

$$ \mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .$$ 

By comparison with the numerator of our update step, we obtain

$$ \mathcal{N}\left(y_{t}\middle| h(x, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(x, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1},M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(x, 0) + C_t x_{t|t-1} -C_t\hat x_{t|t-1}),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}), $$ 

which simplifies to

$$ \mathcal{N}\left(y_{t}\middle| h(x, 0) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(x, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(x, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}). $$ 

At a first glance, this is not looking like a simplification at all. Conceptually, we only transformed 

$$ \frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}. $$


If we look closely at the final expression, we see that \\(p(y)\\) is canceling out. Therefore, the result is simply the remaining part

$$ \mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(x, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}). $$ 

If our reasoning is correct the denominator should be equal to \\(\mathcal{N}(y_{t}\|h(x, 0),M_t^TR_tM_t + C_tP_{t\|t-1}C_t^T)\\), which was canceled out. The denominator can be simplified with the *propagation* formula (Formula 37, Toussaint)

$$ \int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(x, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|h(x, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1}, M_t^TR_tM_t + C_tP_{t|t-1}C_t^T ) = \mathcal{N}(y_{t}|h(x, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T).$$

Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.

Let's summarize our results:

<div class="important_box" markdown="1">
<h1>Bayes filter in linear Gaussian state space models</h1>

The recursive formula for the Bayes filter in linear Gaussian state space models consists of the **prediction step**

$$ \begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} $$


and the **update step**


$$ \begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(x, 0)) \\ 
P_{t|t} &= P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}.  \end{align} $$


</div>
That's it! We derived the equations of the Bayes filter in linear Gaussian state space models, which is nothing else but the good old Kalman filter.
In the next section, we will split these equations up to finally obtain the formulation normally used for the Kalman filter.



<div class="important_box" markdown="1">
<h1>Equations of the Kalman filter</h1>

The recursive formula for the Kalman filter consists of the **prediction step**

$$ \begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} $$


and the **update step**

$$ \begin{align}
z_t &= y_{t}-h(x, 0)\\
S_t &= M_t^TR_tM_t + C_tP_{t|t-1}C_t^T\\
K_t &= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &= (I - K_tC_t)P_{t|t-1}.
\end{align} $$

</div>

Additive noise

$$ f(x_t, u_t, w_t) = \bar{f}(x_t, u_t) + w_t $$

$$ L_t = \nabla_{w_t} f(x_t, u_t, w_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{w_t} \bar{f}(x_t, u_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{w_t} w_t}_{I}|_{x_t=x,v_t=0} = I $$

$$ f(x_t, u_t, w_t) = \bar{f}(x_t, u_t) + w_t $$

$$ M_t = \nabla_{v_t} h(x_t, v_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{v_t} \bar{h}(x_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{v_t} v_t}_{I}|_{x_t=x,v_t=0} = I $$


<div class="important_box" markdown="1">
<h1>Equations of the Kalman filter</h1>

The recursive formula for the Kalman filter consists of the **prediction step**

$$ \begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T   \end{align} $$


and the **update step**

$$ \begin{align}
z_t &= y_{t}-h(x, 0)\\
S_t &= R_t + C_tP_{t|t-1}C_t^T\\
K_t &= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &= (I - K_tC_t)P_{t|t-1}.
\end{align} $$

</div>

<a href='https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm'></a>


<div id="rad_to_s" style="width:100px"></div>
<div id="div1"></div>
<div id="div2"></div>
<!-- <div id="system_dist_approx"  style="width: 600px; height: 600px;"></div> -->
<!--<div id="output_dist_approx"  style="width: 600px; height: 600px;"></div>-->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG"></script>
<script type="text/x-mathjax-config">

var mq = window.matchMedia( "(max-width: 570px)" );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  "HTML-CSS": { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

</script>








