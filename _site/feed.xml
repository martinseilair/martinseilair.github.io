<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="https://martinseilair.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://martinseilair.github.io/" rel="alternate" type="text/html" /><updated>2018-11-25T20:19:35+09:00</updated><id>https://martinseilair.github.io/feed.xml</id><title type="html">Ikigai</title><subtitle>A dump for random thoughts and observations</subtitle><entry><title type="html">Fourier curves</title><link href="https://martinseilair.github.io/jekyll/update/2018/11/24/fourier-curve.html" rel="alternate" type="text/html" title="Fourier curves" /><published>2018-11-24T00:04:07+09:00</published><updated>2018-11-24T00:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/11/24/fourier-curve</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/11/24/fourier-curve.html">&lt;p&gt;Inspired by &lt;a href=&quot;https://twitter.com/pickover/status/1065227111315767297?s=19&quot;&gt;Cliff Pickovers tweet&lt;/a&gt; and powered by my fascination about the Fourier transform, I took the time to derive and implement the Fourier series of a two dimensional  piecewise linear parametric curve. 
&lt;!--more--&gt;&lt;/p&gt;

&lt;p&gt;By clicking on the empty canvas you can design your custom shape. If you close the path, the Fourier series will be animated similarly to the video mentioned above. Have fun!
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;
  &lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;
  &lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/lodash.js/3.10.1/lodash.min.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/euler/mechanical_system.js&quot;&gt;&lt;/script&gt;

&lt;style&gt;
.button {
    background-color: #4CAF50;
    border: none;
    color: white;
    padding: 15px 25px;
    text-align: center;
    font-size: 16px;
    cursor: pointer;
}

.button:hover {
    background-color: green;
}
&lt;/style&gt;

&lt;p&gt;&lt;button onclick=&quot;moore();&quot; class=&quot;button&quot;&gt;Moore curve&lt;/button&gt;&lt;/p&gt;
&lt;div id=&quot;fourier&quot;&gt;&lt;/div&gt;

&lt;p&gt;&lt;button onclick=&quot;toggle_ellipses();&quot; class=&quot;button&quot;&gt;Toggle ellipses&lt;/button&gt; &lt;button onclick=&quot;toggle_lines();&quot; class=&quot;button&quot;&gt;Toggle lines&lt;/button&gt; &lt;button onclick=&quot;reset();&quot; class=&quot;button&quot;&gt;Reset&lt;/button&gt;&lt;/p&gt;

&lt;div id=&quot;yx&quot;&gt;&lt;/div&gt;

&lt;div id=&quot;yy&quot;&gt;&lt;/div&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

function moore(){
	reset();

	var i = 3;
	var hill = 50;

	hilbert(width/2 - hill, height/2 + hill, 3, i, hill, true);
	hilbert(width/2 - hill, height/2 - hill, 3, i, hill, true);


	hilbert(width/2 + hill, height/2 - hill, 1, i, hill, true);
	hilbert(width/2 + hill, height/2 + hill, 1, i, hill, true);



	svg.on(&quot;mousedown&quot;,null)
	points.push(points[0]);
	update_path();
	calc_y();
	//draw_components();

	cx = calc_coeff(y.x);
	cy = calc_coeff(y.y);
	calc_fourier_ramp(cx,cy);
	calc_ellipse()

	timer = window.setInterval(animate, 1000*dt );

}
var rot = [];

rot.push([-1,+1]);
rot.push([-1,-1]);
rot.push([+1,-1]);
rot.push([+1,+1]);

var or = [1,0,0,-1];
var nc = [true, false, false, true]
var iter = [0,1,2,3]
function hilbert(px, py, o, i, l, n){

	if(o==4) o = 0;
	if(o==-1) o = 3;
	//o=Math.min(4,Math.max(o,0))
	if(i==0){
		points.push([px,py]);
	}else{
		var biter = (n==false)?iter.slice(0).reverse():iter;
		for (var j=0;j&lt;biter.length;j++){
				hilbert(px+rot[(biter[j]+o) % 4][0]*l/2, py+rot[(biter[j]+o) % 4][1]*l/2, o + or[biter[j]], i-1, l/2, n?!nc[biter[j]]:nc[biter[j]]);
		}
	}
}






var show_ellipses = true;
var show_lines = true;

function toggle_ellipses(){
	show_ellipses = !show_ellipses;

	eg.style(&quot;visibility&quot;,show_ellipses?&quot;visible&quot;:&quot;hidden&quot;)
}


function toggle_lines(){
	show_lines = !show_lines;
	vg.style(&quot;visibility&quot;,show_lines?&quot;visible&quot;:&quot;hidden&quot;)
}



// define path

var points = [];
var r_0 = 7;
var r = 4;
var T = 5;
var omega = 2*Math.PI/T;
var y = {x: [], y: []};
var time;
var N_start = 1;
var N_max = 256;
var N_inc = 1;
var N = N_start;
var cx;
var cy;
var NP;
var curr_step = 0;
var steps = 1000;
var dt = T/steps;
var fp;
var vp;
var ells = [];


var svg;
var path;
var path_f;
var vg;
var eg;
var timer = null;

var width = 500;
var height = 300;

function reset(){
	d3.select('#fourier').selectAll(&quot;svg&quot;).remove();

	clearInterval(timer);

	points = [];
	N = N_start;
	curr_step = 0;
	ells = [];
	y = {x: [], y: []};
	cx=null;
	cy=null;

	svg = d3.select('#fourier').append('svg')
			.style(&quot;width&quot;,&quot;100%&quot;)
			.style(&quot;border&quot;,&quot;1px solid black&quot;)
			.attr(&quot;viewBox&quot;,&quot;0 0 &quot; + width +  &quot; &quot; + height)
			.on(&quot;mousedown&quot;, click)

	path = svg.append(&quot;path&quot;);
	path_f = svg.append(&quot;path&quot;);
	vg = svg.append(&quot;g&quot;);
	eg = svg.append(&quot;g&quot;);
}

function calc_coeff(f){
	// pre calculate sin and cos

	var S = Array(N+1).fill(null).map(()=&gt;Array(NP).fill(null));
	var C = Array(N+1).fill(null).map(()=&gt;Array(NP).fill(null));
	a = [];

	A = [];
	phi = [];

	for (var k=0; k&lt;N+1;k++){
		for(var i=0; i&lt;NP; i++){
			S[k][i] = Math.sin(k*omega*time[i]);
			C[k][i] = Math.cos(k*omega*time[i]);
		}
	}

	for(var i=0; i&lt;NP-1; i++){
		a[i] = (f[i+1] - f[i])/(time[i+1] - time[i]);	
	}

	for (var k=0; k&lt;N+1;k++){

		var re = 0;
		var im = 0;

		if(k==0){
			for(var i=0; i&lt;NP-1; i++){
			 	re+= (f[i]-time[i]*a[i])*(time[i+1]-time[i]) + a[i]/2*(time[i+1]*time[i+1]-time[i]*time[i]);
			}
			re = 1/T*re;
		}else{
			for(var i=0; i&lt;NP-1; i++){
			 	re+= -f[i]*S[k][i] + f[i+1]*S[k][i+1] -a[i]/(k*omega)*C[k][i] +a[i]/(k*omega)*C[k][i+1];

			 	im+= -f[i]*C[k][i] + f[i+1]*C[k][i+1] +a[i]/(k*omega)*S[k][i] -a[i]/(k*omega)*S[k][i+1];
			}
			re = 1/(2*Math.PI*k)*re;
			im = 1/(2*Math.PI*k)*im;
		}

		if (k==0){
			A[k] = re; 
		}else{
			A[k] = 2*Math.sqrt(re*re + im*im);
		}

		if (k==0){
			phi[k] = Math.PI/2; 
		}else{
			phi[k] = Math.atan2(re, -im);
		}
	}
	return {A: A, phi:phi};
}

function calc_fourier_ramp(cx,cy){

	fp = [];

	vp = [];
	for(var step=0; step&lt;steps+1;step++){

		var t = step/steps*T;

		var p = [points[0][0],points[0][1]]
		var vpp = [];
		for (var k=0; k&lt;N+1;k++){
			p[0]+=cx.A[k]*Math.sin(k*omega*t+cx.phi[k])
			p[1]+=cy.A[k]*Math.sin(k*omega*t+cy.phi[k])
			vpp.push([p[0], p[1]]);
		}
		vp.push(vpp);
		fp.push(p);
	}

}

function draw_fourier_ramp(){

	fps = fp.slice(0, curr_step+1);


	path_f = path_f.data([fps])
    path_f.attr('d', function(d){return line(d)})
        .style('stroke-width', 1)
        .style('stroke', 'black')
        .style('fill', 'none');	
    path_f.enter().append('svg:path').attr('d', function(d){return line(d)})
        .style('stroke-width', 1)
        .style('stroke', 'black')
        .style('fill', 'none');	
    path_f.exit().remove();


	vg.selectAll(&quot;path&quot;).remove()
    for(var k=0;k&lt;N;k++){
    	vg.append(&quot;path&quot;)
    		.attr(&quot;d&quot;,&quot;M &quot; + vp[curr_step][k][0] + &quot; &quot; + vp[curr_step][k][1] + &quot; L &quot; + vp[curr_step][k+1][0] + &quot; &quot; + vp[curr_step][k+1][1])
    		.style('stroke-width', 1)
        	.style('stroke', 'red')
        	.style('fill', 'none');	
    }


}

function calc_ellipse(){
	ells = [];
	eg.selectAll(&quot;g&quot;).remove()
	//var eg = svg.append(&quot;g&quot;);
	for (var k=1; k&lt;N+1;k++){
		var ell = [];
	    for(var step=0; step&lt;steps+1;step++){

	    	 ell.push([vp[step][k][0]-vp[step][k-1][0],vp[step][k][1]-vp[step][k-1][1]]);
	    }

	    c_x = vp[curr_step][k-1][0];
	    c_y = vp[curr_step][k-1][1];

		eeg = eg.append(&quot;g&quot;)
				.attr(&quot;transform&quot;,&quot;translate(&quot; + c_x + &quot;,&quot; + c_y + &quot;)&quot;)
		path_e = eeg.append(&quot;path&quot;);


		path_e = path_e.data([ell])
	    path_e.attr('d', function(d){return line(d)})
	        .style('stroke-width', 1)
	        .style('stroke', 'blue')
	        .style('fill', 'none');	
	    path_e.enter().append('svg:path').attr('d', function(d){return line(d)})
	        .style('stroke-width', 1)
	        .style('stroke', 'blue')
	        .style('fill', 'none');	
	    path_e.exit().remove();

	    ells.push(eeg)
	}
}


function draw_ellipse(){

 	for (var k=1; k&lt;N+1;k++){
	    c_x = vp[curr_step][k-1][0];
	    c_y = vp[curr_step][k-1][1];

		ells[k-1].attr(&quot;transform&quot;,&quot;translate(&quot; + c_x + &quot;,&quot; + c_y + &quot;)&quot;)

	}

}

function update_path(){



	path = path.data([points])
    path.attr('d', function(d){return line(d)})
        .style('stroke-width', 1)
        .style('stroke', 'steelblue')
        .style('fill', 'none');
    path.enter().append('svg:path').attr('d', function(d){return line(d)})
        .style('stroke-width', 1)
        .style('stroke', 'steelblue')
        .style('fill', 'none');
    path.exit().remove()	
}

function animate(){
	
	if(curr_step&gt;steps){
		curr_step = 0;
		//N+=N_inc;
		N*=2;
		if(N&gt;N_max){
			N = N_start;
		}
		cx = calc_coeff(y.x);
		cy = calc_coeff(y.y);
		calc_fourier_ramp(cx,cy);
		calc_ellipse()
	}

	draw_fourier_ramp();
	draw_ellipse()
	curr_step++;
}

function click_circ(i){
	svg.on(&quot;mousedown&quot;,null)
	points.push(points[0]);
	update_path();
	calc_y();
	//draw_components();

	svg.selectAll(&quot;circle&quot;).remove()


	cx = calc_coeff(y.x);
	cy = calc_coeff(y.y);
	calc_fourier_ramp(cx,cy);
	calc_ellipse()

	timer = window.setInterval(animate, 1000*dt );
}

function dist(x1,x2){

	return Math.sqrt((x1[0]-x2[0])*(x1[0]-x2[0]) + (x1[1]-x2[1])*(x1[1]-x2[1]) );
}

function calc_y(){
	// compute distances

	time = [];
	time.push(0);
	var total_dist = 0;
	var d;
	for(var i=0; i&lt;points.length-1; i++){
		d = dist(points[i],points[i+1]);
		total_dist+=d
		time.push(d)
	}

	// total time
	for(var i=1; i&lt;points.length; i++){
		time[i] = time[i-1] + time[i]*T/total_dist;
	}




	for(var i=0; i&lt;points.length; i++){
		y.x.push(points[i][0]-points[0][0])
	}

	for(var i=0; i&lt;points.length; i++){
		y.y.push(points[i][1]-points[0][1])
	}

	NP = time.length;

}

function draw_components(){

	pxmax = Math.max.apply(Math, y.x);
	pxmin = Math.min.apply(Math, y.x);
	// compute function
	var px = [];
	for(var i=0; i&lt;y.x.length;i++){
		px.push([time[i],(y.x[i]-pxmin)/(pxmax-pxmin)])
	} 

	var svgyx = d3.select('#yx').append('svg')
			.style(&quot;width&quot;,&quot;100%&quot;)
			.style(&quot;border&quot;,&quot;1px solid black&quot;)
			.attr(&quot;viewBox&quot;,&quot;0 0 &quot; + T +  &quot; &quot; + (pxmax-pxmin)/(pxmax-pxmin))


	var pathyx = svgyx.append(&quot;path&quot;);


	pathyx = pathyx.data([px])
    pathyx.attr('d', function(d){return line(d)})
        .style('stroke-width', 0.01)
        .style('stroke', 'steelblue')
        .style('fill', 'none');	

	pymax = Math.max.apply(Math, y.y);
	pymin = Math.min.apply(Math, y.y);
	// compute function
	var py = [];
	for(var i=0; i&lt;y.y.length;i++){
		py.push([time[i],(y.y[i]-pymin)/(pymax-pymin)])
	} 

	var svgyy = d3.select('#yy').append('svg')
			.style(&quot;width&quot;,&quot;100%&quot;)
			.style(&quot;border&quot;,&quot;1px solid black&quot;)
			.attr(&quot;viewBox&quot;,&quot;0 0 &quot; + T +  &quot; &quot; + (pymax-pymin)/(pymax-pymin))


	var pathyy = svgyy.append(&quot;path&quot;);


	pathyy = pathyy.data([py])
    pathyy.attr('d', function(d){return line(d)})
        .style('stroke-width', 0.01)
        .style('stroke', 'steelblue')
        .style('fill', 'none');	

}



function click(){
	var coords = d3.mouse(this);

	points.push(coords);   // Push data to our array

	svg.selectAll(&quot;circle&quot;)  // For new circle, go through the update process
		.data(points)
		.enter()
		.append(&quot;circle&quot;)
		.attr(&quot;cx&quot;, function(d) { return d[0]; })
		.attr(&quot;cy&quot;, function(d) { return d[1]; })  // Get attributes from circleAttrs var
		.style(&quot;fill&quot;, function(d,i) { return (i==0)?&quot;#00F&quot;:&quot;#000&quot;; })
		.attr(&quot;r&quot;, function(d,i) { return (i==0)?r_0:r; })
		.on(&quot;mousedown&quot;, function(d,i) { if(i==0)click_circ(i)})  // Get attributes from circleAttrs var

	update_path();



}

var line = d3.line()
            .x(function(d) { return d[0]; })
            .y(function(d) { return d[1]; })


reset();







&lt;/script&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;The following derivation shows a way to obtain the equations of the Fourier series of a two dimensional piecewise linear parametric curve. Please be aware, that this is certainly not the most efficient way to derive it.&lt;/p&gt;

&lt;p&gt;We will treat the \(x\) and \(y\) component of the function separately. Therefore, we calculate the Fourier series for both components separately.&lt;/p&gt;

&lt;h1 id=&quot;fourier-series&quot;&gt;Fourier series&lt;/h1&gt;

&lt;p&gt;The complex Fourier series is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t) = \sum \limits_{-\infty}^{\infty} \hat{y}_k e^{ik\omega t}&lt;/script&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k = \frac{1}{T} \int\limits_0^T y(t) e^{-ik\omega t} \,dt .&lt;/script&gt;

&lt;p&gt;Our component functions \(y(t)\) are piecewise linear and can be defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
y(t) = y_{i} + (t - T_i)\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}  \,\text{ , }\, T_i \leq t &lt; T_{i+1} %]]&gt;&lt;/script&gt;

&lt;p&gt;Let’s calculate the Fourier coefficients. We start with \(k=0\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} \int\limits_{T_i}^{T_{i+1}} \left(y_{i} + (t - T_i)\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) e^{-i0\omega t} \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} \int\limits_{T_i}^{T_{i+1}} \left(y_{i} + (t - T_i)\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} y_{i}\int\limits_{T_i}^{T_{i+1}} 1   \,dt + \frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\int\limits_{T_i}^{T_{i+1}} t \,dt - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\int\limits_{T_i}^{T_{i+1}} 1 \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} y_{i}\left[t\right]_{T_i}^{T_{i+1}}  + \frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\left[\frac{1}{2}t^2\right]_{T_i}^{T_{i+1}} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\left[t\right]_{T_i}^{T_{i+1}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} y_{i}\left[T_{i+1} - T_i \right]  + \frac{1}{2}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\left[T_{i+1}^2 - T_i^2\right] - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\left[T_{i+1} - T_i\right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_0 = \frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \left[T_{i+1} - T_i \right]  + \frac{1}{2}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\left[T_{i+1}^2 - T_i^2\right]&lt;/script&gt;

&lt;p&gt;Now we look at the general case \(k&amp;gt;0\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k = \frac{1}{T} \sum_{i=0}^{N-1} \int\limits_{T_i}^{T_{i+1}} \left(y_{i} + (t - T_i)\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) e^{-ik\omega t} \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k = \frac{1}{T} \sum_{i=0}^{N-1} \int\limits_{T_i}^{T_{i+1}} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) e^{-ik\omega t} \,dt + \frac{1}{T} \sum_{i=0}^{N-1} \int\limits_{T_i}^{T_{i+1}} t\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}e^{-ik\omega t} \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k = \frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right)\int\limits_{T_i}^{T_{i+1}}  e^{-ik\omega t} \,dt + \frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i} \int\limits_{T_i}^{T_{i+1}} te^{-ik\omega t} \,dt&lt;/script&gt;

&lt;p&gt;The first term simplifies to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \int\limits_{T_i}^{T_{i+1}}  e^{-ik\omega t} \,dt&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \left[ \frac{1}{-ik\omega}  e^{-ik\omega t} \right]_{T_i}^{T_{i+1}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{i}{k\omega}\left[   e^{-ik\omega T_{i+1}} - e^{-ik\omega T_i} \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{i}{k\omega}\left[ \cos(-k\omega T_{i+1}) + i\sin(-k\omega T_{i+1})   - \cos(-k\omega T_{i}) - i\sin(-k\omega T_{i})  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{1}{k\omega}\left[ i\cos(-k\omega T_{i+1}) -\sin(-k\omega T_{i+1})   - i\cos(-k\omega T_{i}) +\sin(-k\omega T_{i})  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{1}{k\omega}\left[ i\cos(k\omega T_{i+1}) +\sin(k\omega T_{i+1})   - i\cos(k\omega T_{i}) -\sin(k\omega T_{i})  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{1}{k\omega}\left[\sin(k\omega T_{i+1}) -\sin(k\omega T_{i}) + i\left[\cos(k\omega T_{i+1})    - \cos(k\omega T_{i})\right]  \right]&lt;/script&gt;

&lt;p&gt;We now look at the second term&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i} \int\limits_{T_i}^{T_{i+1}} te^{-ik\omega t} \,dt .&lt;/script&gt;

&lt;p&gt;Fortunately, we know the identity&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_0^1 x^n e^{-ax}\,dx = \frac{n!}{a^{n+1}}\left[1 - e^{-a}\sum_{i=0}^n \frac{a^i}{i!}\right].&lt;/script&gt;

&lt;p&gt;We can use it, by applying the coordinate transformation to our integral&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;t = T_i + (T_{i+1} - T_i)x&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;dt = (T_{i+1} - T_i)\,dx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = \frac{t - T_i}{T_{i+1} - T_i}&lt;/script&gt;

&lt;p&gt;We obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i} \int\limits_{0}^{1} (T_i + (T_{i+1} - T_i)x)e^{-ik\omega (T_i + (T_{i+1} - T_i)x)}(T_{i+1} - T_i) \,dx&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}(T_{i+1} - T_i) e^{-ik\omega T_i} \left[T_i\int\limits_{0}^{1}  e^{-ik\omega (T_{i+1} - T_i)x} \,dx + (T_{i+1} - T_i)\int\limits_{0}^{1} xe^{-ik\omega (T_{i+1} - T_i)x} \,dx \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}(y_{i+1}-y_{i}) e^{-ik\omega T_i} \left[T_i\int\limits_{0}^{1}  e^{-ik\omega (T_{i+1} - T_i)x} \,dx + (T_{i+1} - T_i)\int\limits_{0}^{1} xe^{-ik\omega (T_{i+1} - T_i)x} \,dx \right]&lt;/script&gt;

&lt;p&gt;Now we have to integrals of the identity above. The first integral is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_{0}^{1}  e^{-ik\omega (T_{i+1} - T_i)x} \,dx&lt;/script&gt;

&lt;p&gt;with \(a = ik\omega (T_{i+1} - T_i) \) and \(n=0\). It follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_0^1 x^n e^{-ax}\,dx = \frac{0!}{a^{0+1}}\left[1 - e^{-a}\sum_{i=0}^0 \frac{a^i}{i!}\right] =  \frac{1}{a}\left[1 - e^{-a}\right] =  \frac{1}{ik\omega (T_{i+1} - T_i)}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}\right]&lt;/script&gt;

&lt;p&gt;The second integral is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_{0}^{1} xe^{-ik\omega (T_{i+1} - T_i)x} \,dx&lt;/script&gt;

&lt;p&gt;with \(a = ik\omega (T_{i+1} - T_i) \) and \(n=1\).&lt;/p&gt;

&lt;p&gt;It follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int\limits_0^1 x^n e^{-ax}\,dx = \frac{1!}{a^{1+1}}\left[1 - e^{-a}\sum_{i=0}^1 \frac{a^i}{i!}\right] =  \frac{1}{a^2}\left[1 - e^{-a}(1 + a)\right] =  \frac{1}{-k^2\omega^2 (T_{i+1} - T_i)^2}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}(1 + ik\omega (T_{i+1} - T_i))\right]&lt;/script&gt;

&lt;p&gt;We insert the results back into our equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}(y_{i+1}-y_{i}) e^{-ik\omega T_i} \left[T_i\frac{1}{ik\omega (T_{i+1} - T_i)}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}\right] + (T_{i+1} - T_i)\frac{1}{-k^2\omega^2 (T_{i+1} - T_i)^2}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}(1 + ik\omega (T_{i+1} - T_i))\right] \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}(y_{i+1}-y_{i}) e^{-ik\omega T_i} \left[-\frac{i T_i}{k\omega (T_{i+1} - T_i)}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}\right] -\frac{1}{k^2\omega^2 (T_{i+1} - T_i)}\left[1 - e^{-ik\omega (T_{i+1} - T_i)}(1 + ik\omega (T_{i+1} - T_i))\right] \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)} e^{-ik\omega T_i} \left[-i T_i\left[1 - e^{-ik\omega (T_{i+1} - T_i)}\right] - \frac{1}{k\omega }\left[1 - e^{-ik\omega (T_{i+1} - T_i)}(1 + ik\omega (T_{i+1} - T_i))\right] \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[-i T_i e^{-ik\omega T_i} +i T_ie^{-ik\omega T_i}e^{-ik\omega (T_{i+1} - T_i)} - \frac{1}{k\omega }e^{-ik\omega T_i} + \frac{1}{k\omega }e^{-ik\omega T_i}e^{-ik\omega (T_{i+1} - T_i)}(1 + ik\omega (T_{i+1} - T_i)) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[-i T_i e^{-ik\omega T_i} +i T_ie^{-ik\omega T_{i+1}} - \frac{1}{k\omega }e^{-ik\omega T_i} + \frac{1}{k\omega }e^{-ik\omega T_{i+1}}(1 + ik\omega (T_{i+1} - T_i)) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[-i T_i e^{-ik\omega T_i} +i T_ie^{-ik\omega T_{i+1}} - \frac{1}{k\omega }e^{-ik\omega T_i} + \frac{1}{k\omega }e^{-ik\omega T_{i+1}}+ \frac{1}{k\omega }e^{-ik\omega T_{i+1}}ik\omega (T_{i+1} - T_i) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[-i T_i e^{-ik\omega T_i} +i T_ie^{-ik\omega T_{i+1}} - \frac{1}{k\omega }e^{-ik\omega T_i} + \frac{1}{k\omega }e^{-ik\omega T_{i+1}}+ iT_{i+1}e^{-ik\omega T_{i+1}} - iT_ie^{-ik\omega T_{i+1}} \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[-i T_i e^{-ik\omega T_i} - \frac{1}{k\omega }e^{-ik\omega T_i} + \frac{1}{k\omega }e^{-ik\omega T_{i+1}}+ iT_{i+1}e^{-ik\omega T_{i+1}} \right]&lt;/script&gt;

&lt;p&gt;We replace the exponential terms with \(\sin\) and \(\cos\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[
-i T_i (\cos(-k\omega T_{i}) + i\sin(-k\omega T_{i})) 
- \frac{1}{k\omega }(\cos(-k\omega T_{i}) + i\sin(-k\omega T_{i})) 
+ \frac{1}{k\omega }(\cos(-k\omega T_{i+1}) + i\sin(-k\omega T_{i+1}))
+ iT_{i+1}(\cos(-k\omega T_{i+1}) + i\sin(-k\omega T_{i+1})) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[ 
-i T_i\cos(-k\omega T_{i}) 
+ T_i\sin(-k\omega T_{i}) 
- \frac{1}{k\omega }\cos(-k\omega T_{i}) 
-i\frac{1}{k\omega }\sin(-k\omega T_{i}) 
+ \frac{1}{k\omega }\cos(-k\omega T_{i+1}) 
+ i\frac{1}{k\omega }\sin(-k\omega T_{i+1})
+ iT_{i+1}\cos(-k\omega T_{i+1}) 
- T_{i+1}\sin(-k\omega T_{i+1}) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[ 
-i T_i\cos(k\omega T_{i}) 
- T_i\sin(k\omega T_{i}) 
- \frac{1}{k\omega }\cos(k\omega T_{i}) 
+i\frac{1}{k\omega }\sin(k\omega T_{i}) 
+ \frac{1}{k\omega }\cos(k\omega T_{i+1}) 
-i\frac{1}{k\omega }\sin(k\omega T_{i+1})
+ iT_{i+1}\cos(k\omega T_{i+1}) 
+ T_{i+1}\sin(k\omega T_{i+1}) \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[
- T_i\sin(k\omega T_{i})
- \frac{1}{k\omega }\cos(k\omega T_{i})
+ \frac{1}{k\omega }\cos(k\omega T_{i+1})
+ T_{i+1}\sin(k\omega T_{i+1})
 +i\left[-T_i\cos(k\omega T_{i})   
 +\frac{1}{k\omega }\sin(k\omega T_{i})  
 -\frac{1}{k\omega }\sin(k\omega T_{i+1})
 + T_{i+1}\cos(k\omega T_{i+1}) \right]  \right]&lt;/script&gt;

&lt;p&gt;We now look again at our first term from above and separate it
&lt;script type=&quot;math/tex&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} \left(y_{i} - T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i}\right) \frac{1}{k\omega}\left[\sin(k\omega T_{i+1}) -\sin(k\omega T_{i}) + i\left[\cos(k\omega T_{i+1})    - \cos(k\omega T_{i})\right]  \right]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;into&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;-\frac{1}{T} \sum_{i=0}^{N-1} T_i\frac{y_{i+1}-y_{i}}{T_{i+1} - T_i} \frac{1}{k\omega}\left[\sin(k\omega T_{i+1}) -\sin(k\omega T_{i}) + i\left[\cos(k\omega T_{i+1})    - \cos(k\omega T_{i})\right]  \right]&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1} y_{i}\frac{1}{k\omega}\left[\sin(k\omega T_{i+1}) -\sin(k\omega T_{i}) + i\left[\cos(k\omega T_{i+1})    - \cos(k\omega T_{i})\right]  \right] .&lt;/script&gt;

&lt;p&gt;We combine the result of the second term with first equation of the separation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[
- T_i\sin(k\omega T_{i})
- \frac{1}{k\omega }\cos(k\omega T_{i})
+ \frac{1}{k\omega }\cos(k\omega T_{i+1})
+ T_{i+1}\sin(k\omega T_{i+1})
 +i\left[-T_i\cos(k\omega T_{i})   
 +\frac{1}{k\omega }\sin(k\omega T_{i})  
 -\frac{1}{k\omega }\sin(k\omega T_{i+1})
 + T_{i+1}\cos(k\omega T_{i+1}) \right]

-T_i\sin(k\omega T_{i+1}) 
+T_i\sin(k\omega T_{i}) 
+ i\left[-T_i\cos(k\omega T_{i+1})    
+T_i \cos(k\omega T_{i})\right]
  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[
\underbrace{- T_i\sin(k\omega T_{i})
+T_i\sin(k\omega T_{i}) }_{0}
- \frac{1}{k\omega }\cos(k\omega T_{i})
+ \frac{1}{k\omega }\cos(k\omega T_{i+1})
+ (T_{i+1}-T_i)\sin(k\omega T_{i+1})
 +i\left[\underbrace{-T_i\cos(k\omega T_{i})  
 +T_i \cos(k\omega T_{i}) }_{0}  
 +\frac{1}{k\omega }\sin(k\omega T_{i})  
 -\frac{1}{k\omega }\sin(k\omega T_{i+1})
 + (T_{i+1}-T_i)\cos(k\omega T_{i+1}) \right]
  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{y_{i+1}-y_{i}}{k\omega (T_{i+1} - T_i)}  \left[
- \frac{1}{k\omega }\cos(k\omega T_{i})
+ \frac{1}{k\omega }\cos(k\omega T_{i+1})
+ (T_{i+1}-T_i)\sin(k\omega T_{i+1})
 +i\left[  
 \frac{1}{k\omega }\sin(k\omega T_{i})  
 -\frac{1}{k\omega }\sin(k\omega T_{i+1})
 + (T_{i+1}-T_i)\cos(k\omega T_{i+1}) \right]
  \right]&lt;/script&gt;

&lt;p&gt;Now we incorporate the second part of the separation.&lt;/p&gt;

&lt;p&gt;With the helping variable&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a = \frac{y_{i+1}-y_{i}}{(T_{i+1} - T_i)}&lt;/script&gt;

&lt;p&gt;follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{1}{k\omega}  \left[
 -y_i\sin(k\omega T_{i}) 
+(y_i+a(T_{i+1}-T_i))\sin(k\omega T_{i+1}) 
- \frac{a}{k\omega }\cos(k\omega T_{i})
+ \frac{a}{k\omega }\cos(k\omega T_{i+1})
 +i\left[
 - y_i\cos(k\omega T_{i})
 +(y_i+a(T_{i+1}-T_i))\cos(k\omega T_{i+1})
 +\frac{a}{k\omega }\sin(k\omega T_{i})  
 -\frac{a}{k\omega }\sin(k\omega T_{i+1})
 \right] 

  \right]&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{1}{T} \sum_{i=0}^{N-1}\frac{1}{k\omega}  \left[
 -y_i\sin(k\omega T_{i}) 
+y_{i+1}\sin(k\omega T_{i+1}) 
- \frac{a}{k\omega }\cos(k\omega T_{i})
+ \frac{a}{k\omega }\cos(k\omega T_{i+1})
 +i\left[
 - y_i\cos(k\omega T_{i})
 +y_{i+1}\cos(k\omega T_{i+1})
 +\frac{a}{k\omega }\sin(k\omega T_{i})  
 -\frac{a}{k\omega }\sin(k\omega T_{i+1})
 \right] 

  \right]&lt;/script&gt;

&lt;p&gt;The resulting formula is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k =  \frac{1}{T} \sum_{i=0}^{N-1}\frac{1}{k\omega}  \left[
 -y_i\sin(k\omega T_{i}) 
+y_{i+1}\sin(k\omega T_{i+1}) 
- \frac{a}{k\omega }\cos(k\omega T_{i})
+ \frac{a}{k\omega }\cos(k\omega T_{i+1})
 +i\left[
 - y_i\cos(k\omega T_{i})
 +y_{i+1}\cos(k\omega T_{i+1})
 +\frac{a}{k\omega }\sin(k\omega T_{i})  
 -\frac{a}{k\omega }\sin(k\omega T_{i+1})
 \right] 

  \right]&lt;/script&gt;

&lt;p&gt;We note that we can \(\frac{1}{T}\frac{1}{k\omega} = \frac{1}{2\pi k}\) and finally obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{y}_k =  \frac{1}{2\pi k} \sum_{i=0}^{N-1}  \left[
 -y_i\sin(k\omega T_{i}) 
+y_{i+1}\sin(k\omega T_{i+1}) 
- \frac{a}{k\omega }\cos(k\omega T_{i})
+ \frac{a}{k\omega }\cos(k\omega T_{i+1})
 +i\left[
 - y_i\cos(k\omega T_{i})
 +y_{i+1}\cos(k\omega T_{i+1})
 +\frac{a}{k\omega }\sin(k\omega T_{i})  
 -\frac{a}{k\omega }\sin(k\omega T_{i+1})
 \right] 

  \right]&lt;/script&gt;

&lt;p&gt;In the final step, we can express our result as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y(t) = \sum_{k=0}^N A_k \sin(k\omega t + \varphi_k)&lt;/script&gt;

&lt;p&gt;with coefficients&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
A_k = \begin{cases}
      \hat{y}_0 &amp; k = 0 \\
      2 |\hat{y}_k|      &amp; k&gt;0
    \end{cases} %]]&gt;&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\varphi_l = \begin{cases}
      \frac{\pi}{2} &amp; k = 0 \\
      \text{atan2}( \Re(\hat{y}_k), - \Im(\hat{y}_k))      &amp; k&gt;0
    \end{cases}. %]]&gt;&lt;/script&gt;</content><author><name></name></author><summary type="html">Inspired by Cliff Pickovers tweet and powered by my fascination about the Fourier transform, I took the time to derive and implement the Fourier series of a two dimensional piecewise linear parametric curve.</summary></entry><entry><title type="html">Nonlinear filtering: Particle filter</title><link href="https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html" rel="alternate" type="text/html" title="Nonlinear filtering: Particle filter" /><published>2018-11-08T07:04:07+09:00</published><updated>2018-11-08T07:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html">&lt;p&gt;This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example. &lt;!--more--&gt;If you haven’t read the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-intro.html&quot;&gt;introduction&lt;/a&gt;, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind the particle filter.
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;
  &lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://martinseilair.github.io/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;


	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;particle filter&lt;/strong&gt; is a sample-based approximation of the Bayes filter. It is used to &lt;strong&gt;infer&lt;/strong&gt; the current state of an arbitrary probabilistic state space model given all observations and inputs up to the current timestep and a prior distribution of the initial state.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;In this section, we will develop the method of particle filtering directly from the Bayes filter and mean field particle methods.&lt;/p&gt;

&lt;p&gt;We learned that the recursive equations of the Bayes filter consist of the 
 &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\,dx_t}&lt;/script&gt;

&lt;p&gt;and that the integrals, contained in these equations, are in general intractable. The main idea of the particle filter is to approximate the Bayes filter by approximating the current posterior \(p(x_{t+1}|y_{0:t},u_{0:t})\) and \(p(x_t|y_{0:t},u_{0:t-1})\) with &lt;a href=&quot;https://en.wikipedia.org/wiki/Empirical_measure&quot;&gt;empirical measures&lt;/a&gt;  \(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})\) and \(\hat{p}(x_t|y_{0:t},u_{0:t-1})\). Let’s take a brief look at empirical measures.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Empirical measure&lt;/h1&gt;
  &lt;p&gt;The empirical measure of \(p(x)\) is defined as&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_N(x) = \frac{1}{N}\sum_{i=1}^{N} \delta_{x_i}(x),&lt;/script&gt;

  &lt;p&gt;where \(\delta_{x_i}(x)\) is an abbreviation for the shifted &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta function&lt;/a&gt; \(\delta(x-x_i)\) and \(x_{1:N}\) are \(N\) samples from \(p(x)\).
If the number of samples goes to infinity, the empirical measure will &lt;a href=&quot;https://en.wikipedia.org/wiki/Almost_surely&quot;&gt;almost surely&lt;/a&gt; converge to the distribution \(p(x)\). The following figure shows the distribution \(p(x)\) in red and the empirical measure \(p_N(x)\) in black.&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;dirac_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
  &lt;script&gt;
function load_em_meas(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = sample_gmm(mix, gs, n_sam, dom);
	for (var i = 0; i &lt; n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = &quot;red&quot;;
	create_dirac_plot(&quot;#dirac_plot&quot;, samples, [dat], dom, 0.2, false, 0.7);
}
load_em_meas();
&lt;/script&gt;

  &lt;p&gt;Please be aware that the lines, representing the Dirac delta functions, would actually be infinitely high. But in order to visualize it, we set the length of the lines to be the area under the corresponding Dirac delta function. In our case, this area will be \(\frac{1}{N}\).&lt;/p&gt;

  &lt;p&gt;Up until now, we assumed that each Dirac delta function is weighted uniformly. We can also define a &lt;strong&gt;weighted empirical measure&lt;/strong&gt; by&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p_N(x) =\sum_{i=1}^{N} w_i\delta_{x_i}(x),&lt;/script&gt;

  &lt;p&gt;with \(\sum_{i=1}^{N}w_i = 1\).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;In the following, we will call the tuple \((x_i, w_i)\), corresponding to the position and weight of the \(i\)th Dirac delta function, a &lt;strong&gt;particle&lt;/strong&gt;. Now that we have an idea about empirical measures, we can directly start our derivation with the update step.&lt;/p&gt;

&lt;h1 id=&quot;update-step&quot;&gt;Update step&lt;/h1&gt;

&lt;p&gt;The first step is to replace the posterior distribution \(p(x_t|y_{0:t},u_{0:t-1})\) with the empirical measure \(\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) \). This empirical measure could look like this:&lt;/p&gt;

&lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;prior_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
var n_sam = 50;
var mix = [0.8,0.2];
var gs = [[1.5,0.6],[4.0,0.6]];
var dom = [0.0, 5.0];
var n_plot = 1000;
var gdata= [];
var samples = sample_gmm(mix, gs, n_sam, dom);
var dat = [];
function load_prior_meas(){

	dat.gdata = gdata;
	dat.color = &quot;red&quot;;
	create_dirac_plot(&quot;#prior_plot&quot;, samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas();
&lt;/script&gt;

&lt;p&gt;We plug our empirical measure into the update step and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\int_{x_t} p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) dx_t} .&lt;/script&gt;

&lt;p&gt;We are multiplying the empirical measure and the likelihood function pointwise in the numerator. This pointwise multiplication can be understood graphically:&lt;/p&gt;

&lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;prior_mal_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
function load_prior_meas_mal(){
	dat.gdata = gdata;
	dat.color = &quot;red&quot;;
	create_dirac_plot(&quot;#prior_mal_plot&quot;, samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas_mal();
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;*&lt;/script&gt;

&lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;likelihood_mal_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
function load_likelihood_meas_mal(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = [];
	for (var i = 0; i &lt; n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = &quot;red&quot;;
	
	create_dirac_plot(&quot;#likelihood_mal_plot&quot;, samples, [dat], dom, 0.4, false, 0.2);
}
load_likelihood_meas_mal();
&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=&lt;/script&gt;

&lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;posterior_mal_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
function load_posterior_meas_mal(){
	var sw = samples.map((e)=&gt;{return {x:e, w:gmm(e, mix, gs)};})

	var dat = [];
	dat.gdata = gdata;
	dat.color = &quot;red&quot;;
	create_dirac_plot(&quot;#posterior_mal_plot&quot;, sw, [dat], dom, 0.2, false, 0.2);
}
load_posterior_meas_mal();
&lt;/script&gt;

&lt;p&gt;If we look closely at the denominator, we see that we are integrating a function which is weighted by a Dirac delta function. Therefore, we can use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function#Translation&quot;&gt;sifting property&lt;/a&gt; of the Dirac delta function to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)},&lt;/script&gt;

&lt;p&gt;where we canceled out the factor \(\frac{1}{N}\). As a result, we have a &lt;em&gt;weighted&lt;/em&gt; empirical measure&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_t|y_{0:t},u_{0:t-1}) =\sum_{i=1}^{N} w_i\delta_{\xi_t^i}(x_t),&lt;/script&gt;

&lt;p&gt;with weights&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w_i = \frac{p(y_t|x_t=\xi_t^i))}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)}.&lt;/script&gt;

&lt;p&gt;This leads us directly to the important resampling step.&lt;/p&gt;

&lt;h1 id=&quot;resampling-step&quot;&gt;Resampling step&lt;/h1&gt;

&lt;p&gt;In the resampling step, we are replacing our weighted empirical measure with an (unweighted) empirical measure. This is equivalent to sampling from a &lt;a href=&quot;https://en.wikipedia.org/wiki/Categorical_distribution&quot;&gt;categorical distribution&lt;/a&gt;, where our weights are defining the probability mass function. Therefore, the probability of drawing particles with high weights is higher than drawing particles with low weights. As a result, it is possible that particles with low weights are not drawn at all and particles with high weights are drawn multiple times. 
You may ask, why we are doing this weird resampling step? The main idea is quite simple:
We only have a limited number of particles, therefore, we want to discard hypotheses that have low probability. Otherwise, weights of some of the particles could get close to zero and would become useless.&lt;/p&gt;

&lt;p&gt;It can also be interpreted as a selection step in the context of &lt;a href=&quot;https://en.wikipedia.org/wiki/Evolutionary_algorithm&quot;&gt;evolutionary algorithms&lt;/a&gt;. Only the &lt;em&gt;fittest&lt;/em&gt; particles are able to produce the next generation of particles.&lt;/p&gt;

&lt;p&gt;As a result of the resampling step, we will obtain \(N\) particles \((\hat{\xi} _ t^{i},\frac{1}{N})_{1:N}\), which were drawn from&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\xi}_t^i \sim \hat{q}(x_t|y_{0:t},u_{0:t-1}).&lt;/script&gt;

&lt;h1 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h1&gt;
&lt;p&gt;Let’s look at the prediction step and replace the posterior with our empirical measure from the update step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})\frac{1}{N}\sum_{i=1}^N \delta_{\hat{\xi}_t^i}(x_t) dx_{t}.&lt;/script&gt;

&lt;p&gt;By changing the order of the sum and integral&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N\int_{x_{t}} p(x_{t+1}|x_{t}, u_{t}) \delta_{\hat{\xi}_t^i}(x_t) dx_{t},&lt;/script&gt;

&lt;p&gt;we note that we can use the sifting property again and finally obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N p(x_{t+1}|x_{t} = \hat{\xi}_t^i, u_{t}).&lt;/script&gt;

&lt;p&gt;Therefore, the distribution \(\hat{q}(x_{t+1}|y_{0:t},u_{0:t})\) is a weighted sum of &lt;em&gt;continuous&lt;/em&gt; distributions over \(x_{t+1}\), which is shown schematically in the following figure.&lt;/p&gt;

&lt;div style=&quot;text-align:center; width:100%&quot;&gt;&lt;div id=&quot;pred_plot&quot; style=&quot;width:75%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;
&lt;script&gt;
function load_pred_meas(){
	var n_sam = 50;
	var mix = [0.2, 0.2, 0.2, 0.2];
	var gs = [[1.5,0.6], [4.0,0.3], [2.5,0.3], [3.0,0.4]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var samples = [];
	var datdat = [];

	var sum_data = [...Array(n_plot)].map((e,i)=&gt;{return {x:dom[1]*i/n_plot,y:0}});

	for (var j = 0; j &lt; mix.length; j++) {
		var dat = [];
		var gdata= [];
		dat.color = &quot;red&quot;;
		for (var i = 0; i &lt; n_plot; i++) {
			var x = dom[1]*i/n_plot;	
			var v = mix[j]*gaussian(x,gs[j][0],gs[j][1] );
			gdata.push({x:x, y:v});
			sum_data[i].y+=v;
		}
		dat.gdata = gdata;
		datdat.push(dat);
	}

	var dat = [];
	dat.color=&quot;blue&quot;;
	dat.gdata=sum_data;
	datdat.push(dat);



	create_dirac_plot(&quot;#pred_plot&quot;, samples, datdat, dom, 0.2, false, 0.7);
}
load_pred_meas();
&lt;/script&gt;

&lt;p&gt;We note, that even if we have an empirical measure as current belief distribution, we will obtain a continuous distribution for our new belief.&lt;/p&gt;

&lt;p&gt;Fortunately, we already know how to obtain an empirical measure of a continous distribution: We simply have to sample from it. But how can we sample from our new belief?&lt;/p&gt;

&lt;p&gt;First, we take a sample from our current empirical posterior distribution. Based on this sample we can sample from the system distribution to obtain a sample from the new posterior. This sampling procedure samples exactly from our new continuous posterior distribution.&lt;/p&gt;

&lt;p&gt;Nice! We are essentially done, we expressed the update and prediction step in terms of empirical measures.&lt;/p&gt;

&lt;p&gt;But one thing seems a little bit weird. In order to obtain the empirical measure of the new  posterior in the prediction step, we have to sample from the current posterior, that was obtained from the resampling step. But in the resampling step, we already sampled from our posterior distribution. Therefore, we can take the particles of the current posterior directly as our samples.&lt;/p&gt;

&lt;p&gt;Let’s summarize the algorithm of the particle filter.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Algorithm of the particle filter&lt;/h1&gt;

  &lt;p&gt;The algorithm is &lt;strong&gt;initialized&lt;/strong&gt; with a set of \(N\) particles \((\xi_0^{i},\frac{1}{N})_{1:N}\).&lt;/p&gt;

  &lt;p&gt;We are taking the &lt;strong&gt;update step&lt;/strong&gt; and obtain the distribution&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\frac{1}{N}\sum_{i=1}^N p(y_t|x_t=\xi_t^i)},&lt;/script&gt;

  &lt;p&gt;which is equivalent to the particles \((\xi _ t^{i},\frac{p(y_t|x_t=\xi_t^i)}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)})_{1:N}\).&lt;/p&gt;

  &lt;p&gt;In the &lt;strong&gt;resampling step&lt;/strong&gt;, we are taking \(N\) samples&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\xi}_t^i \sim \hat{q}(x_t|y_{0:t},u_{0:t-1})&lt;/script&gt;

  &lt;p&gt;and obtain new particles \((\hat{\xi} _ t^{i},\frac{1}{N})_{1:N}\).&lt;/p&gt;

  &lt;p&gt;Finally, we are taking the &lt;strong&gt;prediction step&lt;/strong&gt;. We obtain the distribution&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N p(x_{t+1}|x_{t} = \hat{\xi}_t^i, u_{t})&lt;/script&gt;

  &lt;p&gt;and sample from it to get new particles \((\xi _ {t+1}^{i},\frac{1}{N})_{1:N}\).&lt;/p&gt;

  &lt;p&gt;The process starts again with the &lt;strong&gt;update step&lt;/strong&gt;.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Now that we are finally done with the derivation, let’s see how the particle filter performs in our toy example.&lt;/p&gt;

&lt;h1 id=&quot;example&quot;&gt;Example&lt;/h1&gt;

&lt;script&gt;

		// defines scenes
	n_scene = load_race_track(&quot;race_track_particle&quot;, &quot;https://martinseilair.github.io&quot;);
	n_scene.mode = 2;
	//n_scene.filter = &quot;particle&quot;;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;

	n_scene.t = 1;

	n_scene.ids = [&quot;race_track_particle_timestep&quot;, &quot;race_track_particle_likelihood&quot;, &quot;race_track_particle_update&quot;,&quot;race_track_particle_predict&quot;,&quot;race_track_particle_resampling&quot; ];

	n_scene.take_observation = true;


	n_scene.loaded = function(){
		document.getElementById(&quot;race_track_particle_likelihood&quot;).style.display=&quot;block&quot;;
		this.rt.hide_strip(&quot;inner&quot;);
		this.pf = init_particle_filter(this.rc, this.rt)


        var inner_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style(&quot;background-color&quot;)), d3.rgb('#ff834d'), d3.rgb('#8e3323')]);
		this.rt.init_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.rc.state), inner_color, 60);
		

        this.restart = function(){
            for (var i=0; i&lt;this.ids.length;i++){

                document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
            }
            document.getElementById(&quot;race_track_particle_likelihood&quot;).style.display=&quot;block&quot;;
            this.rc.reset();
            this.t = 1;

            //this.bf.reset();
            this.pf.reset()

            this.rt.hide_strip(&quot;inner&quot;);
            this.rt.treeg.style(&quot;opacity&quot;,1.0)
			this.take_observation = true;
        }




        this.rt.set_restart_button(this.restart.bind(this))



        this.toogle_observation = function(){
			if(this.take_observation){
				this.rt.treeg.style(&quot;opacity&quot;,0.2)
				this.take_observation = false;
				if(this.t% 5 ==1){
					document.getElementById(&quot;race_track_particle_likelihood&quot;).style.display=&quot;none&quot;;
					document.getElementById(&quot;race_track_particle_timestep&quot;).style.display=&quot;block&quot;;
					this.t = 4;
				}
			}else{
				this.rt.treeg.style(&quot;opacity&quot;,1.0)
				this.take_observation = true;
			}
        	
        }

		this.rt.tree_click = this.toogle_observation.bind(this)




	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;


		for (var i=0; i&lt;this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
		}
		


		if(this.t % 5 == 0){
			this.rc.step(scene.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById(&quot;race_track_particle_predict&quot;).style.display=&quot;block&quot;;
		}if(this.t % 5 == 1){
	    	
			//this.rt.update_strip(&quot;outer&quot;, get_system_dist_normalized(scene.rc, scene.rt, scene.rc.state, scene.rc.current_input));

			this.pf.predict(this.last_input);
			if(this.take_observation){
				document.getElementById(&quot;race_track_particle_likelihood&quot;).style.display=&quot;block&quot;;
			}else{
				document.getElementById(&quot;race_track_particle_timestep&quot;).style.display=&quot;block&quot;;
				this.t=4;
			}
			
		}else if(this.t % 5 == 2){
			this.rt.show_strip(&quot;inner&quot;);
			this.output = this.rc.output_dist_sample(0);
			this.rt.update_strip(&quot;inner&quot;, get_output_dist_normalized_from_distance(this.rc, this.rt, this.output));
			document.getElementById(&quot;race_track_particle_update&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 5 == 3){
	    	this.pf.update(this.output, 0);
	    	document.getElementById(&quot;race_track_particle_resampling&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 5 == 4){
			this.rt.hide_strip(&quot;inner&quot;);
			this.pf.ancestor_sampling();
			document.getElementById(&quot;race_track_particle_timestep&quot;).style.display=&quot;block&quot;;
		}



	}.bind(n_scene);

	scenes_name[&quot;race_track_particle&quot;] = n_scene;
	scenes.push(n_scene);
&lt;/script&gt;

&lt;svg id=&quot;race_track_particle&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;div id=&quot;race_track_particle_timestep&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_particle'].rc.current_input=0;scenes_name['race_track_particle'].step();&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_particle'].rc.current_input=1;scenes_name['race_track_particle'].step();&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_particle'].rc.current_input=2;scenes_name['race_track_particle'].step();&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_particle_predict&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_particle'].step();&quot;&gt;Predict step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_particle_likelihood&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_particle'].step();&quot;&gt;Observe&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_particle_update&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_particle'].step();&quot;&gt;Update step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_particle_resampling&quot; class=&quot;button_set&quot; onclick=&quot;scenes_name['race_track_particle'].step();&quot;&gt;
&lt;div class=&quot;bt1  bt&quot;&gt;Resampling&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;On the outside of the race track, you will notice uniformly distributed blue dots. These dots are our particles and, therefore, represent a set of hypotheses of the position of the race car. By pressing the &lt;strong&gt;OBSERVE&lt;/strong&gt; button two things will happen: first, we will take a measurement to the distance of the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the &lt;strong&gt;UPDATE STEP&lt;/strong&gt; button, we will perform our update step. The size of the blue dots will change according to their weighting. By pressing the &lt;strong&gt;RESAMPLING&lt;/strong&gt; button, we are performing the resampling step. As a result, you will notice that only the dots with high weights remains. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the &lt;strong&gt;PREDICT STEP&lt;/strong&gt; button. You will see that the dots will change accordingly to your chosen action. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.&lt;/p&gt;

&lt;p&gt;If you want to reset the environment, just press the reset button in the bottom left corner or press the &lt;strong&gt;R&lt;/strong&gt; button on your keyboard.
As before you can control the car by using your keyboard: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward) or the buttons below the race track.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example.</summary></entry><entry><title type="html">Nonlinear filtering: Unscented Kalman filter</title><link href="https://martinseilair.github.io/jekyll/update/2018/11/07/nf-ukf.html" rel="alternate" type="text/html" title="Nonlinear filtering: Unscented Kalman filter" /><published>2018-11-07T17:04:07+09:00</published><updated>2018-11-07T17:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/11/07/nf-ukf</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/11/07/nf-ukf.html">&lt;p&gt;The unscented Kalman filter describes another method for approximating the process of non-linear Bayes filtering. In this article, we will derive the corresponding equations directly from the general Bayes filter. Furthermore, we will get to know a different way to think about the unscented transform.  &lt;!--more--&gt; If you haven’t read the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-intro.html&quot;&gt;introduction&lt;/a&gt;, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind the unscented Kalman filter.&lt;/p&gt;

&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://martinseilair.github.io/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/ukf.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;unscented Kalman filter&lt;/strong&gt; approximates the Bayes filter by approximating the system and observation equations locally. A set of probe points is used to infer the local behavior of the function around the current estimate.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;When I first learned about the unscented Kalman filter, it seemed that it was not derived from the general Bayes filter&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Bayes filter} \to \text{nonlinear models} \to \text{unscented Kalman filter,}&lt;/script&gt;

&lt;p&gt;but rather directly from the Kalman filter&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{Kalman filter} \to \text{nonlinear models} \to \text{unscented Kalman filter}.&lt;/script&gt;

&lt;p&gt;In this post, I want to explore how to derive the unscented Kalman filter from general Bayes filter directly. But there is one catch, to obtain the equations of the unscented Kalman filter, you have to do some steps that seem hard to justify. It is totally possible, that there are deep thoughts behind it, but my limited scope is preventing me to identify them. Nonetheless, I will start to derive a different version of the unscented Kalman filter that is in some regards more principled. In the end, I will adjust this version to match the &lt;em&gt;normal&lt;/em&gt; unscented Kalman filter.&lt;/p&gt;

&lt;p&gt;Let’s start with the derivation!&lt;/p&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;
&lt;p&gt;First of all, if we want to apply the unscented Kalman filter, we assume that we have nonlinear models with additive noise&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &amp;= \mathcal{N}(x_{t+1}|f(x_t, u_t), Q_t) \\
p(y_t|x_t) &amp;=  \mathcal{N}(y_t|h(x_t), R_t).
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Unlike the extended Kalman filter, which is based on linearization, the unscented Kalman filter is based around the &lt;a href=&quot;https://en.wikipedia.org/wiki/Unscented_transform&quot;&gt;unscented transform&lt;/a&gt;. In simplest terms, if you want to transform a Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x) = \mathcal{N}(y|\mu, \Sigma)&lt;/script&gt;

&lt;p&gt;through a nonlinear function, it will give you an estimate of the resulting mean and covariance.&lt;/p&gt;

&lt;p&gt;For the derivation of the equations of unscented Kalman filter, I will state the unscented transform in a slightly different form, which I will call the joint probability interpretation of the unscented transform.&lt;/p&gt;

&lt;div class=&quot;extra_box&quot;&gt;

  &lt;h1 id=&quot;joint-probability-interpretation-of-the-unscented-transform&quot;&gt;Joint probability interpretation of the unscented transform&lt;/h1&gt;

  &lt;p&gt;In this section, we explore an interpretation of the unscented transform in terms of the joint probability. We are in a setting with a Gaussian conditional distribution with nonlinear mean and constant variance&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x) = \mathcal{N}(y|f(x),B)&lt;/script&gt;

  &lt;p&gt;and a Gaussian prior \(p(x) = \mathcal{N}(y|\mu, \Sigma) \).&lt;/p&gt;

  &lt;p&gt;Conceptually, we will first replace the prior with a surrogate distribution \(\hat{p}(x)\). Subsequently, we will approximate the joint distribution of the true conditional distribution \(p(y|x)\) and the surrogate distribution \(\hat{p}(x)\) with a Gaussian distribution \( \hat{p}(x,y) = \mathcal{N}(x,y|\hat{\mu},\hat{\Sigma}).\)&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Surrogate distribution&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;The first step is to replace the Gaussian distribution \( p(x)  \) with a surrogate distribution  \(\hat{p}(x)\) that has the same mean and variance. For this surrogate distribution, we choose a mix of weighted &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta functions&lt;/a&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x) = \sum_{i} w^i \delta_{\chi^i}(x),&lt;/script&gt;

  &lt;p&gt;where \(\delta_{\chi^i}(x)\) is a shorthand for \(\delta(x-\chi^i)\) and the weights are summing to 1:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i} w^i = 1.&lt;/script&gt;

  &lt;p&gt;The shifting parameter of the Dirac delta functions \(\chi^i\) are called &lt;em&gt;sigma points&lt;/em&gt; and are chosen to correspond to&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\chi^0 &amp;= \mu \\
\chi^i &amp;= \mu + a_iL_i  &amp;i=1,\ldots,n \\
\chi^i &amp;= \mu - a_{i-n}L_{i-n}  &amp;i=n+1, \ldots, 2n
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;where \(L_i\) describes the \(i\)th column of the Cholesky decomposition \(\Sigma = LL^T\) and \(n\) is the dimension of the multivariate Gaussian distribution \(p(x)\). The parameters \(a_i\) are arbitrary and can be chosen by the designer. Because of the symmetry of the sigma points, the weights will be symmetric as well&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 w^i &amp;= w^{i+n} &amp; i=1,\ldots,n. \\
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;Let’s answer the question of how we have to choose the weights \(w^i\), to obtain the desired property&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mu &amp;\stackrel{!}{=}\sum_{i} w^i \chi^i \\
\Sigma &amp;\stackrel{!}{=} \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T.\\
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;We start by plugging our sigma points \(\chi^i\) into the mean&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\sum_{i} w^i \chi^i &amp;= w^0 \mu + \sum_{i=1}^{n}w^i(\mu + a_iL_i) + \sum_{i=n+1}^{2n} w^i(\mu + a_{i-n}L_{i-n}) \\
&amp;= \mu\sum_{i=0}^{2n}w^i + \sum_{i=1}^{n}w^ia_iL_i - \sum_{i=1}^{n} w^ia_iL_i \\
&amp;=\mu
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and notice, that this property is independent of the weights \(w^i\).&lt;/p&gt;

  &lt;p&gt;Let’s move to the variance and see how we have to choose the weights \(w^i\) to match&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma \stackrel{!}{=} \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T .&lt;/script&gt;

  &lt;p&gt;We start by plugging the corresponding sigma points \(\chi^i\) into the variance&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 \Sigma &amp;\stackrel{!}{=} \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
&amp;= w^0 (\chi^0-\mu)(\chi^0-\mu)^T + \sum_{i=1}^{n} w^i (\chi^i-\mu)(\chi^i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
 &amp;= w^0 (\mu-\mu)(\mu-\mu)^T + \sum_{i=1}^{n} w^i (\mu + a_iL_i-\mu)(\mu + a_iL_i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\mu - a_{i-n}L_{i-n}-\mu)(\mu - a_{i-n}L_{i-n}-\mu)^T .\\
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;We note, that all occurences of the mean are canceled out. In particular, the first term corresponding to the central sigma point \(\chi^0\) is vanishing completely. Therefore, the corresponding weight plays no direct role for the mean &lt;em&gt;and&lt;/em&gt; variance. By using the symmetry properties we finally arrive at&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp;= \sum_{i=1}^{n} w^i (a_iL_i)(a_iL_i)^T + \sum_{i=n+1}^{2n} w^i (- a_{i-n}L_{i-n})(- a_{i-n}L_{i-n})^T \\
&amp;= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=n+1}^{2n} w^i a_i^2 L_{i-n}L_{i-n}^T \\
&amp;= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^{i+n} a_{i+n}^2 L_{i}L_{i}^T \\
&amp;= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^i a_i^2 L_{i}L_{i}^T \\
&amp;= \sum_{i=1}^{n} 2w^i a_i^2 L_iL_i^T. \\
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;We know, that the variance can be expressed by&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma = LL^T = \sum_{i=1}^{n} L_iL_i^T.&lt;/script&gt;

  &lt;p&gt;By a comparison of coefficients, we notice that the property&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;2w^i a_i^2 = 1&lt;/script&gt;

  &lt;p&gt;has to be satisfied. It follows that the weights should be \(w^i = \frac{1}{2a_i^2} \) for \(i=1,\ldots,n\).&lt;/p&gt;

  &lt;p&gt;Furthermore, by knowing that all weights have to sum to \(1\), it follows that the weight of the central sigma point \(\chi^0\) has to be&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;w^0  = 1 - 2\sum_{i=1}^{n}\frac{1}{2a_i^2}.&lt;/script&gt;

  &lt;p&gt;We finally found our surrogate distribution&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x) = \sum_{i} w^i \delta_{\chi^i}(x)&lt;/script&gt;

  &lt;p&gt;with the correct mean \(\mu\) and variance \(\Sigma\).&lt;/p&gt;

  &lt;p&gt;&lt;strong&gt;Gaussian approximation of the joint probability&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;Let’s look at the second part: We want to approximate the joint distribution of the true conditional distribution \(p(y|x)\) and the surrogate distribution \(\hat{p}(x)\) with a Gaussian distribution \( \hat{p}(x,y) = \mathcal{N}(x,y|\hat{\mu},\hat{\Sigma}).\)&lt;/p&gt;

  &lt;p&gt;We can write the joint probability of \(p(y|x)\) and \(\hat{p}(x)\) as&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x,y) = p(y|x)\hat{p}(x) = \mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x).&lt;/script&gt;

  &lt;p&gt;This joint probability will look like weighted slices of the conditional probability \(p(y|x) \).&lt;/p&gt;

  &lt;p&gt;The next step is to approximate this weird looking distribution with a Gaussian distribution. But how should we choose the parameter of this Gaussian joint distribution? The answer is simple: We are calculating the mean and the variance of the joint probability, which will define the mean and variance of our joint Gaussian distribution. Fair enough! So, let’s dive directly into the calculation of the mean. We are starting directly from the definition of the expectation&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu} = \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy&lt;/script&gt;

  &lt;p&gt;and plug in the corresponding distributions to obtain&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu} = \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy.&lt;/script&gt;

  &lt;p&gt;We interchange the sum with the integrals&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu} = \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy&lt;/script&gt;

  &lt;p&gt;and use the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function#Translation&quot;&gt;sifting property&lt;/a&gt; of the Dirac delta function to obtain&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\mu}= \sum_{i} w^i \int\limits_{x}\mathcal{N}(y|f(\chi^i),B)\begin{pmatrix}
    \chi^i \\
    y \\
    \end{pmatrix} \,dy.&lt;/script&gt;

  &lt;p&gt;We can simplify this expression even further to&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat{\mu}&amp;= \sum_{i} w^i \begin{pmatrix}
    \chi^i \\
    f(\chi^i) \\
    \end{pmatrix} =  \begin{pmatrix}
    \sum_{i} w^i \chi^i \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}\\
&amp;=  \begin{pmatrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}. 
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;We are done! To finish our approximation we have to calculate the covariance of the joint probability. Let’s start again from the definition of the variance&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat{\Sigma} &amp;= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x-\hat{\mu}_x  \\
    y-\hat{\mu}_y \\
    \end{pmatrix}\begin{pmatrix}
    x-\mu  \\
    y-\hat{\mu} \\
    \end{pmatrix}^T \,dx\,dy \\ 
&amp;= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T &amp; (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T &amp; (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy \\ 
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and plug in our distributions to obtain&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat{\Sigma}&amp;= \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T &amp; (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T &amp; (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy. \\ 
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;Again we are rearranging sum and integrals&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat{\Sigma}&amp;= \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T &amp; (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T &amp; (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy. \\ 

\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and use the sifting property to finally obtain&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat{\Sigma}&amp;= \sum_{i} w^i \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T &amp; (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &amp; (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dy \\ 
&amp;= \sum_{i} w^i \begin{pmatrix}
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T \,dy&amp; \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T\,dy\\
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T\,dy &amp; \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T\,dy \\
    \end{pmatrix}  \\ 
&amp;= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T&amp; (\chi^i-\hat{\mu}_x)\left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right]^T\\
    \left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right](\chi^i-\hat{\mu}_x)^T &amp;  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&amp;= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T&amp; (\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    (f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &amp;  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&amp;=  \begin{pmatrix}
    \sum_{i} w^i(\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T&amp; \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &amp;  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&amp;=  \begin{pmatrix}
    \Sigma &amp; \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &amp;  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  .\\ 
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;Now we have also a nice expression for the variance of the approximated joint probability \(\hat{p}(x,y)\).&lt;/p&gt;

  &lt;p&gt;Let’s summarize our result&lt;/p&gt;
  &lt;div class=&quot;important_box&quot;&gt;
    &lt;h1&gt;Joint probability interpretation of the unscented transform&lt;/h1&gt;

    &lt;p&gt;Given the Gaussian conditional distribution with nonlinear mean and constant variance&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x) = \mathcal{N}(y|f(x),B)&lt;/script&gt;

    &lt;p&gt;and a Gaussian prior \(p(x) = \mathcal{N}(y|\mu, \Sigma) \). We can approximate the joint probability \(p(x,y)\) by a Gaussian joint probability \(\hat{p}(x,y)\), where the prior \(p(x)\) is replaced by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x) = \sum_{i} w^i \delta_{\chi^i}(x),&lt;/script&gt;

    &lt;p&gt;with the weights \(w^i\) are defined by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
w^i &amp;= \frac{1}{2a_i^2} &amp; i=1,\ldots,n \\
w^i &amp;= w^{i+n} &amp; i=1,\ldots,n. \\
w^0 &amp;= 1 - 2\sum_{i=1}^{n}\frac{1}{2a_i^2}. &amp;
\end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;and the with sigma points \(\chi^i\) by&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\chi^0 &amp;= \mu \\
\chi^i &amp;= \mu + a_iL_i  &amp;i=1,\ldots,n \\
\chi^i &amp;= \mu - a_{i-n}L_{i-n}  &amp;i=n+1, \ldots, 2n.
\end{align} %]]&gt;&lt;/script&gt;

    &lt;p&gt;The resulting Gaussian approximation has the following form&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\hat{p}(x,y) = \mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{matrix},\begin{matrix}
    \Sigma                                                          &amp;   \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T      &amp;   \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{matrix}\right). %]]&gt;&lt;/script&gt;

  &lt;/div&gt;

&lt;/div&gt;

&lt;p&gt;Please be aware, that this is &lt;strong&gt;not&lt;/strong&gt; the definition of the unscented transform. But it will be easier to work with this form for now and identify the normal unscented transform as a special case.&lt;/p&gt;

&lt;p&gt;Ok, now we know how to approximate the joint probability of Gaussian distributions with nonlinear mean and a Gaussian prior. But what can we do with it?&lt;/p&gt;

&lt;p&gt;If we look closely at the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t}&lt;/script&gt;

&lt;p&gt;of the general Bayes filter, we can find the joint probability of the form we just discussed. For the prediction step we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_x p(y|x)p(x)\,dx \to \int_x p(x,y)\,dx&lt;/script&gt;

&lt;p&gt;and for the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(y|x)p(x)}{\int_x p(y|x)p(x)\,dx} \to \frac{p(x,y)}{\int_x p(x,y)\,dx}.&lt;/script&gt;

&lt;p&gt;That is pretty nice! We can approximate joint probabilities and the equations of the Bayes filter can take joint probabilities as input. So don’t waste time and start directly with the prediction step.&lt;/p&gt;

&lt;h1 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h1&gt;

&lt;p&gt;We want to calculate 
&lt;script type=&quot;math/tex&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}.&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;But instead of using the true joint probability \(p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1})\) we will use our approximation \(\hat{p}(x_{t+1},x_{t}|y_{0:t},u_{0:t})\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} \hat{p}(x_{t+1},x_{t}|y_{0:t},u_{0:t}) dx_{t}.&lt;/script&gt;

&lt;p&gt;Fortunately, marginalizing over a joint Gaussian distribution is very simple. You just have to discard the dimensions you are marginalizing over:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\int\limits_{x}\mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    a  \\
    b \\
    \end{matrix}, \begin{matrix}
    A &amp; C \\
    C^T &amp; B\\
    \end{matrix} \right) \,dx = \mathcal{N}(y|b,B). %]]&gt;&lt;/script&gt;

&lt;p&gt;Therefore, we only have to calculate the mean and covariance of the remaining part. Our predicted state estimate will be a Gaussian distribution with parameters&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat x_{t+1|t} &amp;=  \sum_{i} w^i f(\chi^i,u_t) \\
P_{t+1|t} &amp;= \sum_{i} w^i\left(f(\chi^i,u_t)-\sum_{i} w^i f(\chi^i,u_t)\right)\left(f(\chi^i,u_t)-\sum_{i} w^i f(\chi^i,u_t)\right)^T + Q_t.
\end{align} %]]&gt;&lt;/script&gt;

&lt;h1 id=&quot;update-step&quot;&gt;Update step&lt;/h1&gt;

&lt;p&gt;Now we will look at the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\,dx_t}&lt;/script&gt;

&lt;p&gt;and replace the true joint probability \(p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\) with our approximation \(\hat{p}(y_t,x_t|y_{0:t-1},u_{0:t-1})\) and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{\hat{p}(y_t,x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t} \hat{p}(y_t,x_t|y_{0:t-1},u_{0:t-1})\,dx_t}.&lt;/script&gt;

&lt;p&gt;Now we have to options: Either we just simplify these equations to obtain the final update equation of the unscented Kalman filter or we can be smart and note, that we would compute nothing else, but the update step of the Kalman filter. Let’s take that second option.&lt;/p&gt;

&lt;p&gt;The joint probability in the linear Gaussian case (where we already have the equations of the Kalman filter) can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}\left(\begin{matrix}x_t \\y_t\end{matrix}\middle|\begin{matrix}\hat x_{t|t-1}\\C_t\hat x_{t|t-1} \end{matrix},\begin{matrix}P_{t|t-1} &amp; P_{t|t-1}^TC_t^T\\C_tP_{t|t-1} &amp; R_t + C_tP_{t|t-1}^TC_t^T\end{matrix}\right) . %]]&gt;&lt;/script&gt;

&lt;p&gt;The only thing we have to do is a coefficient comparison with the joint Gaussian of our estimate:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\hat{p}(y_t,x_t|y_{0:t-1},u_{0:t-1}) = \mathcal{N}\left(\begin{matrix}
    x_t  \\
    y_t \\
    \end{matrix}\middle|\begin{matrix}
    \hat x_{t|t-1} \\
    \sum_{i} w^i h(\chi^i) \\
    \end{matrix},\begin{matrix}
    P_{t|t-1}                                                                                       &amp;  \sum_{i} w^i\left(\chi^i-\hat x_{t|t-1}\right)\left(h(\chi^i) -\sum_{i} w^i h(\chi^i)\right)^T\\
    \sum_{i} w^i\left(h(\chi^i) -\sum_{i} w^i h(\chi^i)\right)\left(\chi^i-\hat x_{t|t-1}\right)^T  &amp;  \sum_{i} w^i\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)^T + R_t \\
    \end{matrix}\right). %]]&gt;&lt;/script&gt;

&lt;p&gt;We identify the following correspondences:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
C_t\hat x_{t|t-1} &amp;\,\widehat{=}\, \sum_{i} w^i h(\chi^i) \\
C_tP_{t|t-1}&amp;\,\widehat{=}\, \sum_{i} w^i\left(h(\chi^i) -\sum_{i} w^i h(\chi^i)\right)\left(\chi^i-\hat x_{t|t-1}\right)^T \\
P_{t|t-1}^TC_t^T&amp;\,\widehat{=}\, \sum_{i} w^i\left(\chi^i-\hat x_{t|t-1}\right)\left(h(\chi^i) -\sum_{i} w^i h(\chi^i)\right)^T \\
C_tP_{t|t-1}^TC_t^T&amp;\,\widehat{=}\, \sum_{i} w^i\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)^T.
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;We are done and summarize our results as follows.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Equations of the unscented Kalman filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the unscented Kalman filter consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat x_{t+1|t} &amp;=  \sum_{i} w^i f(\chi^i,u_t) \\
P_{t+1|t} &amp;= \sum_{i} w^i(f(\chi^i,u_t)-\hat{\mu}_y)(f(\chi^i,u_t)-\hat{\mu}_y)^T + Q_t.
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_t &amp;= y_{t}-\sum_{i} w^i h(\chi^i)\\
S_t &amp;= R_t + \sum_{i} w^i\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)\left(h(\chi^i)-\sum_{i} w^i h(\chi^i)\right)^T\\
K_t &amp;= \sum_{i} w^i\left(\chi^i-\hat x_{t|t-1}\right)\left(h(\chi^i) -\sum_{i} w^i h(\chi^i)\right)^TS_t^{-1} \\
\hat x_{t|t} &amp;= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &amp;= P_{t|t-1} -K_tS_tK_t^T.
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;

&lt;h1 id=&quot;unscented-kalman-filter&quot;&gt;Unscented Kalman filter&lt;/h1&gt;

&lt;p&gt;To obtain the equations of the unscented Kalman filter that are normally used, we have to adjust the result of our derivation in two aspects.
The first aspect is, that we are choosing specific values for the parameters \(a_i\) corresponding to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;a_i = \sqrt{n+\lambda}.&lt;/script&gt;

&lt;p&gt;The weights for \(i=1,\ldots,2n \) are set to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w^i =  \frac{\lambda}{2(n + \lambda)}.&lt;/script&gt;

&lt;p&gt;For the center sigma point \(\chi^0\), we will use a different weight \(w_s^0\) and \(w_c^0\) for the calculation of the mean and the variance respectively defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
w_s^0  &amp;= \frac{\lambda}{n + \lambda} \\
w_c^0  &amp;= \frac{\lambda}{n + \lambda} + (1-\alpha^2 + \beta),
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;where the parameter \(\lambda\) is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lambda = \alpha^2(n+\kappa) - n.&lt;/script&gt;

&lt;p&gt;A typical recommendation is to use \(\kappa = 0\), \(\alpha = 10^{-3}\) and \(\beta=2\). We noted above, that the covariance of our surrogate prior \(\hat{p}(x)\) is not depending on the weight \(w^0\). But please be aware, that it will certainly impact the rest of the joint probability distribution.&lt;/p&gt;

&lt;p&gt;The second aspect we have to change (or at least could change), is that we don’t have to recalculate our sigma points after the prediction step. We can simply use the transformed sigma points \(f(\chi^i)\) as our new sigma points.&lt;/p&gt;

&lt;p&gt;Enough of the dry theory! Let’s play around with the unscented Kalman filter in our race track example.&lt;/p&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;svg id=&quot;race_track_mar_loc&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;
&lt;script&gt;


    n_scene = load_race_track(&quot;race_track_mar_loc&quot;,&quot;https://martinseilair.github.io&quot;);
    n_scene.mode = 2;
    n_scene.filter = &quot;&quot;;
    n_scene.dur=slow_dur;
    // define particle filter 

    n_scene.auto_start = false;

    n_scene.t = 1;

    n_scene.ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];

    n_scene.take_observation = true;

    n_scene.loaded = function(){

        var outer_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style(&quot;background-color&quot;)), d3.rgb('#006eff'), d3.rgb('#00028e')]);
        var inner_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style(&quot;background-color&quot;)), d3.rgb('#ff834d'), d3.rgb('#8e3323')]);


        this.ukf = init_ukf_1D(this.rc, this.rc.state, 3*this.rc.sigma_s_no_cache(this.rc.state));
        this.rt.init_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.rc.state), inner_color, 60);
        this.rt.init_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ukf.posterior_mu, this.ukf.posterior_sigma, this.rt), outer_color, 60);



        document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
        this.rt.hide_strip(&quot;inner&quot;);


        this.restart = function(){
            for (var i=0; i&lt;this.ids.length;i++){

                document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
            }
            document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
            this.rc.reset();
            this.t = 1;

            

            //this.bf.reset();
            this.ukf.reset(this.rc.state, this.rc.sigma_o_no_cache(this.rc.state))
            this.rt.hide_strip(&quot;inner&quot;);
            this.rt.show_strip(&quot;outer&quot;);
            this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ukf.posterior_mu, this.ukf.posterior_sigma, this.rt));
            this.rt.treeg.style(&quot;opacity&quot;,1.0)
            this.take_observation = true;
        }


        this.rt.set_restart_button(this.restart.bind(this))

        this.toogle_observation = function(){
            if(this.take_observation){
                this.rt.treeg.style(&quot;opacity&quot;,0.2)
                this.take_observation = false;
                if(this.t% 5 ==1){
                    document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;none&quot;;
                    document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
                    this.t = 3;
                }
            }else{
                this.rt.treeg.style(&quot;opacity&quot;,1.0)
                this.take_observation = true;
            }
            
        }

        this.rt.tree_click = this.toogle_observation.bind(this)


    }.bind(n_scene)


    n_scene.step = function(){
        this.t++;
        for (var i=0; i&lt;this.ids.length;i++){

            document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
        }


        if(this.t % 4 == 0){
            //CHOOSE ACTION
            this.rc.step(this.rc.current_input);
            this.last_input = this.rc.current_input;
            document.getElementById(&quot;race_track_mar_loc_predict&quot;).style.display=&quot;block&quot;;
            this.rt.hide_strip(&quot;inner&quot;);
        }else if(this.t % 4 == 1){
            // PREDICT
            this.ukf.predict(this.last_input);

            // trim ukf posterior

            if(this.ukf.posterior_mu&lt;0){
                this.ukf.posterior_mu+=this.rt.track_length;
            }
            this.ukf.posterior_mu = this.ukf.posterior_mu % this.rt.track_length;

            this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ukf.posterior_mu, this.ukf.posterior_sigma, this.rt));
            if(this.take_observation){
                document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
            }else{
                document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
                this.t=3;
            }
        }else if(this.t % 4 == 2){
            // OBSERVE
            this.rt.show_strip(&quot;inner&quot;);
            this.output = scene.rc.output_dist_sample(0);
            var likelihood = this.ukf.get_likelihood(this.output,this.ukf.posterior_mu)
            this.rt.update_strip(&quot;inner&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, likelihood.mu, likelihood.sigma, this.rt));

            document.getElementById(&quot;race_track_mar_loc_update&quot;).style.display=&quot;block&quot;;
        }else if(this.t % 4 == 3){
            // UPDATE
            this.ukf.update(this.output);

            this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ukf.posterior_mu, this.ukf.posterior_sigma, this.rt));

            document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
        }

    }.bind(n_scene);

    scenes_name[&quot;race_track_mar_loc&quot;] = n_scene;
    scenes.push(n_scene);

&lt;/script&gt;

&lt;div id=&quot;race_track_mar_loc_timestep&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_predict&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Predict step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_likelihood&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Observe&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_update&quot; class=&quot;button_set&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;
&lt;div class=&quot;bt1  bt&quot;&gt;Update step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;On the outside of the race track, you will notice a blue colored strip. This strip represents our current posterior of the current position of the race car. We will start with a Gaussian prior distribution around the true mean. By pressing the &lt;strong&gt;OBSERVE&lt;/strong&gt; button two things will happen: first, we will take a measurement of the distance to the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the &lt;strong&gt;UPDATE STEP&lt;/strong&gt; button, we will perform our update step and show the resulting posterior at the outer strip. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the &lt;strong&gt;PREDICT STEP&lt;/strong&gt; button. You will see that the outer strip will change accordingly. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.&lt;/p&gt;

&lt;p&gt;What if our distance meter is not working anymore? By either clicking on the tree or pressing the &lt;strong&gt;W&lt;/strong&gt; button on your keyboard, you can turn off your measurement device. Therefore, the observation and update step will be skipped. The tree will become opaque, if your measurement device is turned off.&lt;/p&gt;

&lt;p&gt;If you want to reset the environment, just press the reset button in the bottom left corner or press the &lt;strong&gt;R&lt;/strong&gt; button on your keyboard.
As before you can control the car by using your keyboard: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward) or the buttons below the race track.&lt;/p&gt;

&lt;p&gt;If you still didn’t have enough of nonlinear filtering, you should check out the next article in this series about the derivation of the &lt;a href=&quot;/jekyll/update/2018/11/08/nf-particle.html&quot;&gt;particle filter&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;</content><author><name></name></author><summary type="html">The unscented Kalman filter describes another method for approximating the process of non-linear Bayes filtering. In this article, we will derive the corresponding equations directly from the general Bayes filter. Furthermore, we will get to know a different way to think about the unscented transform.</summary></entry><entry><title type="html">Observability: A Bayesian perspective</title><link href="https://martinseilair.github.io/jekyll/update/2018/11/07/observability.html" rel="alternate" type="text/html" title="Observability: A Bayesian perspective" /><published>2018-11-07T01:04:07+09:00</published><updated>2018-11-07T01:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/11/07/observability</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/11/07/observability.html">&lt;p&gt;Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability.
&lt;!--more--&gt;
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;
  &lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Let’s begin by stating the definition of &lt;a href=&quot;https://en.wikipedia.org/wiki/Observability&quot;&gt;observability&lt;/a&gt; from classical control theory.&lt;/p&gt;
&lt;div class=&quot;important_box&quot;&gt;
  Formally, a system is said to be observable, if for any possible sequence of state and control vectors (the latter being variables whose values one can choose), the current state (the values of the underlying dynamically evolving variables) can be determined in finite time using only the outputs.
&lt;/div&gt;

&lt;p&gt;We can easily translate this definition into the language of Bayesian inference:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  A system is said to be observable if for any possible initial state and sequence of control vectors, the probability mass of the posterior of the current state will collapse into a single point in finite time.
&lt;/div&gt;

&lt;p&gt;Normally, we are using the idea of observability in the setting of deterministic time-invariant linear state space models, which are defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= Ax_t + B u_t \\ 
y_t &amp;= Cx_t  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A\), input matrix \(B\) and output matrix \(C\).&lt;/p&gt;

&lt;p&gt;Based on the methods shown in the last post about &lt;a href=&quot;/jekyll/update/2018/11/06/linalg-gaussian.html&quot;&gt;linear algebra with Gauss and Bayes&lt;/a&gt;, we can reformulate these deterministic equations with Gaussian distributions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|Ax_t + Bu_t, \delta I)&lt;/script&gt;

&lt;p&gt;and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(y_t|Cx_t, \delta I)&lt;/script&gt;

&lt;p&gt;where \(\delta \to 0\).&lt;/p&gt;

&lt;p&gt;Now that we arrived at a probabilistic description, we can use Bayesian inference to infer the current state \(x_t\). In particular, we are interested in the uncertainty of our estimate of the current state: Our system will be observable if the covariance of the estimate will go to zero.&lt;/p&gt;

&lt;h1 id=&quot;derivation&quot;&gt;Derivation&lt;/h1&gt;

&lt;p&gt;First of all, we are defining a Gaussian prior distribution of the initial state \(x_0\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_0) = \mathcal{N}(x_0|0,I).&lt;/script&gt;

&lt;p&gt;The choice of the mean and covariance are actually arbitrary, as long as the covariance is positive definite.&lt;/p&gt;

&lt;p&gt;Now let’s plug our distributions into the equations of the Bayes filter, which are described by the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

&lt;p&gt;Fortunately, we already know how to do inference in linear Gaussian state space models. We can simply use the equations of the Kalman filter and obtain the following equations for the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;=  A \hat x_{t|t} + Bu_t \\ 
P_{t+1|t} &amp;= \delta I + A P_{t|t} A^T  \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t|t} &amp;= \hat x_{t|t-1} + P_{t|t-1}C^T(\delta I + CP_{t|t-1}C^T)^{-1}(y_{t}-C\hat x_{t|t-1}) \\ 
P_{t|t} &amp;= P_{t|t-1} - P_{t|t-1}C^T (\delta I + CP_{t|t-1}C^T)^{-1}CP_{t|t-1} .\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;If this was too fast, please check out the earlier blog post on &lt;a href=&quot;/jekyll/update/2018/10/10/kalman_filter.html&quot;&gt;Kalman filtering&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Observability depends only on the covariance of the estimates \( P \). Therefore, the question of observability of a linear state space model is reduced to the question, if the equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
P_{t+1|t} &amp;= \delta I + A P_{t|t} A^T \\ 
P_{t|t} &amp;=(I-P_{t|t-1}C^T(\delta I + CP_{t|t-1}C^T)^{-1}C)P_{t|t-1}  
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;are going to transform an arbitrary positive definite initial covariance matrix \(P_0\) to 0.&lt;/p&gt;
&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;When we combine the prediction and update to a single equation&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t+1} = \delta I + A (I-P_{t}C^T(\delta I + CP_{t}C^T)^{-1}C)P_{t} A^T&lt;/script&gt;

  &lt;p&gt;and look very closely we can identify the &lt;a href=&quot;https://en.wikipedia.org/wiki/Algebraic_Riccati_equation#Context_of_the_discrete-time_algebraic_Riccati_equation&quot;&gt;discrete-time algebraic Ricatti equation&lt;/a&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t+1} = \delta I + AP_{t} A^T  - AP_{t}C^T(\delta I + CP_{t}C^T)^{-1}CP_{t} A^T.&lt;/script&gt;

&lt;/div&gt;
&lt;p&gt;Let’s try to interpret what our two equations are doing with the covariance estimate \(P\). As a mental model, it is helpful to imagine the particular covariance matrices as subspaces.
We begin with our prior variance \(P_0\). We have selected our prior variance in such a way, that it describes the entire state space.&lt;/p&gt;

&lt;p&gt;We are starting by taking an update step. The update step can be interpreted as calculating the intersection of the prior subspace and the subspace defined by all points \(x\) that map to the observed output \(y_0\). We will call this last subspace the &lt;em&gt;inverse subspace&lt;/em&gt;.
We took the intersection of the whole state space and &lt;em&gt;inverse subspace&lt;/em&gt;. As a result, our posterior will be simply the inverse subspace.
Let’s see what happens, if we take the prediction step. If we assume that \(A\) has full rank, the dimensionality of the subspace will remain the same. Depending on the matrix \(A\) two things can happen:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The transformation won’t change the subspace, but only the representation of the subspace. It is an &lt;a href=&quot;https://en.wikipedia.org/wiki/Invariant_subspace&quot;&gt;invariant subspace&lt;/a&gt; with respect to transformation \(A\).&lt;/li&gt;
  &lt;li&gt;The transformation is changing the subspace.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Depending on these two cases we will have two cases for the next update step:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The transformation didn’t change the subspace.&lt;/strong&gt; In this case, the update step would have no effect, because we are intersecting again with the &lt;em&gt;same&lt;/em&gt; inverse subspace. Formally, after the prediction step our posterior would still be the orthogonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Projectors&quot;&gt;projector&lt;/a&gt; onto the kernel of \(C\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{0|0} = I - C^+C.&lt;/script&gt;

&lt;p&gt;We know that \(C(I - C^+C) = 0\) and \((I - C^+C)C^T = 0\), therefore, our update step simplifies to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t|t} =P_{t|t-1}-\underbrace{P_{t|t-1}C^T}_{0}(\delta I + \underbrace{CP_{t|t-1}}_{0}C^T)^{-1}\underbrace{CP_{t|t-1}}_{0} =  P_{t|t-1}.&lt;/script&gt;

&lt;p&gt;It seems, that we can’t rid of this &lt;em&gt;unobservable&lt;/em&gt; subspace. Therefore, we have &lt;strong&gt;no&lt;/strong&gt; observability.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The transformation did change the subspace.&lt;/strong&gt; In this case, the intersection with the inverse subspace will again have an effect. The dimensionality of the posterior subspace will get smaller.&lt;/p&gt;

&lt;p&gt;We have to repeat the process of prediction and updating until the subspace of our posterior has either dimension zero or the prediction step is again not changing our subspace.
In the first case, we have no uncertainty: The system is observable. In the second case, we identified a invariant subspace. Therefore, the system is not observable.&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post, we looked at the concept of observability from a Bayesian standpoint. We found an intuitive way to reason about the effect of the update step and prediction step in terms of subspaces, described by covariance matrices.&lt;/p&gt;</content><author><name></name></author><summary type="html">Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability.</summary></entry><entry><title type="html">Linear algebra with Gauss and Bayes</title><link href="https://martinseilair.github.io/jekyll/update/2018/11/06/linalg-gaussian.html" rel="alternate" type="text/html" title="Linear algebra with Gauss and Bayes" /><published>2018-11-06T12:04:07+09:00</published><updated>2018-11-06T12:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/11/06/linalg-gaussian</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/11/06/linalg-gaussian.html">&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Linear_algebra&quot;&gt;Linear algebra&lt;/a&gt; is a wonderful field of mathematics with endless applications. Despite its obvious beauty, it can also be quite confusing. Especially, when it comes to subspaces, inverses and determinants. In this article, I want to present a different view on some aspects of linear algebra with the help of &lt;a href=&quot;https://en.wikipedia.org/wiki/Multivariate_normal_distribution&quot;&gt;Gaussian distributions&lt;/a&gt; and &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes theorem&lt;/a&gt;.
&lt;!--more--&gt;
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;
&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;
  &lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;
  &lt;script type=&quot;text/javascript&quot; src=&quot;https://martinseilair.github.io/assets/js/math.min.js&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This post is about the very basic equation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Ax = b,&lt;/script&gt;

&lt;p&gt;where \(A \in \mathbb{R}^{m\times n}\), \(x \in \mathbb{R}^{n}\) and \(b \in \mathbb{R}^{m}\). With the help of the Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|\mu, \Sigma),&lt;/script&gt;

&lt;p&gt;we can restate this equation in the language of probability theory.&lt;/p&gt;

&lt;h1 id=&quot;matrix-transformation&quot;&gt;Matrix transformation&lt;/h1&gt;
&lt;p&gt;The transformation of our vector \(x\) with matrix \(A\) becomes a marginalization&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_\hat{x} \mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x) \,d\hat{x}.&lt;/script&gt;

&lt;p&gt;Please be aware, that this formula is &lt;strong&gt;not&lt;/strong&gt; equivalent to our matrix product above. We introduced two new variables \(\Sigma_x\) and \(\Sigma_b\), which are representing the covariance matrices of the corresponding Gaussian distributions.
The two formulations will become equivalent if the covariance matrices will go to zero.
Let’s check if this is true!&lt;/p&gt;

&lt;p&gt;We can use the propagation formula&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ).&lt;/script&gt;

&lt;p&gt;from the &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Gaussian identities&lt;/a&gt; by Marc Toussaint to reformulate our marginalization.&lt;/p&gt;

&lt;p&gt;In our case, we will end up with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_\hat{x} \mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x) \,d\hat{x} = \mathcal{N}(b|Ax, \Sigma_b + A\Sigma_xA^T ).&lt;/script&gt;

&lt;p&gt;If we let \(\Sigma_x\) and \(\Sigma_b\) go to zero, the resulting covariance will go to zero as well. In the limit, the entire probability mass will be concentrated at our mean \(Ax\), which is exactly what we wanted to show.
Up until now, there is nothing fancy about this result. It’s just a weird way to write the matrix multiplication. But when we think about the inverse of the transformation \(Ax = b\), things will become more interesting.&lt;/p&gt;

&lt;h1 id=&quot;inverse&quot;&gt;Inverse&lt;/h1&gt;

&lt;p&gt;What does it mean to take the inverse transformation? What is the desired result? By taking the inverse operation, we are simply asking for the set of points \(x\) which would be transformed to \(b\). We normally express the inverse transformation as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x = A^{-1}b.&lt;/script&gt;

&lt;p&gt;But you have to be careful: This equations only holds, if the matrix \(A\) has full rank. In this case, there is exactly one point \(x\) that maps to \(b\). But we shouldn’t waste our time on special cases. Let’s directly look at the general case for an arbitrary matrix \(A\).&lt;/p&gt;

&lt;p&gt;We come back to our old friends Gauss and Bayes and try to formulate the inverse operation in terms of Gaussian distributions and Bayes theorem&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|y) = \frac{p(y|x)p(x)}{\int_x p(y|x)p(x) \,dx}.&lt;/script&gt;

&lt;p&gt;We insert our Gaussian distributions and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\hat{x}|b) =  \frac{\mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x)}{\int_\hat{x} \mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x) \,d\hat{x}}.&lt;/script&gt;

&lt;p&gt;To simplify this expression we can use equation 39 and 40 from the &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Gaussian identities&lt;/a&gt; and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x)}{\int_\hat{x} \mathcal{N}(b|A\hat{x}, \Sigma_b)\mathcal{N}(\hat{x}|x, \Sigma_x) \,d\hat{x}} = \mathcal{N}(\hat{x}|\mu, \Sigma)&lt;/script&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \mu &amp;= x + \Sigma_xA^T(\Sigma_b + A\Sigma_xA^T)^{-1}(b-Ax) \\ 
\Sigma &amp;= \Sigma_x - \Sigma_xA^T (\Sigma_b + A\Sigma_xA^T)^{-1}A\Sigma_x. \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;In this context, \(x\) and \(\Sigma_x\) are describing our prior belief about the set of points \(x\) that map to \(b\). We have no prior information about the inverse and could choose any prior that has probability mass at every point in the space.
We will set \(x=0\) and \(\Sigma_x = I\). Our equations will simplify to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \mu &amp;= A^T(\Sigma_b + AA^T)^{-1}b \\ 
\Sigma &amp;= I - A^T (\Sigma_b + AA^T)^{-1}A. \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;But what is with \(\Sigma_b\)? We want to calculate the inverse of the deterministic linear transformation. Therefore, \(\Sigma_b\) has to go to zero. We are doing this in a fancy way by defining&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Sigma_b = \delta I&lt;/script&gt;

&lt;p&gt;and letting the scalar \(\delta\) go to zero.&lt;/p&gt;

&lt;p&gt;The resulting formula will be&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} \mu &amp;= A^T(\delta I + AA^T)^{-1}b \\ 
\Sigma &amp;= I - A^T (\delta I + AA^T)^{-1}A \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with \(\delta \to 0 \).&lt;/p&gt;

&lt;p&gt;If we look closely we can identify the exact definition of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse&quot;&gt;pseudoinverse&lt;/a&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;A^+ = A^T(\delta I + AA^T)^{-1},&lt;/script&gt;

&lt;p&gt;where \(\delta \to 0\). In this particular form, we don’t we have to think about rank or invertibility: It is valid for any matrix!&lt;/p&gt;

&lt;p&gt;Ok, but what’s going with the covariance matrix \(\Sigma\)? It can be identified as the orthogonal &lt;a href=&quot;https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Projectors&quot;&gt;projector&lt;/a&gt; onto the kernel of \(A\)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;I - A^+A.&lt;/script&gt;

&lt;p&gt;Ok, nice! But what does it mean? The mean \(\mu\) and covariance \(\Sigma\) together are describing an affine linear space. The mean \(\mu\) can be interpreted as a translation vector to the linear subspace described by the covariance \(\Sigma\).&lt;/p&gt;

&lt;p&gt;The beautiful thing is, that we don’t have to trust the equations. We can simply plot&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(\hat{x}|A^+b, I - A^+A)&lt;/script&gt;

&lt;p&gt;with a very small \(\delta\) and &lt;em&gt;see&lt;/em&gt; the resulting subspace.&lt;/p&gt;

&lt;p&gt;But the niceness doesn’t stop here. We can imagine a setting, where we are not getting the whole vector \(b\) at once, but each dimension \(b_i\) separately. In this case, Bayes theorem tells us quite clearly what to do. We can update our belief of the inverse sequentially&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(\hat{x}|b) =  \frac{\mathcal{N}(b_1|A_1\hat{x}, \Sigma_{b_1})\,\ldots\,\mathcal{N}(b_m|A_m\hat{x}, \Sigma_{b_m})\mathcal{N}(\hat{x}|x, \Sigma_x)}{\int_\hat{x} \mathcal{N}(b_1|A_1\hat{x}, \Sigma_{b_1})\,\ldots\,\mathcal{N}(b_m|A_m\hat{x}, \Sigma_{b_m})\mathcal{N}(\hat{x}|x, \Sigma_x) \,d\hat{x}},&lt;/script&gt;

&lt;p&gt;where we assume that \(\Sigma_b\) is a diagonal matrix.&lt;/p&gt;

&lt;p&gt;We did not only obtain a nice way to describe inverses, but have found a general representation of arbitrary affine linear subspaces&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a, A(\delta)),&lt;/script&gt;

&lt;p&gt;with translation vector \(a\) and covariance matrix \(A(\delta)\), where \(\delta \to 0\).&lt;/p&gt;

&lt;h1 id=&quot;intersection-of-subspaces&quot;&gt;Intersection of subspaces&lt;/h1&gt;

&lt;p&gt;Now, we can ask what the intersection of two affine linear subspaces is. An intuitive example for this are two planes intersecting in a line. With our representation of affine linear subspaces, asking for the intersection is easy! We just have to multiply the Gaussian distributions and we are done. In the &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Gaussian identities&lt;/a&gt;, we find the formula for the product&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a, A) \mathcal{N}(x|b, B) = \mathcal{N}(x|B(A+B)^{-1}a + A(A+B)^{-1}b, A(A+B)^{-1}B)\mathcal{N}(a|b, A+B).&lt;/script&gt;

&lt;p&gt;Therefore, the subspace of the intersection can be described by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mu &amp;= B(\delta)(A(\delta)+B(\delta))^{-1}a + A(\delta)(A(\delta)+B(\delta))^{-1}b \\
\Sigma &amp;=  A(\delta)(A(\delta)+B(\delta))^{-1}B(\delta)
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;for \(\delta\to 0\).&lt;/p&gt;

&lt;p&gt;Please be aware, that things will break if there is no intersection at all. In the case of two parallel lines and before you take the limit, the resulting mean \(\mu\) will lie directly in the middle of the connecting line of the two means \(a\) and \(b\) and \(\Sigma\) will be the same as \(A(\delta)=B(\delta)\).&lt;/p&gt;

&lt;h1 id=&quot;determinant&quot;&gt;Determinant&lt;/h1&gt;

&lt;p&gt;Let’s assume you have a unit &lt;a href=&quot;https://en.wikipedia.org/wiki/Hypercube&quot;&gt;hypercube&lt;/a&gt;, which is transformed via the regular square matrix \(A \in \mathbb{R}^{n \times n}\). The volume of the resulting &lt;a href=&quot;https://en.wikipedia.org/wiki/Parallelepiped&quot;&gt;parallelepiped&lt;/a&gt; is equal to the absolute value of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Determinant&quot;&gt;determinant&lt;/a&gt; of \(A\). Therefore, the determinant can be described as the scaling factor of \(A\).&lt;/p&gt;

&lt;p&gt;Can we find this scaling factor with our Gaussian distributions as well? Clearly, probability distributions are always normalized: the probability &lt;em&gt;volume&lt;/em&gt; won’t change at all. But we remember that the definition of the multivariate Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|\mu, \Sigma) = \underbrace{\frac{1}{\sqrt{(2\pi)^n \det(\Sigma)}}}_{Z} \exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)&lt;/script&gt;

&lt;p&gt;has a normalizer \(Z\). If we transform the distribution \(\mathcal{N}(x|\mu, \delta_x I)\) with  \(\mathcal{N}(b|Ax, \delta_b I)\), we will obtain \(\mathcal{N}(b|A\mu, A(\delta_xI))A^T + \delta_b I)\). What will be the ratio of the coprresponding normalizers \(\frac{Z_b}{Z_x}\)?&lt;/p&gt;

&lt;p&gt;Let’s find out! We insert the normalizers&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\sqrt{(2\pi)^n \det(A(\delta_xI)) A^T + \delta_b I)}}{\sqrt{(2\pi)^n \det(\delta_x I)}},&lt;/script&gt;

&lt;p&gt;combine the square roots and cancel out \((2\pi)^n\) to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sqrt{\frac{\det(A(\delta_xI)) A^T + \delta_b I)}{ \det(\delta_x I)}}.&lt;/script&gt;

&lt;p&gt;We assume a deterministic linear transformation, therefore, we let \(\delta_b\) go to zero and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align} 
\sqrt{\frac{\det(\delta_xA^TA)}{\det(\delta_xI)}} &amp;= \sqrt{\frac{\delta_x^n\det(A^TA)}{\delta_x^n}} \\
&amp;= \sqrt{\det(A^TA)} \\
&amp;= \sqrt{\det(A^T)\det(A)} \\
&amp;= \sqrt{\det(A)\det(A)} \\
&amp;= \sqrt{\det(A)^2}  \\
&amp;= \det(A)  \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Nice! We identified the ratio of the normalizing factors of the Gaussian distributions as the determinant of \(A\).&lt;/p&gt;

&lt;h1 id=&quot;summary&quot;&gt;Summary&lt;/h1&gt;

&lt;p&gt;In this post, we took a brief look on linear algebra expressed in terms of Gaussian distributions. We saw, that we can perform matrix transformations by marginalization and that the inverse of a matrix \(A\) can be obtained naturally with Bayes theorem. As a side product, we learned a natural way to describe affine linear subspaces. The intersection of two affine linear subspaces is again an affine linear space. We saw how to calculate these intersections by simple multiplication of the corresponding Gaussian distributions. Finally, we found a nice interpretation of the determinant in terms of normalizers. Furthermore, we learned that the word natural comes naturally with Bayes.&lt;/p&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;</content><author><name></name></author><summary type="html">Linear algebra is a wonderful field of mathematics with endless applications. Despite its obvious beauty, it can also be quite confusing. Especially, when it comes to subspaces, inverses and determinants. In this article, I want to present a different view on some aspects of linear algebra with the help of Gaussian distributions and Bayes theorem.</summary></entry><entry><title type="html">Nonlinear filtering: Extended Kalman filter</title><link href="https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html" rel="alternate" type="text/html" title="Nonlinear filtering: Extended Kalman filter" /><published>2018-10-31T23:04:07+09:00</published><updated>2018-10-31T23:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html">&lt;p&gt;This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter.  &lt;!--more--&gt; If you haven’t read the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-intro.html&quot;&gt;introduction&lt;/a&gt;, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind extended Kalman filter.
&lt;!--more--&gt;
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/ekf.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://martinseilair.github.io/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;


	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;extended Kalman filter&lt;/strong&gt; approximates the Bayes filter by linearizing the system and observation equations.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;We will start the derivation directly from the recursive equations of the Bayes filter with the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

&lt;p&gt;The extended Kalman filter is normally formulated with nonlinear functions with additive noise. In this article, we directly derive the general case for non-additive noise and obtain the extended Kalman filter as a special case. Therefore, our equations of the system are&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 x_{t+1} &amp;= f(x_t, u_t, w_t) \\
 y_t &amp;= h(x_k, v_k).
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).&lt;/p&gt;

&lt;p&gt;In our formula of the Bayes filter, we can’t find any functions \(f(x_t, u_t, w_t)\) or \(h(x_k, v_k)\). Therefore, our first step will be to express these functions as probability distributions \(p(x_{t+1}|x_{t}, u_{t})\) and \(p(y_t|x_t)\). The next box will show how to achieve this in general. &lt;strong&gt;Warning:&lt;/strong&gt; Distributions are very weird and the following treatment is &lt;strong&gt;not rigorous&lt;/strong&gt;.&lt;/p&gt;
&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;We want to calculate \(p(y|x)\) given the function \(y=f(x,z)\) and \(p(z)\).
We can express the deterministic function \(y=f(x,z)\) as probability distribution&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x,z) = \delta(y-f(x,z)),&lt;/script&gt;

  &lt;p&gt;with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta function&lt;/a&gt; \(\delta(x)\).&lt;/p&gt;

  &lt;p&gt;In order to calculate \(p(y|x)\) we can simply marginalize out \(z\):&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(y|x) &amp;= \int_z p(y|x,z)p(z)\, dx \\
 &amp;= \int_z \delta(y-f(x,z))p(z)\, dx 
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;By using the composition rule of the Dirac delta function, we can express this as&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y|x) = \sum_i \frac{p(z_i)}{\left|\det\nabla_z f(x,z)|_{z_i}\right|},&lt;/script&gt;

  &lt;p&gt;where the sum goes over all \(z_i\) which satisfy the equation \(y = f(x,z)\).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In our case, we can express our system function \(x_{t+1} = f(x_t,u_t,w_t)\) as a probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \sum_i \frac{p(w_i)}{\left|\det\nabla_w f(x_t, u_t, w_t)|_{w_i}\right|},&lt;/script&gt;

&lt;p&gt;where the sum goes over all \(w_i\) which satisfy \(x_{t+1} = f(x_t,u_t,w_t)\).&lt;/p&gt;

&lt;p&gt;Similarly, we can express our observation function  \(y_t = h(x_k, v_k)\) as probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|x_t) = \sum_i \frac{p(v_i)}{\left|\det\nabla_v h(x_t, v_t)|_{v_i}\right|}&lt;/script&gt;

&lt;p&gt;where the sum goes over all \(v_i\) which satisfy \(x_{t+1} = h(x_t,v_t)\).&lt;/p&gt;

&lt;p&gt;Even if we are assuming Gaussian process and measurement noise, the resulting distributions of our model will, in general, be non-Gaussian.
This is where the extended Kalman filter comes into play. It approximates our probability distributions by using linearization.
In my limited scope, linearization of a probability distribution makes no sense. But what is linearized instead?&lt;/p&gt;

&lt;p&gt;Instead of linearizing our probability distributions we will do this with our deterministic functions before we express them as probability distributions.&lt;/p&gt;

&lt;p&gt;In order to linearize, we are performing a first-order Taylor expansion&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t, u_t, w_t) \approx f(x_t, u_t, w_t)|_{x_t=\hat x_{t|t},u_t=u,w_t=0} + \nabla_{x_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}(x_t - x)+ \nabla_{u_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}(u_t - u) + \nabla_{w_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}w_t&lt;/script&gt;

&lt;p&gt;around the current state estimate \(x_t = \hat x_{t|t}\), current input \(u_t=u\) and zero process noise \(w_t=0\).&lt;/p&gt;

&lt;p&gt;To unclutter the notation, we define \(A_t = \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) , \(B_t = \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) and \(L_t = \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) to finally obtain the much cleaner representation&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{f}(x_t, u_t, w_t) = f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u)  + L_tw_t.&lt;/script&gt;

&lt;p&gt;Now we are ready to transform our linearized deterministic system function into a probability distribution. We will use the same formula as above, but replace the true function \(f(x_t, u_t, w_t)\) with \(\hat{f}(x_t, u_t, w_t)\), and arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \sum_i \frac{\mathcal{N}(w_i|0,Q_t)}{\left|\det\nabla_w \hat{f}(x_t, u_t, w_t)|_{w_i}\right|},&lt;/script&gt;

&lt;p&gt;where the sum is over all \(w_i\) which satisfy \(x_{t+1} = \hat{f}(x_t, u_t, w_t)\). Let’s try to simplify this expression! First, we notice that if the matrix \(L_t\) is invertible, then there is exactly one \(w\) that satisfies \(x_{t+1} = \hat{f}(x_t, u_t, w_t)\). We can express it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;w = L_t^{-1}\left(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)\right).&lt;/script&gt;

&lt;p&gt;Note, that if the martrix \(L_t\) is not invertible, we would have to integrate over the whole null space and the sum would become an integral.&lt;/p&gt;

&lt;p&gt;Next, let’s look at the denominator. We can express the derivative of \(\hat{f}(x_t, u_t, w_t)\) with respect to \(w_t\) as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\nabla_w \hat{f}(x_t, u_t, w_t) =\nabla_w( f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u)  + L_tw_t) = L_t.&lt;/script&gt;

&lt;p&gt;Let’s plug in the information our new information about \(w\) and the derivative:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \frac{\mathcal{N}(L_t^{-1}\left(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)\right)|0,Q_t)}{\left|\det L_t\right|}.&lt;/script&gt;

&lt;p&gt;We apply the transformation identity (Formula 35, &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Toussaint&lt;/a&gt;):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \frac{\frac{1}{\left|\det L_t^{-1}\right|}\mathcal{N}(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)|0,L_tQ_tL_t^T)}{\left|\det L_t\right|}&lt;/script&gt;

&lt;p&gt;And apply it once more, to obtain our final result:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \mathcal{N}(x_{t+1}|f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u),L_tQ_tL_t^T).&lt;/script&gt;

&lt;p&gt;We are done! The linearization of our function has lead us back to Gaussianity!&lt;/p&gt;

&lt;p&gt;With the same strategy, we obtain for our observation model&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|x_t) = \mathcal{N}(y_T|h(\hat x_{t|t-1}, 0) + C_t(x_t - \hat x_{t|t-1}),M_tQ_tM_t^T),&lt;/script&gt;

&lt;p&gt;with \(C_t = \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\) and \(M_t = \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\)&lt;/p&gt;

&lt;p&gt;Our linearization led to a Gaussian system and observation model. Therefore, the distribution of the updated state estimate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) := \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t})&lt;/script&gt;

&lt;p&gt;and the distribution of the predicted state estimate&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) := \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})&lt;/script&gt;

&lt;p&gt;will be Gaussians as well.&lt;/p&gt;

&lt;p&gt;Now we are ready to plug our surrogate into the equations of the Bayes filter:&lt;/p&gt;
&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;&lt;strong&gt;Prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u),L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

  &lt;p&gt;&lt;strong&gt;Update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(x, 0) C_t(x_t - \hat x_{t|t-1}) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(x, 0) C_t(x_t - \hat x_{t|t-1}) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;/div&gt;

&lt;p&gt;Let’s try to simplify these equations!&lt;/p&gt;

&lt;h3 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h3&gt;

&lt;p&gt;We will start with the prediction step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) - A_t\hat x_{t|t} - B_tu + A_tx_t + B_tu_t,L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

&lt;p&gt;To find an expression for our prediction step we can simply use the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Toussaint&lt;/a&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ).&lt;/script&gt;

&lt;p&gt;By comparison with our expression, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat x_{t+1|t} = f(x_{t|t}, u_t, 0) -A_t \hat x_{t|t} - B_tu_t + A_t \hat x_{t|t} + B_tu_t = f(x_{t|t}, u_t, 0)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t+1|t} = L_t^TQ_tL_t + A_t P_{t|t} A_t^T  .&lt;/script&gt;

&lt;h3 id=&quot;update-step&quot;&gt;Update step&lt;/h3&gt;

&lt;p&gt;We will start to simplify the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t\hat x_{t|t-1} + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t\hat x_{t|t-1} + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Toussaint&lt;/a&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A &amp; A^TF^T\\FA &amp; B + FA^TF^T\end{matrix}\right) . %]]&gt;&lt;/script&gt;

&lt;p&gt;Then again, this joint distribution can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D &amp; F\\F^T &amp; E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) . %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine the two previous equations to the following expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .&lt;/script&gt;

&lt;p&gt;By comparison with the numerator of our update step, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1},M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0) + C_t x_{t|t-1} -C_t\hat x_{t|t-1}),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}),&lt;/script&gt;

&lt;p&gt;which simplifies to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;We applied the same trick as in the &lt;a href=&quot;/jekyll/update/2018/10/10/kalman_filter.html&quot;&gt;derivation of the Kalman filter&lt;/a&gt;: Conceptually, we only transformed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}.&lt;/script&gt;

&lt;p&gt;If we look closely at the final expression, we see that \(p(y)\) is canceling out. Therefore, the result is simply the remaining part&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;If our reasoning is correct the denominator should be equal to \(\mathcal{N}(y_{t}|h(x, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)\), which was canceled out. The denominator can be simplified with the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Toussaint&lt;/a&gt;)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1}, M_t^TR_tM_t + C_tP_{t|t-1}C_t^T ) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T).&lt;/script&gt;

&lt;p&gt;Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.&lt;/p&gt;

&lt;p&gt;Let’s summarize our results:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Extended Kalman filter with non-additive noise&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the extended Kalman filter with non-additive noise consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &amp;= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t|t} &amp;= \hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)) \\ 
P_{t|t} &amp;= P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}  \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;with&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A_t &amp;= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &amp;= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &amp;= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
L_t &amp;= \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
M_t &amp;= \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}.
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;
&lt;p&gt;That’s it! We derived the equations of the extended Kalman filter. To bring the equations in a more implementation friendly form, we are restating the extended Kalman filter as:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Extended Kalman filter with non-additive noise&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the extended Kalman filter with non-additive noise consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &amp;= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_t &amp;= y_{t}-h(\hat x_{t|t-1}, 0)\\
S_t &amp;= M_t^TR_tM_t + C_tP_{t|t-1}C_t^T\\
K_t &amp;= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &amp;= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &amp;= (I - K_tC_t)P_{t|t-1}
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;with&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A_t &amp;= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &amp;= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &amp;= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
L_t &amp;= \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
M_t &amp;= \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}.
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;

&lt;p&gt;As promised we will also look at the special case with &lt;strong&gt;additive&lt;/strong&gt; noise. Therefore, our functions will look like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_t, u_t, w_t) = \bar{f}(x_t, u_t) + w_t.&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h(x_t, v_t) = \bar{h}(x_t) + v_t.&lt;/script&gt;

&lt;p&gt;In this case, the matrix \(L_t\) and \(M_t\) will be identity matrices:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L_t = \nabla_{w_t} f(x_t, u_t, w_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{w_t} \bar{f}(x_t, u_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{w_t} w_t}_{I}|_{x_t=x,v_t=0} = I&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;M_t = \nabla_{v_t} h(x_t, v_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{v_t} \bar{h}(x_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{v_t} v_t}_{I}|_{x_t=x,v_t=0} = I.&lt;/script&gt;

&lt;p&gt;Finally, we can state the equations of the extended Kalman filter with additive noise.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Extended Kalman filter with additive noise&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the extended Kalman filter with additive noise consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;= f(x_{t|t}, u_t) \\ 
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T   \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_t &amp;= y_{t}-h(\hat x_{t|t-1})\\
S_t &amp;= R_t + C_tP_{t|t-1}C_t^T\\
K_t &amp;= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &amp;= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &amp;= (I - K_tC_t)P_{t|t-1}
\end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;with&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
A_t &amp;= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &amp;= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &amp;= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Enough of the dry theory! Let’s play around with the grid-based filter in our race track example.&lt;/p&gt;

&lt;svg id=&quot;race_track_mar_loc&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;
&lt;script&gt;


	n_scene = load_race_track(&quot;race_track_mar_loc&quot;,&quot;https://martinseilair.github.io&quot;);
	n_scene.mode = 2;
	n_scene.filter = &quot;&quot;;
	n_scene.dur=slow_dur;
	// define particle filter 

	n_scene.auto_start = false;

	n_scene.t = 1;
	n_scene.take_observation = true;
	n_scene.ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];

	n_scene.loaded = function(){

		var outer_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style(&quot;background-color&quot;)), d3.rgb('#006eff'), d3.rgb('#00028e')]);
		var inner_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style(&quot;background-color&quot;)), d3.rgb('#ff834d'), d3.rgb('#8e3323')]);

		this.ekf = init_ekf_1D(this.rc, this.rc.state, 3*this.rc.sigma_s_no_cache(this.rc.state));
		this.rt.init_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.rc.state), inner_color, 60);
		this.rt.init_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt), outer_color, 60);



		document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
		this.rt.hide_strip(&quot;inner&quot;);


		this.restart = function(){
			for (var i=0; i&lt;this.ids.length;i++){

				document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
			}
			document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
			this.rc.reset();
			this.t = 1;

			

			//this.bf.reset();
			this.ekf.reset(this.rc.state, this.rc.sigma_o_no_cache(this.rc.state))
			this.rt.hide_strip(&quot;inner&quot;);
			this.rt.show_strip(&quot;outer&quot;);
			this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));
			this.rt.treeg.style(&quot;opacity&quot;,1.0)
			this.take_observation = true;
		}


		this.rt.set_restart_button(this.restart.bind(this))

		this.toogle_observation = function(){
			if(this.take_observation){
				this.rt.treeg.style(&quot;opacity&quot;,0.2)
				this.take_observation = false;
				if(this.t% 5 ==1){
					document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;none&quot;;
					document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
					this.t = 3;
				}
			}else{
				this.rt.treeg.style(&quot;opacity&quot;,1.0)
				this.take_observation = true;
			}
        	
        }

		this.rt.tree_click = this.toogle_observation.bind(this)


	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;
		for (var i=0; i&lt;this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
		}


		if(this.t % 4 == 0){
			//CHOOSE ACTION
			this.rc.step(this.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById(&quot;race_track_mar_loc_predict&quot;).style.display=&quot;block&quot;;
			this.rt.hide_strip(&quot;inner&quot;);
		}else if(this.t % 4 == 1){
			// PREDICT
			this.ekf.predict(this.last_input);

			// trim ekf posterior
			if(this.ekf.posterior_mu&lt;0){
				this.ekf.posterior_mu+=this.rt.track_length;
			}
			this.ekf.posterior_mu = this.ekf.posterior_mu % this.rt.track_length;
			this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));
			if(this.take_observation){
				document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
			}else{
				document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
				this.t=3;
			}
		}else if(this.t % 4 == 2){
			// OBSERVE
			this.rt.show_strip(&quot;inner&quot;);
			this.output = scene.rc.output_dist_sample(0);
			var likelihood = this.ekf.get_likelihood(this.output,this.ekf.posterior_mu)
			this.rt.update_strip(&quot;inner&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, likelihood.mu, likelihood.sigma, this.rt));

			document.getElementById(&quot;race_track_mar_loc_update&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 4 == 3){
			// UPDATE

			this.ekf.update(this.output);

			this.rt.update_strip(&quot;outer&quot;, get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));

			document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
		}

	}.bind(n_scene);

	scenes_name[&quot;race_track_mar_loc&quot;] = n_scene;
	scenes.push(n_scene);

&lt;/script&gt;

&lt;div id=&quot;race_track_mar_loc_timestep&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_predict&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Predict step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_likelihood&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Observe&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_update&quot; class=&quot;button_set&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;
&lt;div class=&quot;bt1  bt&quot;&gt;Update step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;On the outside of the race track, you will notice a blue colored strip. This strip represents our current posterior of the current position of the race car. We will start with a Gaussian prior distribution around the true mean. By pressing the &lt;strong&gt;OBSERVE&lt;/strong&gt; button two things will happen: first, we will take a measurement of the distance to the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the &lt;strong&gt;UPDATE STEP&lt;/strong&gt; button, we will perform our update step and show the resulting posterior at the outer strip. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the &lt;strong&gt;PREDICT STEP&lt;/strong&gt; button. You will see that the outer strip will change accordingly. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.&lt;/p&gt;

&lt;p&gt;What if our distance meter is not working anymore? By either clicking on the tree or pressing the &lt;strong&gt;W&lt;/strong&gt; button on your keyboard, you can turn off your measurement device. Therefore, the observation and update step will be skipped. The tree will become opaque, if your measurement device is turned off.&lt;/p&gt;

&lt;p&gt;If you want to reset the environment, just press the reset button in the bottom left corner or press the &lt;strong&gt;R&lt;/strong&gt; button on your keyboard.
As before you can control the car by using your keyboard: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward) or the buttons below the race track.&lt;/p&gt;

&lt;p&gt;We see, that it is actually working pretty well. But one thing seems particularly weird: At certain positions on the race track, the brownish inner strip (our likelihood) seems to be uniformly distributed. This is not a bug, but a shortcoming of the linearization. We will always experience this behavior, if the part of the road at our current posterior mean is pointing only in &lt;em&gt;tangential&lt;/em&gt; direction, with the tree as the center. In other words: A small change in position wouldn’t change the distance. When we are linearizing, we assume this local behavior applies for the whole system and we won’t get any new information out of our measurement. To get an intuitive understanding of this, you can imagine two parallel lines. Our car is driving along one of the parallel lines and we take the nearest distance to the other line as a measurement. This measurement would be uninformative because the distance to the parallel line is always the same.&lt;/p&gt;

&lt;p&gt;Are you curious about the derivation of unscented Kalman filter? Then you should definitely check out the next article of the nonlinear filter series covering the &lt;a href=&quot;/jekyll/update/2018/11/07/nf-ukf.html&quot;&gt;unscented Kalman filter&lt;/a&gt;. See you there!&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;rad_to_s&quot; style=&quot;width:100px&quot;&gt;&lt;/div&gt;
&lt;div id=&quot;div1&quot;&gt;&lt;/div&gt;
&lt;div id=&quot;div2&quot;&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;system_dist_approx&quot;  style=&quot;width: 600px; height: 600px;&quot;&gt;&lt;/div&gt; --&gt;
&lt;!--&lt;div id=&quot;output_dist_approx&quot;  style=&quot;width: 600px; height: 600px;&quot;&gt;&lt;/div&gt;--&gt;
&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;</content><author><name></name></author><summary type="html">This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter.</summary></entry><entry><title type="html">Nonlinear filtering: Grid-based filter</title><link href="https://martinseilair.github.io/jekyll/update/2018/10/29/nf-grid-based.html" rel="alternate" type="text/html" title="Nonlinear filtering: Grid-based filter" /><published>2018-10-29T18:05:07+09:00</published><updated>2018-10-29T18:05:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/10/29/nf-grid-based</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/10/29/nf-grid-based.html">&lt;p&gt;The process of Bayes filtering requires to solve integrals, that are in general intractable. One approach to circumvent this problem is the use of grid-based filtering. In this article, we will derive this method directly from the recursive equations of the Bayes filter.  &lt;!--more--&gt;This marks the first part of the nonlinear filtering series. If you haven’t read the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-intro.html&quot;&gt;introduction&lt;/a&gt;, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind grid-based filtering.&lt;/p&gt;

&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://martinseilair.github.io/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;grid-based filter&lt;/strong&gt; approximates the Bayes filter by &lt;strong&gt;restricting&lt;/strong&gt; and &lt;strong&gt;discretizing&lt;/strong&gt; the state, input and observation space, to obtain &lt;strong&gt;finite&lt;/strong&gt; domains.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;We will start the derivation directly from the recursive equations of the Bayes filter with the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

&lt;p&gt;The first and most important step in order to arrive at the equations of the grid-based filter is to discretize our system and observation model. Given a set of discretization points \((x^1,\ldots,x^X)\), \((y^1,\ldots,y^Y)\) and \((u^1,\ldots,u^U)\) the result of the discretization can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|x_{t}, u_{t}) = \frac{1}{Z}p(x_{t+1}|x_{t}, u_{t})\sum_{k=1}^X\sum_{l=1}^X \sum_{m=1}^U  \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t)&lt;/script&gt;

&lt;p&gt;for the system model and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t) = \frac{1}{Z}p(y_{t}|x_{t})\sum_{k=1}^Y\sum_{l=1}^X  \delta_{y^k}(y_{t})\delta_{x^l}(x_t)&lt;/script&gt;

&lt;p&gt;for our observation model, where \(Z\) is a normalization factor.&lt;/p&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;These equations can look confusing and random. Especially if you are not familiar with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta function&lt;/a&gt;. This box will give a short introduction to the Dirac delta function and provide the tools you will need. &lt;strong&gt;Warning:&lt;/strong&gt; The Dirac delta function is actually not a function, but a distribution or a measure!&lt;/p&gt;

  &lt;p&gt;The Dirac delta function \(\delta(x)\) can be &lt;em&gt;imagined&lt;/em&gt; as a function, that is &lt;em&gt;infinite&lt;/em&gt; at \(x=0\) and &lt;em&gt;zero&lt;/em&gt; at \(x \neq 0\) with the property&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_x \delta(x)\,dx = 1.&lt;/script&gt;

  &lt;p&gt;It is not possible to plot the Dirac delta function, but we can visualize it schematically by using an arrow pointing upwards, as shown in the figure below.&lt;/p&gt;
  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;The length of the line represents the area under the function. The expression \(\delta(x-y)\) is a shifted version of the Dirac delta function with its peak at \(y\).&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_shift&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;To unclutter the notation, we will use the shorthand \(\delta_y(x)\) for \(\delta(x-y)\).&lt;/p&gt;

  &lt;p&gt;The &lt;em&gt;sifting property&lt;/em&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_x f(x)\delta_y(x)\,dx = f(y)&lt;/script&gt;

  &lt;p&gt;will play an important role in our derivation. Intuitively, it is sifting out the function value of \(f(x)\) at \(y\).&lt;/p&gt;

  &lt;p&gt;The multivariate Dirac delta function, which can be &lt;em&gt;loosely&lt;/em&gt; thought of as &lt;em&gt;infinite&lt;/em&gt; at \((x_1,\ldots,x_n)=(0,\ldots,0)\) and &lt;em&gt;zero&lt;/em&gt; at \((x_1,\ldots,x_n) \neq (0,\ldots,0)\) is defined as the product of several Dirac delta functions:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta(x_1,\ldots,x_n) = \delta(x_1)* \ldots *\delta(x_n).&lt;/script&gt;

  &lt;p&gt;Several Dirac delta functions can also be combined to form a comb or grid, which is shown in the following figure.&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_comb&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;To obtain such a grid, the Dirac delta functions are simply added together&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N\delta_{x_i}(x).&lt;/script&gt;

  &lt;p&gt;Finally, we can multiply the grid with a function \(f(x)\) :&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^Nf(x)\delta_{x_i}(x).&lt;/script&gt;

  &lt;p&gt;The particular Dirac delta functions are weighted by the functional value at the corresponding point. This can be visualized as following.&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_wcomb&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;Now that we have a better understanding of the Dirac delta function, let’s look again at our system and observation model. In our system model, we find a multivariate Dirac delta function&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t),&lt;/script&gt;

  &lt;p&gt;where the particular Dirac delta functions could be multivariate as well. We also note, that the multivariate Dirac delta function is embedded in a triple sum over the corresponding discrete spaces. It describes, therefore, a grid of Dirac delta functions.
Finally, we multiply this grid with our system model. To obtain a proper probability distribution, we normalize our distribution with \(Z\).
The observation model can be treated likewise.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;If we start the recursion with a discretized prior distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_0) = \sum_{k=1}^X w^k \delta_{x^k}(x_0)&lt;/script&gt;

&lt;p&gt;all of our belief distributions will be discretized as well.
Therefore, the posterior will have the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t}|\cdot) = \sum_{k=1}^X w^k \delta_{x^k}(x_t).&lt;/script&gt;

&lt;h1 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h1&gt;

&lt;p&gt;Let’s see what happens, if we plug the discretized version of our system model into the equations of the prediction step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} \frac{1}{Z}p(x_{t+1}|x_{t}, u_{t})\sum_{k=1}^X\sum_{l=1}^X\sum_{m=1}^U \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;First of all, we know our current input is \(u^i\). To evaluate \(\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) \) at \(u^i\), we simply have to compute \(\int_{u_t}\hat{p}(x_{t+1}|y_{0:t},u_{0:t})\delta_{u^i}(u_t)\,d_t \). As a result, we will obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{k=1}^X\sum_{l=1}^X  \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;The following expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{l=1}^X \delta_{x^l}(x_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t)&lt;/script&gt;

&lt;p&gt;is not zero only if \(l = n\). Therefore, we can replace this expression with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{n=1}^X w^n \delta_{x^n}(x_t)&lt;/script&gt;

&lt;p&gt;and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{k=1}^X\sum_{n=1}^X  \delta_{x^k}(x_{t+1}) w^n \delta_{x^n}(x_t) dx_{t} .&lt;/script&gt;

&lt;p&gt;By rearranging the integral and sums we  obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{n=1}^X   w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;Using the sifting property we will arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\sum_{n=1}^Xp(x_{t+1}|x_{t} = x^n, u_t=u^m)    w^n&lt;/script&gt;

&lt;h1 id=&quot;update-step&quot;&gt;Update step&lt;/h1&gt;

&lt;p&gt;Let’s start our treatment of the update step and by looking at the numerator. Again, we plug in our discretized distributions to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{Z}p(y_{t}|x_{t})\sum_{k=1}^Y\sum_{l=1}^X  \delta_{y^k}(y_{t})\delta_{x^l}(x_t)\sum_{m=1}^X w^m \delta_{x^m}(x_t).&lt;/script&gt;

&lt;p&gt;We know, that we have an observation \(y^k\), which we can plug in in the same way as the input above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{Z}p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)&lt;/script&gt;

&lt;p&gt;The denominator is simply the integral over \(x_t\) of the expression above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_t}\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t = \frac{1}{Z}\int_{x_t}p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t) \,dx_t.&lt;/script&gt;

&lt;p&gt;With help of the sifting property, we obtain the final expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_t}\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t = \frac{1}{Z}\sum_{m=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)&lt;/script&gt;

&lt;p&gt;for the denominator.&lt;/p&gt;

&lt;p&gt;The full update step is then defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)}{\sum_{l=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)} .&lt;/script&gt;

&lt;p&gt;Let’s summarize our results!&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Grid-based filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the grid-based filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\sum_{n=1}^Xp(x_{t+1}|x_{t} = x^n, u_t=u^m)    w^n&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)}{\sum_{l=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with a discrete prior distribution over the initial state \(\hat{p}(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;h1 id=&quot;discrete-probability-distribution&quot;&gt;Discrete probability distribution&lt;/h1&gt;

&lt;p&gt;Our discretized system and observation model is non-zero only at the points on the grid. Nonetheless, the model itself is still continuous: we can evaluate the function at points, which are not part of the grid. Then again, the posterior will always remain on the grid during the filtering process. Thus, we can represent our system as a discrete probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_{t+1}=k|x_t=l,u=m) = \frac{1}{Z(l,m)} p(x_{t+1}=x^k|x_{t}=x^l, u_t=u^m)&lt;/script&gt;

&lt;p&gt;with the normalization factor \(Z(l,m) = \sum_{k=1}^X p(x_{t+1}=x^k|x_{t}=x^l, u_t=u^m) \).&lt;/p&gt;

&lt;p&gt;Similarly, the observation model can be represented by the discrete probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y_{t}=k|x_t=l) = \frac{1}{Z(l)} p(y_{t}=y^k|x_{t}=x^l)&lt;/script&gt;

&lt;p&gt;with the normalization factor \(Z(l) = \sum_{k=1}^X p(y_{t}=y^k|x_{t}=x^l) \). With this representation, we arrive at the Bayes filter for discrete probability distributions.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Discrete Bayes filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the discrete Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_{t+1}|y_{0:t},u_{0:t}) = \sum_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1})&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_t|y_{0:t},u_{0:t-1}) = \frac{P(y_t|x_t)P(x_t|y_{0:t-1},u_{0:t-1})}{\sum_{x_t} P(y_t|x_t)P(x_t|y_{0:t-1},u_{0:t-1})} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with a discrete prior distribution over the initial state \(P(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Enough of the dry theory! Let’s play around with the grid-based filter in our race track example.&lt;/p&gt;

&lt;svg id=&quot;race_track_mar_loc&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;
&lt;script&gt;


	n_scene = load_race_track(&quot;race_track_mar_loc&quot;,&quot;https://martinseilair.github.io&quot;,400);
	n_scene.mode = 2;
	n_scene.filter = &quot;bayes&quot;;
	n_scene.dur=slow_dur;
	// define particle filter 

	n_scene.auto_start = false;

	n_scene.t = 1;

	n_scene.ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];

	n_scene.take_observation = true;

	n_scene.loaded = function(){
		//var ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];
		//for (var i=0; i&lt;ids.length;i++){

		//	document.getElementById(ids[i]).style.display=&quot;none&quot;;
		//}
		document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
		this.rt.hide_strip(&quot;inner&quot;);


		this.restart = function(){
			for (var i=0; i&lt;this.ids.length;i++){

				document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
			}
			document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
			this.rc.reset();
			this.t = 1;
			this.bf.reset();
			this.rt.hide_strip(&quot;inner&quot;);
			this.rt.show_strip(&quot;outer&quot;);
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
			this.rt.treeg.style(&quot;opacity&quot;,1.0)
			this.take_observation = true;
		}


		this.rt.set_restart_button(this.restart.bind(this))

		this.toogle_observation = function(){
			if(this.take_observation){
				this.rt.treeg.style(&quot;opacity&quot;,0.2)
				this.take_observation = false;
				if(this.t% 5 ==1){
					document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;none&quot;;
					document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
					this.t = 3;
				}
			}else{
				this.rt.treeg.style(&quot;opacity&quot;,1.0)
				this.take_observation = true;
			}
        	
        }

		this.rt.tree_click = this.toogle_observation.bind(this)



	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;
		for (var i=0; i&lt;this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
		}
		//document.getElementById(ids[this.t%3]).style.display=&quot;block&quot;;


		if(this.t % 4 == 0){
			this.rc.step(this.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById(&quot;race_track_mar_loc_predict&quot;).style.display=&quot;block&quot;;
			this.rt.hide_strip(&quot;inner&quot;);
		}else if(this.t % 4 == 1){
			this.bf.predict(this.last_input);
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
			if(this.take_observation){
				document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
			}else{
				document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
				this.t=3;
			}
		}else if(this.t % 4 == 2){
			this.rt.show_strip(&quot;inner&quot;);
			this.output = scene.rc.output_dist_sample(0);
			this.rt.update_strip(&quot;inner&quot;, get_output_dist_normalized_from_distance(this.rc, this.rt, this.output));
			document.getElementById(&quot;race_track_mar_loc_update&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 4 == 3){
			
	    	var y = scene.bf.cont_2_disc_output(this.output);
			this.bf.update(y);
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
			document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
		}



	}.bind(n_scene);

	scenes_name[&quot;race_track_mar_loc&quot;] = n_scene;
	scenes.push(n_scene);

&lt;/script&gt;

&lt;div style=&quot;float:right&quot; class=&quot;slidecontainer&quot;&gt;
  &lt;input type=&quot;range&quot; min=&quot;100&quot; max=&quot;700&quot; value=&quot;400&quot; class=&quot;slider&quot; id=&quot;myRange&quot; /&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_timestep&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_predict&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Predict step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_likelihood&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Observe&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_update&quot; class=&quot;button_set&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;
&lt;div class=&quot;bt1  bt&quot;&gt;Update step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;On the outside of the race track, you will notice a blue colored strip. This strip represents our current posterior of the current position of the race car. At the beginning, we have no knowledge about the position of the race car and assign uniform probability over all positions. By pressing the &lt;strong&gt;OBSERVE&lt;/strong&gt; button two things will happen: first, we will take a measurement of the distance to the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the &lt;strong&gt;UPDATE STEP&lt;/strong&gt; button, we will perform our update step and show the resulting posterior at the outer strip. You will note, that both strips will have the same form after the update. The reason is simple: we just multiplied our likelihood with a constant vector and normalized afterward. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the &lt;strong&gt;PREDICT STEP&lt;/strong&gt; button. You will see that the outer strip will change accordingly. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.&lt;/p&gt;

&lt;p&gt;What if our distance meter is not working anymore? By either clicking on the tree or pressing the &lt;strong&gt;W&lt;/strong&gt; button on your keyboard, you can turn off your measurement device. Therefore, the observation and update step will be skipped. The tree will become opaque, if your measurement device is turned off.&lt;/p&gt;

&lt;p&gt;With the slider below the race track, you can choose a grid size of the discrete probability models. If you want to reset the environment, just press the reset button in the bottom left corner or press the &lt;strong&gt;R&lt;/strong&gt; button on your keyboard.
As before you can control the car by using your keyboard: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward) or the buttons below the race track.&lt;/p&gt;

&lt;script&gt;



var slider = document.getElementById(&quot;myRange&quot;);

slider.oninput = function() {

	scene = scenes_name['race_track_mar_loc'];

    // delete strips
    scene.rt.delete_strip(&quot;inner&quot;);
    scene.rt.delete_strip(&quot;outer&quot;);


    // set strip domain
    scene.rt.set_strip_domain(parseInt(this.value));


    // create new bayes filter
    scene.bf = init_bayes_filter(scene.rc, scene.rt);

	// create strips
    scene.rt.init_strip(&quot;inner&quot;,get_output_dist_normalized(scene.rc, scene.rt, scene.rc.state) , scene.rt.strip_color.inner, scene.rt.strip_width.inner)
    scene.rt.init_strip(&quot;outer&quot;, normalize_vector(scene.bf.posterior), scene.rt.strip_color.outer, scene.rt.strip_width.outer)
    scene.restart()
}
&lt;/script&gt;

&lt;p&gt;Still feeling hungry for Bayes filters? Then you should definitely check out the next part of the nonlinear filtering series covering the derivation of the &lt;a href=&quot;/jekyll/update/2018/10/31/nf-ekf.html&quot;&gt;extended Kalman filter&lt;/a&gt;. See you there!&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;script&gt;


dirac_plot(&quot;#dirac&quot;,[{x:0,w:0.7,t:&quot;&quot;}], false, null);
dirac_plot(&quot;#dirac_shift&quot;,[{x:0.25,w:0.7,t:&quot;y&quot;}], true, null);

var nd = 17;
var grid = [...Array(nd)].map((e,i)=&gt;{return (i - (nd-1)/2)/(1.5*nd)});
comb = [...Array(nd)].map((e,i)=&gt;{return {x:grid[i], w:0.7, t:&quot;&quot;}})
dirac_plot(&quot;#dirac_comb&quot;,comb, true, null);

wcomb = [...Array(nd)].map((e,i)=&gt;{return {x:grid[i], w:0.1*gaussian(grid[i],0,0.15), t:&quot;&quot;}})

var np = 101;
gauss = [...Array(np)].map((e,i)=&gt;{return {x:(i - (np-1)/2)/(1.5*np), w:0.1*gaussian((i - (np-1)/2)/(1.5*np),0,0.15), t:&quot;&quot;};})

dirac_plot(&quot;#dirac_wcomb&quot;,wcomb, true, gauss);
&lt;/script&gt;</content><author><name></name></author><summary type="html">The process of Bayes filtering requires to solve integrals, that are in general intractable. One approach to circumvent this problem is the use of grid-based filtering. In this article, we will derive this method directly from the recursive equations of the Bayes filter.</summary></entry><entry><title type="html">Nonlinear filtering: Introduction</title><link href="https://martinseilair.github.io/jekyll/update/2018/10/29/nf-intro.html" rel="alternate" type="text/html" title="Nonlinear filtering: Introduction" /><published>2018-10-29T18:04:07+09:00</published><updated>2018-10-29T18:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/10/29/nf-intro</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/10/29/nf-intro.html">&lt;p&gt;This post will start a series of articles that will treat common nonlinear filtering methods that are based on the Bayes filter. The motivation is to provide an intuitive understanding of these methods by deriving them directly from the general Bayes filter. This derivation is done in steps, that are supposed to be as atomic as possible. Furthermore, each nonlinear filtering method will be shown in action by providing an interactive example to play around with. This series will require some basic knowledge in math. Especially in linear algebra and probability theory.&lt;/p&gt;

&lt;!--more--&gt;
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;https://martinseilair.github.io/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;





function draw_ssm(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 120;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 4;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;T-1) ind+=(t-T+1)


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }


    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}

    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);

  	var svg_w = margin_x + dist_x*(T-1) + 50;
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }



	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Recently, I wrote an article about the &lt;a href=&quot;/jekyll/update/2018/10/10/kalman_filter.html&quot;&gt;derivation of the Kalman filter&lt;/a&gt;. When we are using the Kalman filter we assume that our models are linear Gaussian state space models. In this scenario, we are lucky because we can compute everything analytically in closed form. But at some point in time, we have to face the cruel truth that the real world is normally anything but linear. With nonlinear models, it is not possible to compute the equation of the Bayes filter in closed form, anymore. Nonetheless, we are still lucky, because there are many methods that provide approximations of the Bayes filter. Unfortunately, learning about those methods can be quite frustrating, many resources simply state the corresponding equations and are not providing any further explanations and intuitions. This series of blog posts is an attempt to fix this and provide derivations of these methods directly from the equations of the Bayes filter. Furthermore, presenting merely concatenations of equations can be very frustrating and tiring as well. Thus, the derivations are enriched with figures and examples to obtain a better understanding of what we are actually doing.
The current plan is to write articles about the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/jekyll/update/2018/10/29/nf-grid-based.html&quot;&gt;Grid-based filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/jekyll/update/2018/10/31/nf-ekf.html&quot;&gt;Extended Kalman filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/jekyll/update/2018/11/07/nf-ukf.html&quot;&gt;Unscented Kalman filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/jekyll/update/2018/11/08/nf-particle.html&quot;&gt;Particle filter&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this first post, I will take the chance to introduce the concept of the Bayes filter and to present the running example for the rest of this series.
Let’s get started!&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter&quot;&gt;Bayes filter&lt;/h2&gt;

&lt;p&gt;Let’s try to state the main idea of the Bayes filter in a compact manner.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;Bayes filter&lt;/strong&gt; is used to &lt;strong&gt;infer&lt;/strong&gt; the current state of a probabilistic state space model given all observations and inputs up to the current timestep and a prior distribution of the initial state.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We define our probabilistic state space model by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
x_{t+1} &amp;\sim p(x_{t+1}|x_{t}, u_{t}) \\

y_t &amp;\sim p(y_t|x_t) 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and an initial state distribution \(p(x_0)\). In the figure below, our model is visualized as a probabilistic graphical model.&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Knowledge about probabilistic graphical models is not required to be able to follow this series, but it can be quite helpful for understanding the dependencies between the state, input and output.&lt;/p&gt;

&lt;p&gt;Now that we have briefly introduced the state space model, we can formulate the objective of Bayes filters in a more rigorous way: We want to calculate \(p(x_t|y_{0:t},u_{0:t-1})\) given the prior initial distribution \(p(x_0)\), where the shorthand \(y_{n:m}\) stands for the set \(y_n,…,y_m\). With help of the basic rules of probability, we can find a way to calculate this expression in a recursive fashion. This recursion is composed of two distinct steps: the &lt;strong&gt;update&lt;/strong&gt; and &lt;strong&gt;predict&lt;/strong&gt; step.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Recursive formula of the Bayes filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

  &lt;p&gt;The recursion is initialized with a prior distribution over the initial state \(p(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;We will use the definition above as our starting point for the derivations of the particular nonlinear filtering methods. But first, it’s time to introduce the example, that will stay with us for the rest of this series.&lt;/p&gt;

&lt;h2 id=&quot;interactive-example-race-track&quot;&gt;Interactive example: Race track&lt;/h2&gt;

&lt;p&gt;The example that will accompany us for the rest of this series, consists of a race car that can drive around a race track. The race car can be controlled by the discrete actions of &lt;em&gt;forward&lt;/em&gt;, &lt;em&gt;backward&lt;/em&gt; and &lt;em&gt;stop&lt;/em&gt;. After the action is taken some process noise is added. Therefore, we won’t know exactly where the race car will be after we have taken this action. Inside the race course is a tree, which serves as a natural landmark. The race car has a distance meter on board, which obtains noisy measurements of the distance to this tree. The following graphic shows the race car in action. Please use your keyboard to set the input of the race car: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward). Alternatively, you can use the buttons below.&lt;/p&gt;

&lt;svg id=&quot;race_track_intro&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;
	// defines scenes
	n_scene = load_race_track(&quot;race_track_intro&quot;,&quot;https://martinseilair.github.io&quot;);
	n_scene.mode = 0;
	n_scene.filter = null;
	n_scene.dur=fast_dur;
	n_scene.auto_start = true;

	n_scene.loaded = function(){
		document.getElementById(&quot;race_track_intro_input&quot;).style.display=&quot;block&quot;;
	}.bind(n_scene)

	n_scene.step= function(){
		this.rc.step(this.rc.current_input);
	}.bind(n_scene);

	n_scene.key_down = function(key){
		input = key_to_input(key);
		if(input&gt;=0 &amp;&amp; input &lt;=2){
			this.rc.current_input = input;
		}
	}.bind(n_scene)


	n_scene.on_click = function(key){
		ani(this);
	}.bind(n_scene)
	scenes_name['race_track_intro'] = n_scene;
	scenes.push(n_scene);
&lt;/script&gt;

&lt;div id=&quot;race_track_intro_input&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=0&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=1&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=2&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have a good idea how the model will behave, you have the choice of either going directly to the first post about the grid-based filter or to learn more about the model of the dynamic \(p(x_{t+1}|x_{t}, u_{t})\) and observation \(p(y_t|x_t)\) of the example in the following section.&lt;/p&gt;

&lt;h2 id=&quot;simulation-model&quot;&gt;Simulation model&lt;/h2&gt;

&lt;p&gt;If you want to apply Bayes filters to real-life scenarios, a mathematical model of your system is needed. In general, you will have to learn it from data or derive it directly from the laws of physics. In our case we are lucky, because we are the designer of the real model and simply will use this exact model. Let’s start by taking a closer look at the system dynamic.&lt;/p&gt;

&lt;h1 id=&quot;system-dynamic&quot;&gt;System dynamic&lt;/h1&gt;

&lt;p&gt;The state \(x_t\) of the system is the current position on the race track. It is defined as the path length from the start to the current position following the race track. Having only the position without the velocity as a state representation is not very realistic, because it is not possible to model momentum. But for the sake of simplicity and visualisation, we will go without it. The input \(u_t\) is either \(-1\), \(0\) or \(1\) for backward, stop and forward.&lt;/p&gt;

&lt;p&gt;Our system dynamics are defined by a Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \mathcal{N}(x_{t+1}|\mu_s(x_t, u_t) ,\sigma_s^2(x_t) )&lt;/script&gt;

&lt;p&gt;with nonlinear mean  \(\mu_s(x_t, u_t)\) and variance \(\sigma_s^2(x_t)\). To obtain the mean of the next state \(x_{t+1}\) the input \(u_t\), which is weighted by \(v\) and \(b(\kappa)\), is simply added to the current state \(x_t\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_s(x_t, u_t) = x_t + b(\kappa)vu_t.&lt;/script&gt;

&lt;p&gt;The factor \(v\) is the velocity of the car, we will call it the step size. Intuitively, by multiplying \(u_t\) with the step size \(v\) you map the input from action space to the race track space. The weighting factor \(b(\kappa) = e^{- c\kappa}\), with the hand-tweaked parameter \(c\), is used to model a more realistic driving behavior that depends on the curvature&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa(x_t) ={\frac {|L_x'(x_t)L_y''(x_t)-L_y'(x_t)L_x''(x_t)|}{\left(L_x'^2(x_t)+L_y'^2(x_t)\right)^{\frac {3}{2}}}}&lt;/script&gt;

&lt;p&gt;of the track at the current position \(x_t\). The function \(L(x_t)\) maps the current position of the car \(x_t\) to \((x,y)\) coordinates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x_t) =  \begin{pmatrix}
    L_x(x_t) \\
    L_y(x_t) \\
    \end{pmatrix}.&lt;/script&gt;

&lt;p&gt;Intuitively, if we are in a sharp curve the curvature is low and we drive faster. If we are on a more straight part of the track, the curvature is high and we drive faster.&lt;/p&gt;

&lt;p&gt;The variance of the next state \(x_{t+1}\) is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_s^2(x_t, u_t) = \left[db(\kappa)v\right]^2.&lt;/script&gt;

&lt;p&gt;The variance depends mainly on the curvature \(\kappa\) at the current position. If we have a low curvature, the mean of the next step will be larger. But if we take a larger step it is natural to assume, that we have a higher variance. The hand-tweaked parameter \(d\) and the step size \(v\) are fixed for the whole environment. Please be aware, that the variance is not depending on the input \(u_t\) itself, but only on the step size \(v\).&lt;/p&gt;

&lt;p&gt;If you move your mouse or your finger over the race track below, you will notice a small blue strip outside the race track. This is the probability density of our dynamic model \(p(x_{t+1}|x_t,u_t)\). Therefore, it shows the distribution over the next state \(x_{t+1}\) given the current state \(x_t\) and action \(u_t\). If you want to check out the distribution for other inputs, you can use again your keyboard or the buttons below the race track.&lt;/p&gt;

&lt;svg id=&quot;race_track_sys_dist&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;


	// defines scenes
	n_scene = load_race_track(&quot;race_track_sys_dist&quot;, &quot;https://martinseilair.github.io&quot;);
	n_scene.mode = 3;
	n_scene.me_show_system = true;

	n_scene.me_show_observation_transposed = false;
	n_scene.me_show_observation = false;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	n_scene.rc.current_input = 0;

	n_scene.loaded = function(){
		document.getElementById(&quot;race_track_sys_dist_input&quot;).style.display=&quot;block&quot;;
		this.nearest = []
		this.nearest.pos = this.rc.state;
	}.bind(n_scene)


	n_scene.mouse_touch = function(coords){
		var min_dist = 100.0;
		this.nearest = this.rt.get_nearest_pos(coords);
		if(this.nearest.distance &lt; min_dist){
			this.rt.update_car(this.nearest.pos,this.dur, 0);
			this.rt.update_strip(&quot;outer&quot;, get_system_dist_normalized(this.rc, this.rt, this.nearest.pos, this.rc.current_input));
			this.rt.show_strip(&quot;outer&quot;);	
		}else{
			this.rt.hide_strip(&quot;outer&quot;);
		}

	}.bind(n_scene)

	n_scene.update_strip_sys = function(){
		if(this.rt.is_strip_visible(&quot;outer&quot;)){
			this.rt.update_strip(&quot;outer&quot;, get_system_dist_normalized(this.rc, this.rt, this.nearest.pos, this.rc.current_input));
		}
	}.bind(n_scene)


	n_scene.key_down = function(key){
		input = key_to_input(key);
		if(input&gt;=0 &amp;&amp; input &lt;=2){
			this.rc.current_input = input;
			this.update_strip_sys();
		}
		
		
	}.bind(n_scene)
	
	scenes_name['race_track_sys_dist'] = n_scene;
	scenes.push(n_scene);
&lt;/script&gt;

&lt;div id=&quot;race_track_sys_dist_input&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=0;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=1;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=2;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 id=&quot;observation-model&quot;&gt;Observation model&lt;/h1&gt;

&lt;p&gt;The race car has a distance meter on board, which will provide us with noisy measurements of the distance to the tree inside the race track, where the position of the tree in \((x,y)\) coordinates is defined as \(T = (T_x, T_y)\). As in the model of the system dynamics, we will model the uncertainty of the measurement with a Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|x_t) = \mathcal{N}(y_t| \mu_o(x_t), \sigma_o^2(x_t))&lt;/script&gt;

&lt;p&gt;with nonlinear mean  \(\mu_o(x_t)\) and variance \(\sigma_o^2(x_t)\).&lt;/p&gt;

&lt;p&gt;The mean of our observation is the exact distance to the tree&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_o(x_t) = d(L(x_t),T),&lt;/script&gt;

&lt;p&gt;where \(d(\cdot,\cdot)\) is defined as the Euclidean distance.&lt;/p&gt;

&lt;p&gt;The variance of our measuring device&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_o^2(x_t) = \left[ad(L(x_t),T)\right]^2&lt;/script&gt;

&lt;p&gt;depends on the distance as well. The farther we are away from the tree, the more noise will be present in the signal. The parameter \(a\) is again hand-tweaked and constant.&lt;/p&gt;

&lt;p&gt;By moving with your mouse or finger over the race track below you will notice two things. Again, there appears a strip, this time inside the race track and in a brownish color. Furthermore, you will notice another light green strip at the trunk of the tree. Both strips are showing parts of our observation model \(p(y_t|x_t)\). The light green strip shows the probability of observing a distance measurement at our current position. Not surprisingly, we find the maximum probability density at the true distance of tree. Like stated above, the variance is varying depending on the distance to the tree. 
The brownish strip answers another question: Given the true distance \(d\) at our current position, what is the probability of obtaining this measurement \(d\) from another position on the race track. It represents  \(p(y_t=d|x_t)\) and is, therefore, a function of the race track \(x\). Please be aware, that this is not a proper probability density, because it is not integrating to 1.&lt;/p&gt;

&lt;svg id=&quot;race_track_obs_dist&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;

	// defines scenes
	n_scene = load_race_track(&quot;race_track_obs_dist&quot;, &quot;https://martinseilair.github.io&quot;);
	n_scene.mode = 3;
	n_scene.me_show_system = false;
	n_scene.me_show_observation_transposed = true;
	n_scene.me_show_observation = true;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	// define particle filter 


	n_scene.mouse_touch = function(coords){
		var min_dist = 100.0;
		this.nearest = this.rt.get_nearest_pos(coords);
		if(this.nearest.distance &lt; min_dist){
			this.rt.update_car(this.nearest.pos,this.dur, 0);
			this.rt.update_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.nearest.pos));
			this.rt.show_strip(&quot;inner&quot;);

			this.rt.update_dist_strip(normalize_vector(this.rc.output_dist_array(this.rt.dist_strip_domain, this.nearest.pos, 0)), this.nearest.pos, 0);
			this.rt.show_dist_strip();

		}else{
			this.rt.hide_strip(&quot;inner&quot;);
			this.rt.hide_dist_strip();
		}

	}.bind(n_scene)

	scenes.push(n_scene);
&lt;/script&gt;

&lt;p&gt;Now that we know the inner workings of our model, we are well prepared to start the series with the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-grid-based.html&quot;&gt;grid-based filter&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/d3_graphical_model.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://martinseilair.github.io/assets/js/svg_mathjax.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;new Svg_MathJax().install();&lt;/script&gt;</content><author><name></name></author><summary type="html">This post will start a series of articles that will treat common nonlinear filtering methods that are based on the Bayes filter. The motivation is to provide an intuitive understanding of these methods by deriving them directly from the general Bayes filter. This derivation is done in steps, that are supposed to be as atomic as possible. Furthermore, each nonlinear filtering method will be shown in action by providing an interactive example to play around with. This series will require some basic knowledge in math. Especially in linear algebra and probability theory.</summary></entry><entry><title type="html">The score-Fisher-information-KL connection</title><link href="https://martinseilair.github.io/jekyll/update/2018/10/17/fisher.html" rel="alternate" type="text/html" title="The score-Fisher-information-KL connection" /><published>2018-10-17T18:04:07+09:00</published><updated>2018-10-17T18:04:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/10/17/fisher</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/10/17/fisher.html">&lt;p&gt;This article is a brief summary of some relationships between the log-likelihood, score, Kullback-Leibler divergence and Fisher information. No explanations, just pure math.
&lt;!--more--&gt;&lt;/p&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_SVG&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

function getPosition(el) {
  var xPos = 0;
  var yPos = 0;
 
  while (el) {
    if (el.tagName == &quot;BODY&quot;) {
      // deal with browser quirks with body/window/document and page scroll
      var xScroll = el.scrollLeft || document.documentElement.scrollLeft;
      var yScroll = el.scrollTop || document.documentElement.scrollTop;
 
      xPos += (el.offsetLeft - xScroll + el.clientLeft);
      yPos += (el.offsetTop - yScroll + el.clientTop);
    } else {
      // for all other non-BODY elements
      xPos += (el.offsetLeft - el.scrollLeft + el.clientLeft);
      yPos += (el.offsetTop - el.scrollTop + el.clientTop);
    }
 
    el = el.offsetParent;
  }
  return {
    x: xPos,
    y: yPos
  };
}

function switch_tab(button){

    var p1 = getPosition(button)

	// get parent elements
	var bar = button.parentElement;
	var con = bar.parentElement;
	var tab_id = con.getAttribute(&quot;class&quot;).split(&quot; &quot;)[1];


	var c = bar.childNodes;
	var ci = 0;

	for (var i=0;i&lt;c.length;i++){

		if(c[i].tagName==&quot;H3&quot;){	
			if (c[i]===button){
				break;
			}
			ci++;
		}
	}
	// get tab id
	cons = document.querySelectorAll('.tab-container.' + tab_id);;

	for (var j=0;j&lt;cons.length;j++){

		// change bar
		var bar_j = cons[j].querySelector(&quot;.tab-bar&quot;);
		var buttons = bar_j.childNodes;
		var bi = 0;

		for (var i=0;i&lt;buttons.length;i++){
			if(buttons[i].tagName==&quot;H3&quot;){	
				if (bi == ci){
					buttons[i].className = &quot;tab-button-selected&quot;;
				}else{
					buttons[i].className = &quot;tab-button&quot;;
				}
				bi++;
			}
		}

		var bi = 0;

		for (var i=0;i&lt;cons[j].childNodes.length;i++){
			if(cons[j].childNodes[i].tagName==&quot;DIV&quot; &amp;&amp; cons[j].childNodes[i].className!=&quot;tab-bar&quot;){	
				if (bi == ci){
					cons[j].childNodes[i].className = &quot;tab-visible&quot;;
				}else{
					cons[j].childNodes[i].className = &quot;tab-invisible&quot;;
				}
				bi++;
			}
		}

	}

	span_cons = document.querySelectorAll('.span-container.' + tab_id );
	
	for (var j=0;j&lt;span_cons.length;j++){
		var bi = 0;
		for (var i=0;i&lt;span_cons[j].childNodes.length;i++){
			if(span_cons[j].childNodes[i].tagName==&quot;SPAN&quot;){	
				if (bi == ci){
					span_cons[j].childNodes[i].className = &quot;span-visible&quot;;
				}else{
					span_cons[j].childNodes[i].className = &quot;span-invisible&quot;;
				}
				bi++;
			}
		}
	}

	var p2 = getPosition(button)
	window.scrollBy(0,-p1.y+p2.y+15);





}

&lt;/script&gt;

&lt;style type=&quot;text/css&quot;&gt;


div.tab-container {

	width:100%;

	margin-bottom:15px;

}


div.tab-bar {
	display:inline-block;
	height:30px;

	border-bottom:solid 1px #ebebeb;
	border-left:solid 1px #ebebeb;
	border-right:solid 1px #ebebeb;
	padding:0;
	padding-bottom:2px;
	vertical-align: middle;
}


h3.tab-button-selected {
	height:100%;
	display:inline-block;
	max-width: 200px;
	padding-left:15px;
	padding-right:15px;
	border-top: 2px solid  #c90606;
	color:#c90606;
	border-bottom: 2px transparent;
	text-align:center;
	padding: 0 25px;
    line-height:30px;
    font-size:90%;
    font-weight: 500;
    text-transform: uppercase;
}

h3.tab-button {

	height:20px;
	display:inline-block;
	max-width: 200px;
	padding-left:15px;
	padding-right:15px;
	border-bottom: 2px transparent;
	border-top: 2px transparent;
	color:#757575;
	text-align:center;
	padding: 0 25px;
	line-height:0px;
	font-size:90%;
	font-weight: 500;
	font-family: 'Roboto', sans-serif;
	text-transform: uppercase;
}


div.tab-visible {

	width:100%;
	display:block;
	overflow: auto;
	background-color: #f7f7f7;
	border:solid 1px #ebebeb;

}

div.tab-invisible {

	width:100%;
	display:none;
	overflow: auto;
}

span.span-visible {
	display:inline-block;
}

span.span-invisible {
	display:none;
}

&lt;/style&gt;

&lt;h2 id=&quot;log-likelihood&quot;&gt;Log-likelihood&lt;/h2&gt;
&lt;p&gt;The log-likelihood is defined the logarithm of the likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(x;\theta')  =  \log p(x|\theta').&lt;/script&gt;

&lt;p&gt;Let’s perform a &lt;strong&gt;Taylor approximation&lt;/strong&gt; of the log-likelihood \(\log p(x|\theta’)\) around the current estimate \(\theta\):&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
	$$\begin{align*}
 \log p(x|\theta') =&amp;amp; \log p(x|\theta')|_{\theta' =  \theta} +  \sum_i \left. \frac{\partial \log p(x|\theta')}{\partial \theta'_i} \right| _{\theta' = \theta}   (\theta'_i - \theta_i) \\ &amp;amp;+ \frac{1}{2}\sum_i\sum_j \left. \frac{\partial^2 \log p(x|\theta')}{\partial \theta'_i\partial \theta'_j}\right| _{\theta' = \theta}   (\theta'_i - \theta_i)(\theta'_j - \theta_j) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
	$$\begin{align*}
\log p(x|\theta') =&amp;amp; \log p(x|\theta')|_{\theta' = \theta} + \left. \nabla_{\theta'} \log p(x|\theta')^T\right| _{\theta' = \theta} (\theta' - \theta) \\ &amp;amp;+ \frac{1}{2}\left. (\theta' - \theta)^T\nabla^2_{\theta'} \log p(x|\theta')\right| _{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;linear&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;\(\frac{\partial}{\partial \theta’_i} \log p(x|\theta’)\)&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;\(\nabla_{\theta’} \log p(x|\theta’)\)&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$  \frac{\partial}{\partial \theta'_i} \log p(x|\theta') = \frac{1}{p(x|\theta')} \frac{\partial}{\partial \theta'_i} p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \nabla_{\theta'} \log p(x|\theta') = \frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta')  $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;by using the &lt;em&gt;log derivative trick&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left. \frac{\partial}{\partial \theta'_i} \log p(x|\theta') \right|_{\theta' = \theta} = \frac{1}{p(x|\theta)} \frac{\partial}{\partial \theta_i} p(x|\theta). $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \left. \nabla_{\theta'} \log p(x|\theta') \right|_{\theta' = \theta} = \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta).  $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;quadratic&lt;/strong&gt; term of the decomposition  &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial^2 \log p(x|\theta’)}{\partial \theta’_i\partial \theta’_j}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla^2_{\theta’} \log p(x|\theta’)\)
&lt;/span&gt;&lt;/span&gt; 
can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial^2 \log p(x|\theta')}{\partial \theta'_\partial \theta'_j} =&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right) \\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')} \frac{\partial}{\partial \theta'_i} p(x|\theta')\right) \\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')}\right) \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial }{\partial \theta'_j} \left(\frac{\partial}{\partial \theta'_i} p(x|\theta')\right)\\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')}\right) \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  \\
=&amp;amp; - \frac{1}{p(x|\theta')^2} \frac{\partial}{\partial \theta'_j} p(x|\theta') \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  \\
=&amp;amp; - \frac{\partial}{\partial \theta'_j} \log p(x|\theta') \frac{\partial}{\partial \theta'_i} \log p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla^2_{\theta'} \log p(x|\theta') =&amp;amp; \nabla_{\theta'} \nabla_{\theta'}^T \log p(x|\theta') \\
=&amp;amp; \nabla_{\theta'} \left(\frac{1}{p(x|\theta')}\nabla_{\theta'}^T p(x|\theta')\right) \\
=&amp;amp; \nabla_{\theta'} p(x|\theta')  \nabla_{\theta'}^T \left(\frac{1}{p(x|\theta')}\right) +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^T\nabla_{\theta'} p(x|\theta') \\
=&amp;amp;- \frac{1}{p(x|\theta')^2} \nabla_{\theta'} p(x|\theta') \nabla_{\theta'} p(x|\theta') ^T  +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^2 p(x|\theta') \\
=&amp;amp;- \nabla_{\theta'} \log p(x|\theta') \nabla_{\theta'} \log p(x|\theta') ^T  +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^2 p(x|\theta')
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial^2 \log p(x|\theta')}{\partial \theta'_\partial \theta'_j}\right|_{\theta' = \theta} =&amp;amp; - \frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta) +    \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}  
\end{align*}.
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla^2_{\theta'} \log p(x|\theta')\right|_{\theta' = \theta} =&amp;amp;- \nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta) ^T  +  \frac{1}{p(x|\theta)}\nabla_{\theta}^2 p(x|\theta)
\end{align*}.
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, you can express the &lt;strong&gt;Taylor approximation&lt;/strong&gt; as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
	$$\begin{align*}
 \log p(x|\theta') =&amp;amp; \log p(x|\theta) +  \sum_i \frac{1}{p(x|\theta)} \frac{\partial}{\partial \theta_i} p(x|\theta)   (\theta'_i - \theta_i) \\ &amp;amp;+ \frac{1}{2} \sum_i\sum_j \left(- \nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta) ^T  +  \frac{1}{p(x|\theta)}\nabla_{\theta}^2 p(x|\theta)\right) (\theta'_i - \theta_i)(\theta'_j - \theta_j) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
	$$\begin{align*}
\log p(x|\theta') =&amp;amp; \log p(x|\theta) + \left. \nabla_{\theta'} \log p(x|\theta')^T\right| _{\theta' = \theta} (\theta' - \theta) \\ &amp;amp;+ \left. \frac{1}{2} (\theta' - \theta)^T\nabla^2_{\theta'} \log p(x|\theta')\right| _{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;mean&quot;&gt;Mean&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
 \mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')] &amp;=  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \log p(x|\theta')dx \\
&amp;= -H(\theta^*,\theta') \\
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;h1 id=&quot;variance&quot;&gt;Variance&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} \text{Var}(\log p(x|\theta')) &amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')])^2\right] \\
&amp;=  \mathbb{E}_{p(x|\theta^*)}\left[(\log p(x|\theta'))^2\right]-\mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')]^2 \\
&amp;=  \mathbb{E}_{p(x|\theta^*)}\left[(\log p(x|\theta'))^2\right]+H(\theta^*,\theta')^2 \\

\end{align*} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;score&quot;&gt;Score&lt;/h2&gt;

&lt;p&gt;The score is defined as the derivative of the log-likelihood&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ V_i(x;\theta')  =  \frac{\partial}{\partial \theta'_i} \log p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ V(x;\theta')  =  \nabla_{\theta'} \log p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;mean-1&quot;&gt;Mean&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right] =  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \frac{\partial}{\partial \theta'_i} \log p(x|\theta')dx$$

&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right] =  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \nabla_{\theta'} \log p(x|\theta')dx$$

&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;variance-1&quot;&gt;Variance&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
\begin{align*} \text{Var}\left(\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right) &amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\left(\frac{\partial}{\partial \theta'_i} \log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]\right)^2\right] \\
&amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right] +  \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]^2 \\
\end{align*} 

&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
\begin{align*} \text{Var}\left(\nabla_{\theta'} \log p(x|\theta')\right) &amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\left(\nabla_{\theta'}\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\right)\left(\nabla_{\theta'}\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\right)^T\right] \\
&amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\nabla_{\theta'} \log p(x|\theta')^T\right] +  \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]^T \\
\end{align*} 
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;kullback-leibler-divergence&quot;&gt;Kullback-Leibler divergence&lt;/h2&gt;

&lt;p&gt;The Kullback-Leibler divergence is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\textrm{KL}(\theta||\theta') = \int \limits_{-\infty}^{\infty} p(x|\theta) \log \frac{p(x|\theta)}{p(x|\theta')} dx .&lt;/script&gt;

&lt;p&gt;Let’s perform a &lt;strong&gt;Taylor approximation&lt;/strong&gt; around the current estimate \(\theta\):&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} +  \sum_i \left.\frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i}\right|_{\theta' = \theta}   (\theta'_i - \theta_i)  \\ &amp;amp;+  \frac{1}{2}\sum_i\sum_j \left.\frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j}\right|_{\theta' = \theta}  (\theta'_i - \theta_i) (\theta'_j - \theta_j) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} + \left. \nabla_{\theta'}D_\textrm{KL}(\theta||\theta')^T\right|_{\theta' = \theta}(\theta' - \theta) \\
&amp;amp; +  \frac{1}{2}\left. (\theta' - \theta)^T\nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta')\right|_{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;constant&lt;/strong&gt; term can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} =&amp;amp; 0.
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} =&amp;amp;  0.
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;linear&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial D_\textrm{KL}(\theta||\theta’)}{\partial \theta’_i}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla_{\theta’}D_\textrm{KL}(\theta||\theta’)\)
&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i} =&amp;amp; \frac{\partial}{\partial \theta'_i}\left(\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta) dx -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx\right) \\
=&amp;amp; -\int \limits_{-\infty}^{\infty} p(x|\theta) \frac{\partial}{\partial \theta'_i}\log p(x|\theta') dx\\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\frac{1}{p(x|\theta')}\frac{\partial}{\partial \theta'_i} p(x|\theta') dx \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]\\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta')}\frac{\partial}{\partial \theta'_i} p(x|\theta')\right].
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla_{\theta'}D_\textrm{KL}(\theta||\theta') =&amp;amp;  \nabla_{\theta'}\left(\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta) dx -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx\right) \\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'} \log p(x|\theta') dx \\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta') dx \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta')\right].
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i} \right|_{\theta' = \theta} =&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i}p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \frac{\partial}{\partial \theta_i} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i} 1 \\
=&amp;amp; 0 
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla_{\theta'}D_\textrm{KL}(\theta||\theta') \right|_{\theta' = \theta} =&amp;amp;   - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \nabla_{\theta} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta} 1 \\
=&amp;amp; 0 
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;quadratic&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial^2 D_\textrm{KL}(\theta||\theta’)}{\partial \theta’_i\partial \theta’_j}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla^2_ {\theta’}D_\textrm{KL}(\theta’||\theta’)\)
&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j} =&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{\partial}{\partial \theta'_i} D_\textrm{KL}(\theta||\theta')\right) \\
=&amp;amp; -\frac{\partial }{\partial \theta'_j} \left( \int \limits_{-\infty}^{\infty} p(x|\theta)\frac{\partial}{\partial \theta'_i} \log p(x|\theta') dx \right) \\
=&amp;amp; - \int \limits_{-\infty}^{\infty} p(x|\theta)\frac{\partial^2  \log p(x|\theta')}{\partial \theta'_i  \partial \theta'_j}   dx \\
=&amp;amp; -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta')}{\partial \theta'_i  \partial \theta'_j}\right]
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta') =&amp;amp; \nabla_{\theta'} \nabla_{\theta'}^T D_\textrm{KL}(\theta||\theta') \\
=&amp;amp; -\nabla_{\theta'} \left( \int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'}^T \log p(x|\theta') dx \right) \\
=&amp;amp; - \int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'}^2 \log p(x|\theta') dx \\
=&amp;amp; -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'}^2 \log p(x|\theta')\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j} \right|_{\theta' = \theta} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right]
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta') \right|_{\theta' = \theta} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, you can express the &lt;strong&gt;Taylor approximation&lt;/strong&gt; as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; -\frac{1}{2}\sum_i\sum_j  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j} \right]  (\theta'_i - \theta_i) (\theta'_j - \theta_j) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; -\frac{1}{2} (\theta' - \theta)^T\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right](\theta' - \theta) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross entropy&lt;/h2&gt;

&lt;p&gt;The cross entropy is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\theta,\theta') = -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx .&lt;/script&gt;

&lt;p&gt;The cross entropy and the Kullback-Leibler differ only by the constant entropy \(H(\theta\). Therefore, the linear and quadratic terms of those are the same.&lt;/p&gt;

&lt;h2 id=&quot;fisher-information&quot;&gt;Fisher Information&lt;/h2&gt;
&lt;h1 id=&quot;the-fisher-definition-can-be-defined-as-&quot;&gt;The fisher definition can be defined as …&lt;/h1&gt;

&lt;h1 id=&quot;-expectation-of-the-squared-score&quot;&gt;… expectation of the squared score&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-variance-of-the-score&quot;&gt;… variance of the score&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij} &amp;amp;=  \text{Var}\left(\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right) \\
 &amp;amp;=  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] +  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right]^2 \\
\end{align*} 
 $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ 
\begin{align*}
\mathcal{I(\theta)} &amp;amp;=  \text{Var}\left(\nabla_{\theta'} \log p(x|\theta')\right) \\
&amp;amp;=  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\nabla_{\theta'} \log p(x|\theta')^T\right] +  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]^T \\
\end{align*} 
 $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*} \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] =&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i}p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \frac{\partial}{\partial \theta_i} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i} 1 \\
=&amp;amp; 0 
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*} \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta)\right] =&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \nabla_{\theta} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta} 1 \\
=&amp;amp; 0 
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;follows&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-curvature-of-the-kl-divergence&quot;&gt;… curvature of the KL-divergence&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij} =&amp;amp;\left. \frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta_i\partial \theta_j} \right|_{\theta' = \theta}\\
 =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right] \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[-\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta) +    \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j}\right] \\
=&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] -    \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\mathcal{I(\theta)} =&amp;amp;\left. \nabla^2_{\theta}D_\textrm{KL}(\theta||\theta) \right|_{\theta = \theta}\\
 =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
 =&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[-\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T +    \frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right] \\
=&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] -    \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}\right]=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j} dx  \\
=&amp;amp; \int \limits_{-\infty}^{\infty}  \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j} dx  \\
=&amp;amp;  \frac{\partial^2 }{\partial \theta_i\partial \theta_j}\int \limits_{-\infty}^{\infty} p(x|\theta)  dx  \\
=&amp;amp;  \frac{\partial^2 }{\partial \theta_i\partial \theta_j}1  \\
=&amp;amp; 0 .
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*} 
\mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right]=&amp;amp;  \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta) dx  \\
=&amp;amp;  \int \limits_{-\infty}^{\infty}  \nabla^2_{\theta} \log p(x|\theta) dx  \\
=&amp;amp;  \nabla^2_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta)  dx  \\
=&amp;amp;  \nabla^2_{\theta}1   \\
=&amp;amp;0
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;follows&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-curvature-of-the-cross-entropy&quot;&gt;… curvature of the cross entropy&lt;/h1&gt;

&lt;p&gt;Cross entropy and Kullback-Leibler divergence only differ by constant additive term. Therefore, curvature has to be identical.&lt;/p&gt;

&lt;h1 id=&quot;-expected-negative-curvature-of-log-likelihood&quot;&gt;… expected negative curvature of log-likelihood&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij}  =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right] \\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\mathcal{I(\theta)} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;em&gt;curvature of the KL-divergence&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;-expected-value-of-the-observed-information&quot;&gt;… expected value of the observed information&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\left[\mathcal{J(\theta)}\right]_{ij}\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\mathcal{J(\theta)}\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the observed information \(\mathcal{J(\theta)}\) is defined as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{J(\theta)}\right]_{ij} =  -\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j} $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{J(\theta)} =  -\nabla_{\theta}^2 \log p(x|\theta) $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We note that this case has the same form as the curvature of the KL-divergence.&lt;/p&gt;</content><author><name></name></author><summary type="html">This article is a brief summary of some relationships between the log-likelihood, score, Kullback-Leibler divergence and Fisher information. No explanations, just pure math.</summary></entry><entry><title type="html">Derivation of the Kalman filter</title><link href="https://martinseilair.github.io/jekyll/update/2018/10/10/kalman_filter.html" rel="alternate" type="text/html" title="Derivation of the Kalman filter" /><published>2018-10-10T19:35:07+09:00</published><updated>2018-10-10T19:35:07+09:00</updated><id>https://martinseilair.github.io/jekyll/update/2018/10/10/kalman_filter</id><content type="html" xml:base="https://martinseilair.github.io/jekyll/update/2018/10/10/kalman_filter.html">&lt;p&gt;The concept and the equations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kalman_filter&quot;&gt;Kalman filter&lt;/a&gt; can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_inference&quot;&gt;Bayesian inference&lt;/a&gt;. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.
&lt;!--more--&gt;&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
    function draw_ssm(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});
        state_ns.push({title: &quot;\\( x_&quot; + t +&quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:&quot;#FFFFFF&quot;});
        output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});


        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

  	nodes = input_ns.concat(state_ns).concat(output_ns);
    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_ind(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 120;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 4;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;T-1) ind+=(t-T+1)


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = (t==T-1) ? &quot;#e3e5feff&quot; :statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }


    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}

    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);

  	var svg_w = margin_x + dist_x*(T-1) + 50;
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_indi(svg){

      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 100;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;2) ind+=(t-2)
      	if (t&gt;2) ind+=&quot;+&quot;+(t-2)


      	input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = (t==2) ? &quot;#e3e5feff&quot; :statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

    estate_h = {x: state_ns[T-1].x + radius*4, y: state_ns[T-1].y}
    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    einput_h = {x: input_ns[T-1].x + 2*Math.sqrt(2)*radius, y: input_ns[T-1].y+ 2*Math.sqrt(2)*radius}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}


    edges.push({source: state_ns[T-1], target: estate_h, dash:&quot;5,5&quot;})
    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: input_ns[T-1], target: einput_h, dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);
    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

	function draw_ssm_obs(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});
        state_ns.push({title: &quot;\\( x_&quot; + t +&quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:&quot;#FFFFFF&quot;});

        if (t==0||t==4) output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
        if (t==2) {
        	output_ns.push({title: &quot;\\( z_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*(t+0.5) , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
			output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*(t-0.5) , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
        }
        



        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

      edges.push({source: state_ns[0], target: output_ns[0], dash:&quot;&quot;})
      edges.push({source: state_ns[2], target: output_ns[1], dash:&quot;&quot;})
      edges.push({source: state_ns[2], target: output_ns[2], dash:&quot;&quot;})
      edges.push({source: state_ns[4], target: output_ns[3], dash:&quot;&quot;})

  	nodes = input_ns.concat(state_ns).concat(output_ns);

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);

    }
 &lt;/script&gt;

&lt;script src=&quot;//d3js.org/d3.v3.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;First of all, let’s try to formulate the main idea of Kalman filtering in one sentence:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;Kalman filter&lt;/strong&gt; is used to &lt;strong&gt;infer&lt;/strong&gt; the current state of a &lt;strong&gt;linear Gaussian state space model&lt;/strong&gt; given all observations and inputs up to the current timestep and a Gaussian prior distribution of the initial state.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Indeed, the process of Kalman filtering is simply Bayesian inference in the domain of linear Gaussian state space models. The information encoded in our formulation is sufficient to uniquely define what the Kalman filter should output. But it doesn’t tell us anything about how to compute it. In this article, we will find an efficient recursive method that will lead us to the familiar equations.&lt;/p&gt;

&lt;p&gt;Let’s get started with the derivation by defining &lt;em&gt;linear Gaussian state space models&lt;/em&gt; and &lt;em&gt;Bayesian inference&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;linear-gaussian-state-space-models&quot;&gt;Linear Gaussian state space models&lt;/h2&gt;

&lt;p&gt;A linear Gaussian state space model is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= A_tx_t + B_t u_t + w_t \\ 
y_t &amp;= C_tx_t + v_t \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A_t\), input matrix \(B_t\), output matrix \(C_t\), Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).&lt;/p&gt;

&lt;p&gt;Alternatively, we can use the probability density functions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;\sim \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\\ 
 y_t &amp;\sim \mathcal{N}(y_t|C_tx_t, R_t)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;to describe the system in a more compact fashion.&lt;/p&gt;

&lt;p&gt;Linear Gaussian state space models can also be described in the language of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_model&quot;&gt;probabilistic graphical models&lt;/a&gt; (or more precisely in the language of &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Bayesian networks&lt;/a&gt;). The figure below shows such a model up to time \(T = 4\).&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Every node represents a random variable and the edges are representing conditional dependencies between the respective nodes. Random variables, that are observed (or given) are shaded in light blue. In our case this is the output \(y_t\) and the input \(u_t\). The state \(x_t\) is not observed (or latent).&lt;/p&gt;

&lt;h2 id=&quot;bayesian-inference&quot;&gt;Bayesian inference&lt;/h2&gt;

&lt;p&gt;In simplest terms Bayesian inference tries to update a hypothesis/belief of something, that is not directly observable, in the face of new information by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes’ rule&lt;/a&gt;. A bit more formal: The goal is to update the prior distribution \(p(x)\) given new data \(\mathcal{D}\) to obtain the posterior distribution \(p(x|\mathcal{D})\) with help of Bayes rule&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\mathcal{D}) = \frac{p(\mathcal{D}|x)p(x)}{p(\mathcal{D})}&lt;/script&gt;

&lt;p&gt;with likelihood \(p(\mathcal{D}|x)\) and evidence \(p(\mathcal{D})\).&lt;/p&gt;

&lt;p&gt;This idea is very general and can be applied to dynamical models quite easily. The most common inference tasks in dynamical models are filtering, smoothing and prediction. These methods differ only in the form of the posterior distribution.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt;: What is my belief about the &lt;strong&gt;current state&lt;/strong&gt; \(x_t\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; 	$$ p(x_t|y_0,...,y_t,u_0,...,u_{t-1})  $$ &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt;: What is my belief about &lt;strong&gt;all states&lt;/strong&gt; \(x_t, … ,x_0\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; $$ p(x_t,...,x_0|y_0,...,y_t,u_0,...,u_{t-1}) $$ &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: What is my belief about the &lt;strong&gt;next state&lt;/strong&gt; \(x_{t+1}\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; $$ p(x_{t+1}|y_0,...,y_t,u_0,...,u_{t-1}) $$ &lt;/div&gt;

&lt;p&gt;The name Kalman &lt;em&gt;filter&lt;/em&gt; reveals, that we will be interested in the filtering problem. Therefore, we want to infer the current state \(x_t\) based on all recent observations \(y_0,…,y_t\) and inputs \(u_0,…,u_{t-1}\).
Now that we have defined what we are looking for, let’s try to find a way to efficiently calculate it. We will start by finding a recursive method for &lt;em&gt;general&lt;/em&gt; dynamical models defined by the probabilistic graphical model above.&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter-for-state-space-models&quot;&gt;Bayes filter for state space models&lt;/h2&gt;

&lt;p&gt;We have the task to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \). For this purpose only the structure of the graphical model will matter: it governs the conditional dependencies.
To unclutter the notation we will use \(\Box_{n:m}\) for \(\Box_n,…,\Box_m\).&lt;/p&gt;

&lt;p&gt;With help of &lt;strong&gt;Bayes’ rule&lt;/strong&gt; we can rewrite the formula as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t,y_{0:t-1},u_{0:t-1})p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.&lt;/script&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;If you are not very familiar with Bayes’ rule this can be quite confusing. There are much more moving parts than in the very simple definition. Nonetheless, there is an intuitive explanation.
It is Bayes’ rule applied in a world, where we already observed \(\mathcal{W}\) in the past (every term is conditioned on \(\mathcal{W}\)):&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\mathcal{D},\mathcal{W}) = \frac{p(\mathcal{D}|x,\mathcal{W})p(x|\mathcal{W})}{p(\mathcal{D}|\mathcal{W})}.&lt;/script&gt;

  &lt;p&gt;In our case \(x:=x_t \), \(\mathcal{D}:=y_t \) and \(\mathcal{W}:=(y_{0:t-1},u_{0:t-1}) \).&lt;/p&gt;

&lt;/div&gt;
&lt;p&gt;We note that \(y_t\) is independent of \(y_{0:t-1}\) and  \(u_{0:t-1}\) given \(x_t\). It follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.&lt;/script&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;This conditional independence property is not obvious as well. When it comes to conditional dependencies, it is always a good idea to look at the graphical model.&lt;/p&gt;

  &lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm_ind(this);&quot;&gt;&lt;/svg&gt;
  &lt;p&gt;In the figure above we notice that the node \(x_t\) is shaded (observed). This node blocks the way of \(y_{0:t-1}\) and  \(u_{0:t-1}\) to \(y_t\). We have proven the conditional independence &lt;em&gt;visually&lt;/em&gt;. You can learn more about conditional independence in probabilistic graphical models in &lt;a href=&quot;https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt; (Chapter 8.2).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The denominator is simply the integral of the numerator&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|y_{0:t-1},u_{0:t-1}) = \int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) dx_t .&lt;/script&gt;

&lt;p&gt;Great! We successfully expressed our equation in simpler terms. In return, we obtained the new expression \(p(x_t|y_{0:t-1},u_{0:t-1})\), which we have to calculate as well. Using marginalization we can express it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t,x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;We can split the expression in the integral with product rule, which leads to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1},y_{0:t-1},u_{0:t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;Note that \(x_t\) is independent of \(y_{0:t-1}\) and \(u_{0:t-2}\) given \(x_{t-1}\). Furthermore, \(x_{t-1}\) is independent of \(u_{t-1}\). We obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1}, u_{t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-2}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;We note that \(p(x_{t-1}|y_{0:t-1},u_{0:t-2})\) has the same form as our expression we started from only shifted by one time step. Our recursive formula is complete!&lt;/p&gt;

&lt;p&gt;Let’s summarize our results!&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Bayes filter for state space models&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with the prior distribution over the initial state \(p(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Up to this point, we assumed that we obtain exactly one observation at every timestep. This rather limiting assumption is violated in many real-life scenarios. Multiple or even no observations per timestep are possible. This behavior is exemplified in the probabilistic graphical model below.&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm_obs(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Fortunately, handling these cases is very simple. For every observation we make, we calculate the update step with the newest estimate available. Furthermore, it is not necessary that the observations are coming from the same output function (illustrated by the outputs \(y_2\) and \(z_2\) at \(t=2\)). &lt;a href=&quot;https://en.wikipedia.org/wiki/Information_integration&quot;&gt;Information integration/fusion&lt;/a&gt; is very natural in Bayesian inference.&lt;/p&gt;

&lt;p&gt;Nice! We just derived the equations of the Bayes filter for general state space models!
Now let’s translate this into the linear state space scenario.&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter-in-linear-gaussian-state-space-models&quot;&gt;Bayes filter in linear Gaussian state space models&lt;/h2&gt;

&lt;p&gt;Let’s start by identifying the probability distributions we already know:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &amp;= \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t) \\
p(y_t|x_t) &amp;=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Furthermore, we assume that the prior distribution of the initial state is Gaussian as well. All probability distributions in our model are Gaussian. Therefore, the distributions \(p(x_t|y_{0:t-1},u_{0:t-1})\) and \(p(x_t|y_{0:t},u_{0:t-1})\) will also be in form of Gaussian distributions, because our recursive formula is only using marginalization and Bayes’ rule, which are closed under Gaussian distributions. In the context of Kalman filtering, these are normally defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_t|y_{0:t},u_{0:t-1}) &amp;:= \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t}) \\
p(x_t|y_{0:t-1},u_{0:t-1}) &amp;:= \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) .
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Please note, that these distributions are still implicitly dependent on the inputs and outputs. The mean and the covariance are a &lt;em&gt;sufficient statistic&lt;/em&gt; of the in- and outputs.&lt;/p&gt;

&lt;p&gt;The index \(\Box_{n|m}\) of the parameters indicates that the state at time \(n\) is estimated, based on the outputs upto time \(m\).
The expression \(\hat x_{t|t}\) is called the &lt;em&gt;updated&lt;/em&gt; state estimate and \( P_{t|t}\) the &lt;em&gt;updated&lt;/em&gt; error covariance. Moreover, \(\hat x_{t|t-1}\) is called the &lt;em&gt;predicted&lt;/em&gt; state estimate and \( P_{t|t-1}\) the &lt;em&gt;predicted&lt;/em&gt; error covariance.&lt;/p&gt;

&lt;p&gt;In summary, these are the equations for the Bayes filter in linear Gaussian state space models:&lt;/p&gt;
&lt;div class=&quot;important_box&quot;&gt;

  &lt;p&gt;&lt;strong&gt;Prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

  &lt;p&gt;&lt;strong&gt;Update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;/div&gt;

&lt;p&gt;Let’s try to simplify these equations!&lt;/p&gt;

&lt;h3 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h3&gt;

&lt;p&gt;We will start with the prediction step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

&lt;p&gt;In order to find a closed form solution of this integral, we could simply plug in the corresponding expressions of the Gaussian distributions and solve the integral. Fortunately, Marc Toussaint already gathered the most important &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Gaussian identities&lt;/a&gt;, which will lighten our workload a lot.  To find an expression for our prediction step we can simply use the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dy = \mathcal{N}(x|a + Fb, A + FBF^T ).&lt;/script&gt;

&lt;p&gt;By comparison with our expression, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\hat x_{t+1|t} &amp;=  A_t \hat x_{t|t} + B_tu_t \\
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T.
\end{align} %]]&gt;&lt;/script&gt;

&lt;h3 id=&quot;update-step&quot;&gt;Update step&lt;/h3&gt;

&lt;p&gt;We will start to simplify the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A &amp; A^TF^T\\FA &amp; B + FA^TF^T\end{matrix}\right) . %]]&gt;&lt;/script&gt;

&lt;p&gt;Then again, this joint distribution can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D &amp; F\\F^T &amp; E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) . %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine the two previous equations to the following expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .&lt;/script&gt;

&lt;p&gt;By comparison with the numerator of our update step, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})\mathcal{N}(y_{t}|C_tx_{t}, R_t ) = \mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}),  P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;At a first glance, this is not looking like a simplification at all. Conceptually, we only transformed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}.&lt;/script&gt;

&lt;p&gt;If we look closely at the final expression, we see that \(p(y)\) is canceling out. Therefore, the result is simply the remaining part&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}),P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;If our reasoning is correct the denominator should be equal to \(\mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)\), which was canceled out. The denominator can be simplified with the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|C_t\hat x_{t|t-1}, R_t + C_tP_{t|t-1}C_t^T ).&lt;/script&gt;

&lt;p&gt;Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.&lt;/p&gt;

&lt;p&gt;Let’s summarize our results:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Bayes filter in linear Gaussian state space models&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in linear Gaussian state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T  \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t|t} &amp;= \hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}) \\ 
P_{t|t} &amp;= P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}.  \end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;
&lt;p&gt;That’s it! We derived the equations of the Bayes filter in linear Gaussian state space models, which is nothing else but the good old Kalman filter.
In the next section, we will split these equations up to finally obtain the formulation normally used for the Kalman filter.&lt;/p&gt;

&lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman filter&lt;/h2&gt;

&lt;p&gt;In order to obtain the familiar equations of the Kalman filter we have to define&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Innovation&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;z_t = y_{t}-C_t\hat x_{t|t-1}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Innovation covariance&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_t = R_t + C_tP_{t|t-1}C_t^T&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimal Kalman gain&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;K_t = P_{t|t-1}C_t^TS_t^{-1}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;extra_box&quot;&gt;

  &lt;p&gt;&lt;strong&gt;What is the meaning of \(z_t\) and \(S_t\)?&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;The denominator of the update step is&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(y_{t}|C_t\hat x_{t|t-1},R_t + C_tP_{t|t-1}^TC_t^T)&lt;/script&gt;

  &lt;p&gt;and can be transformed by (Formula 34, Toussaint)&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a,A) = \mathcal{N}(x+f|a+f,A)&lt;/script&gt;

  &lt;p&gt;to obtain the expression&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(y_{t} - C_t\hat x_{t|t-1}|0,R_t + C_tP_{t|t-1}^TC_t^T) = \mathcal{N}(z_t|0,S_t).&lt;/script&gt;

  &lt;p&gt;Therefore, the innovation \(z_t\) is the deviation of the expected output and the observed output.
The random variable \(z_t\) has a Gaussian distribution with zero mean and variance \(S_t\).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Let’s plug these definitions into the equations of our update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat x_{t|t} = \hat x_{t|t-1} + \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}(\underbrace{y_{t}-C_t\hat x_{t|t-1}}_{z_t})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t|t} = P_{t|t-1} - \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}P_{t|t-1} .&lt;/script&gt;

&lt;p&gt;This leads us to the final equations of the Kalman filter.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Equations of the Kalman filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Kalman filter consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_t &amp;= y_{t}-C_t\hat x_{t|t-1}\\
S_t &amp;= R_t + C_tP_{t|t-1}C_t^T\\
K_t &amp;= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &amp;= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &amp;= (I - K_tC_t)P_{t|t-1}.
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This article presented the derivation of the Kalman filter from first principles using Bayesian inference. The goal was to derive the Kalman filter in a clear and straightforward fashion. The steps were designed to be as atomic as possible, in order to be comprehensible for readers, who are not so familiar with the tools we used. Summarized, the derivation was performed in the following four subsequent steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We realized, that we have to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \).&lt;/li&gt;
  &lt;li&gt;Derived the recursive equations of the Bayes filter to efficiently calculate this distribution.&lt;/li&gt;
  &lt;li&gt;Inserted the corresponding distributions of the linear Gaussian state space model.&lt;/li&gt;
  &lt;li&gt;Added some “sugar” to obtain the usual equations of the Kalman filter.&lt;/li&gt;
&lt;/ol&gt;

&lt;script src=&quot;https://martinseilair.github.io/assets/js/d3_graphical_model.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://martinseilair.github.io/assets/js/svg_mathjax.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;new Svg_MathJax().install();&lt;/script&gt;</content><author><name></name></author><summary type="html">The concept and the equations of the Kalman filter can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using Bayesian inference. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.</summary></entry></feed>