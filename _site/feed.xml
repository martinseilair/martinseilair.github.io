<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.4">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2018-10-29T19:02:16+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ikigai</title><subtitle>A dump for random thoughts and observations</subtitle><entry><title type="html">Nonlinear filtering: Grid-based filter</title><link href="http://localhost:4000/jekyll/update/2018/10/29/nf-grid-based.html" rel="alternate" type="text/html" title="Nonlinear filtering: Grid-based filter" /><published>2018-10-29T18:05:07+09:00</published><updated>2018-10-29T18:05:07+09:00</updated><id>http://localhost:4000/jekyll/update/2018/10/29/nf-grid-based</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/10/29/nf-grid-based.html">&lt;p&gt;The process of Bayes filtering requires to solve integrals, that are in general intractable. One approach to circumvent this problem is the use of grid-based filtering. In this article, we will derive this method directly from the recursive equations of the Bayes filter.  &lt;!--more--&gt;This marks the first part of the nonlinear filtering series. If you haven’t read the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-intro.html&quot;&gt;introduction&lt;/a&gt;, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind grid-based filtering.&lt;/p&gt;

&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; async=&quot;&quot; src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://localhost:4000/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;grid-based filter&lt;/strong&gt; approximates the Bayes filter by &lt;strong&gt;restricting&lt;/strong&gt; and &lt;strong&gt;discretizing&lt;/strong&gt; the state, input and observation space, to obtain &lt;strong&gt;finite&lt;/strong&gt; domains.&lt;/p&gt;
&lt;/div&gt;

&lt;h2 id=&quot;derivation&quot;&gt;Derivation&lt;/h2&gt;

&lt;p&gt;We will start the derivation directly from the recursive equations of the Bayes filter with the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

&lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

&lt;p&gt;The first and most important step in order to arrive at the equations of the grid-based filter is to discretize our system and observation model. Given a set of discretization points \((x^1,\ldots,x^X)\), \((y^1,\ldots,y^Y)\) and \((u^1,\ldots,u^U)\) the result of the discretization can be written as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|x_{t}, u_{t}) = \frac{1}{Z}p(x_{t+1}|x_{t}, u_{t})\sum_{k=1}^X\sum_{l=1}^X \sum_{m=1}^U  \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t)&lt;/script&gt;

&lt;p&gt;for the system model and&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t) = \frac{1}{Z}p(y_{t}|x_{t})\sum_{k=1}^Y\sum_{l=1}^X  \delta_{y^k}(y_{t})\delta_{x^l}(x_t)&lt;/script&gt;

&lt;p&gt;for our observation model, where \(Z\) is a normalization factor.&lt;/p&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;These equations can look confusing and random. Especially if you are not familiar with the &lt;a href=&quot;https://en.wikipedia.org/wiki/Dirac_delta_function&quot;&gt;Dirac delta function&lt;/a&gt;. This box will give a short introduction to the Dirac delta function and provide the tools you will need. &lt;strong&gt;Warning:&lt;/strong&gt; The Dirac delta function is actually not a function, but a distribution or a measure!&lt;/p&gt;

  &lt;p&gt;The Dirac delta function \(\delta(x)\) can be &lt;em&gt;imagined&lt;/em&gt; as a function, that is &lt;em&gt;infinite&lt;/em&gt; at \(x=0\) and &lt;em&gt;zero&lt;/em&gt; at \(x \neq 0\) with the property&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_x \delta(x)\,dx = 1.&lt;/script&gt;

  &lt;p&gt;It is not possible to plot the Dirac delta function, but we can visualize it schematically by using an arrow pointing upwards, as shown in the figure below.&lt;/p&gt;
  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;The length of the line represents the area under the function. The expression \(\delta(x-y)\) is a shifted version of the Dirac delta function with its peak at \(y\).&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_shift&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;To unclutter the notation, we will use the shorthand \(\delta_y(x)\) for \(\delta(x-y)\).&lt;/p&gt;

  &lt;p&gt;The &lt;em&gt;sifting property&lt;/em&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_x f(x)\delta_y(x)\,dx = f(y)&lt;/script&gt;

  &lt;p&gt;will play an important role in our derivation. Intuitively, it is sifting out the function value of \(f(x)\) at \(y\).&lt;/p&gt;

  &lt;p&gt;The multivariate Dirac delta function, which can be &lt;em&gt;loosely&lt;/em&gt; thought of as &lt;em&gt;infinite&lt;/em&gt; at \((x_1,\ldots,x_n)=(0,\ldots,0)\) and &lt;em&gt;zero&lt;/em&gt; at \((x_1,\ldots,x_n) \neq (0,\ldots,0)\) is defined as the product of several Dirac delta functions:&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta(x_1,\ldots,x_n) = \delta(x_1)* \ldots *\delta(x_n).&lt;/script&gt;

  &lt;p&gt;Several Dirac delta functions can also be combined to form a comb or grid, which is shown in the following figure.&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_comb&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;To obtain such a grid, the Dirac delta functions are simply added together&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^N\delta_{x_i}(x).&lt;/script&gt;

  &lt;p&gt;Finally, we can multiply the grid with a function \(f(x)\) :&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{i=1}^Nf(x)\delta_{x_i}(x).&lt;/script&gt;

  &lt;p&gt;The particular Dirac delta functions are weighted by the functional value at the corresponding point. This can be visualized as following.&lt;/p&gt;

  &lt;div style=&quot;text-align:center; width:100%;display:inline-block;&quot;&gt;&lt;div id=&quot;dirac_wcomb&quot; style=&quot;width:85%;display:inline-block;&quot;&gt;&lt;/div&gt;&lt;/div&gt;

  &lt;p&gt;Now that we have a better understanding of the Dirac delta function, let’s look again at our system and observation model. In our system model, we find a multivariate Dirac delta function&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t),&lt;/script&gt;

  &lt;p&gt;where the particular Dirac delta functions could be multivariate as well. We also note, that the multivariate Dirac delta function is embedded in a triple sum over the corresponding discrete spaces. It describes, therefore, a grid of Dirac delta functions.
Finally, we multiply this grid with our system model. To obtain a proper probability distribution, we normalize our distribution with \(Z\).
The observation model can be treated likewise.&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;If we start the recursion with a discretized prior distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_0) = \sum_{k=1}^X w^k \delta_{x^k}(x_0)&lt;/script&gt;

&lt;p&gt;all of our belief distributions will be discretized as well.
Therefore, the posterior will have the form&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t}|\cdot) = \sum_{k=1}^X w^k \delta_{x^k}(x_t).&lt;/script&gt;

&lt;h1 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h1&gt;

&lt;p&gt;Let’s see what happens, if we plug the discretized version of our system model into the equations of the prediction step:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} \frac{1}{Z}p(x_{t+1}|x_{t}, u_{t})\sum_{k=1}^X\sum_{l=1}^X\sum_{m=1}^U \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\delta_{u^m}(u_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;First of all, we know our current input is \(u^i\). To evaluate \(\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) \) at \(u^i\), we simply have to compute \(\int_{u_t}\hat{p}(x_{t+1}|y_{0:t},u_{0:t})\delta_{u^i}(u_t)\,d_t \). As a result, we will obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{k=1}^X\sum_{l=1}^X  \delta_{x^k}(x_{t+1})\delta_{x^l}(x_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;The following expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{l=1}^X \delta_{x^l}(x_t)\sum_{n=1}^X w^n \delta_{x^n}(x_t)&lt;/script&gt;

&lt;p&gt;is not zero only if \(l = n\). Therefore, we can replace this expression with&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_{n=1}^X w^n \delta_{x^n}(x_t)&lt;/script&gt;

&lt;p&gt;and obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{k=1}^X\sum_{n=1}^X  \delta_{x^k}(x_{t+1}) w^n \delta_{x^n}(x_t) dx_{t} .&lt;/script&gt;

&lt;p&gt;By rearranging the integral and sums we  obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\int_{x_{t}}p(x_{t+1}|x_{t}, u_t=u^m) \sum_{n=1}^X   w^n \delta_{x^n}(x_t) dx_{t}&lt;/script&gt;

&lt;p&gt;Using the sifting property we will arrive at&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\sum_{n=1}^Xp(x_{t+1}|x_{t} = x^n, u_t=u^m)    w^n&lt;/script&gt;

&lt;h1 id=&quot;update-step&quot;&gt;Update step&lt;/h1&gt;

&lt;p&gt;Let’s start our treatment of the update step and by looking at the numerator. Again, we plug in our discretized distributions to obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{Z}p(y_{t}|x_{t})\sum_{k=1}^Y\sum_{l=1}^X  \delta_{y^k}(y_{t})\delta_{x^l}(x_t)\sum_{m=1}^X w^m \delta_{x^m}(x_t).&lt;/script&gt;

&lt;p&gt;We know, that we have an observation \(y^k\), which we can plug in in the same way as the input above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{Z}p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)&lt;/script&gt;

&lt;p&gt;The denominator is simply the integral over \(x_t\) of the expression above:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_t}\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t = \frac{1}{Z}\int_{x_t}p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t) \,dx_t.&lt;/script&gt;

&lt;p&gt;With help of the sifting property, we obtain the final expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_t}\hat{p}(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t = \frac{1}{Z}\sum_{m=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)&lt;/script&gt;

&lt;p&gt;for the denominator.&lt;/p&gt;

&lt;p&gt;The full update step is then defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)}{\sum_{l=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)} .&lt;/script&gt;

&lt;p&gt;Let’s summarize our results!&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Grid-based filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the grid-based filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{Z}\sum_{k=1}^X\delta_{x^k}(x_{t+1})\sum_{n=1}^Xp(x_{t+1}|x_{t} = x^n, u_t=u^m)    w^n&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{p}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_{t}=y^k|x_{t})\sum_{m=1}^Xw^m\delta_{x^m}(x_t)}{\sum_{l=1}^Xw^mp(y_{t}=y^k|x_{t}=x^k)} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with a discrete prior distribution over the initial state \(\hat{p}(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;h1 id=&quot;discrete-probability-distribution&quot;&gt;Discrete probability distribution&lt;/h1&gt;

&lt;p&gt;Our discretized system and observation model is non-zero only at the points on the grid. Nonetheless, the model itself is still continuous: we can evaluate the function at points, which are not part of the grid. Then again, the posterior will always remain on the grid during the filtering process. Thus, we can represent our system as a discrete probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_{t+1}=k|x_t=l,u=m) = \frac{1}{Z(l,m)} p(x_{t+1}=x^k|x_{t}=x^l, u_t=u^m)&lt;/script&gt;

&lt;p&gt;with the normalization factor \(Z(l,m) = \sum_{k=1}^X p(x_{t+1}=x^k|x_{t}=x^l, u_t=u^m) \).&lt;/p&gt;

&lt;p&gt;Similarly, the observation model can be represented by the discrete probability distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(y_{t}=k|x_t=l) = \frac{1}{Z(l)} p(y_{t}=y^k|x_{t}=x^l)&lt;/script&gt;

&lt;p&gt;with the normalization factor \(Z(l) = \sum_{k=1}^X p(y_{t}=y^k|x_{t}=x^l) \). With this representation, we arrive at the Bayes filter for discrete probability distributions.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Discrete Bayes filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the discrete Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_{t+1}|y_{0:t},u_{0:t}) = \sum_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1})&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;P(x_t|y_{0:t},u_{0:t-1}) = \frac{P(y_t|x_t)P(x_t|y_{0:t-1},u_{0:t-1})}{\sum_{x_t} P(y_t|x_t)P(x_t|y_{0:t-1},u_{0:t-1})} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with a discrete prior distribution over the initial state \(P(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;h2 id=&quot;example&quot;&gt;Example&lt;/h2&gt;

&lt;p&gt;Enough of the dry theory! Let’s play around with the grid-based filter in our race track example.&lt;/p&gt;

&lt;svg id=&quot;race_track_mar_loc&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;
&lt;script&gt;


	n_scene = load_race_track(&quot;race_track_mar_loc&quot;,&quot;http://localhost:4000&quot;,400);
	n_scene.mode = 2;
	n_scene.filter = &quot;bayes&quot;;
	n_scene.dur=slow_dur;
	// define particle filter 

	n_scene.auto_start = false;

	n_scene.t = 1;

	n_scene.ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];

	n_scene.loaded = function(){
		//var ids = [&quot;race_track_mar_loc_likelihood&quot;, &quot;race_track_mar_loc_update&quot;,&quot;race_track_mar_loc_timestep&quot;, &quot;race_track_mar_loc_predict&quot; ];
		//for (var i=0; i&lt;ids.length;i++){

		//	document.getElementById(ids[i]).style.display=&quot;none&quot;;
		//}
		document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
		this.rt.hide_strip(&quot;inner&quot;);


		this.restart = function(){
			for (var i=0; i&lt;this.ids.length;i++){

				document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
			}
			document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
			this.rc.reset();
			this.t = 1;
			this.bf.reset();
			this.rt.hide_strip(&quot;inner&quot;);
			this.rt.show_strip(&quot;outer&quot;);
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
		}


		this.rt.set_restart_button(this.restart.bind(this))



	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;
		for (var i=0; i&lt;this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display=&quot;none&quot;;
		}
		//document.getElementById(ids[this.t%3]).style.display=&quot;block&quot;;


		if(this.t % 4 == 0){
			this.rc.step(this.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById(&quot;race_track_mar_loc_predict&quot;).style.display=&quot;block&quot;;
			this.rt.hide_strip(&quot;inner&quot;);
		}else if(this.t % 4 == 1){
			this.bf.predict(this.last_input);
			
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
			document.getElementById(&quot;race_track_mar_loc_likelihood&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 4 == 2){
			this.rt.show_strip(&quot;inner&quot;);
			this.rt.update_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.rc.state));
			document.getElementById(&quot;race_track_mar_loc_update&quot;).style.display=&quot;block&quot;;
		}else if(this.t % 4 == 3){
			var output = scene.rc.output_dist_sample(0);
	    	var y = scene.bf.cont_2_disc_output(output);
			this.bf.update(y);
			this.rt.update_strip(&quot;outer&quot;, normalize_vector(this.bf.posterior));
			document.getElementById(&quot;race_track_mar_loc_timestep&quot;).style.display=&quot;block&quot;;
		}



	}.bind(n_scene);

	scenes_name[&quot;race_track_mar_loc&quot;] = n_scene;
	scenes.push(n_scene);

&lt;/script&gt;

&lt;div style=&quot;float:right&quot; class=&quot;slidecontainer&quot;&gt;
  &lt;input type=&quot;range&quot; min=&quot;100&quot; max=&quot;700&quot; value=&quot;400&quot; class=&quot;slider&quot; id=&quot;myRange&quot; /&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_timestep&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_predict&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Predict step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_likelihood&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt1  bt&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;Observe&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;div id=&quot;race_track_mar_loc_update&quot; class=&quot;button_set&quot; onclick=&quot;scenes_name['race_track_mar_loc'].step();&quot;&gt;
&lt;div class=&quot;bt1  bt&quot;&gt;Update step&lt;/div&gt;
  &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;On the outside the race track, you will notice a blue colored strip. This strip represents our current posterior of the current position of the race car. At the beginning, we have no knowledge about the position of the race car and assign uniform probability over all positions. By pressing the &lt;strong&gt;OBSERVE&lt;/strong&gt; button two things will happen: first, we will take a measurement of the distance of the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the &lt;strong&gt;UPDATE STEP&lt;/strong&gt; button, we will perform our update step and show the resulting posterior at the outer strip. You will note, that both strips will have the same form after the update. The reason is simple: we just multiplied our likelihood with a constant vector and normalized afterward. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the &lt;strong&gt;PREDICT STEP&lt;/strong&gt; button. You will see that the outer strip will change accordingly. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.&lt;/p&gt;

&lt;p&gt;With the slider below the race track, you can choose a grid size of the discrete probability models. If you want to reset the environment, just press the reset button in the bottom left corner.
As before you can control the car by using your keyboard: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward) or the buttons below the race track.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;script&gt;

function dirac_plot(div_id, x, show_y, path){
	var dp = d3.select(div_id);

	var margin = {top: 20, right: 30, bottom: 30, left: 30},
	width = dp.node().getBoundingClientRect().width - margin.right- margin.left,
	height = width/4;


	var svg = dp.append(&quot;svg&quot;)
	  .attr(&quot;viewBox&quot;,&quot;0 0 &quot; + (width + margin.left + margin.right) + &quot; &quot; + (height + margin.top + margin.bottom))
	  .append(&quot;g&quot;)
	    .attr(&quot;transform&quot;, &quot;translate(&quot; + margin.left + &quot;,&quot; + margin.top + &quot;)&quot;);

	  var markersize=10;
	  var defs = svg.append('svg:defs');
	  defs.append('svg:marker')
	    .attr('id', 'end-arrow-axis')
	    .attr('viewBox', '0 -5 10 10')
	    .attr('refX', &quot;0&quot;)
	    .attr('refY', &quot;0&quot;)
	    .attr('markerWidth', markersize)
	    .attr('markerHeight', markersize)
	    .attr('orient', 'auto')
	    .attr('markerUnits','userSpaceOnUse')
	    .append('svg:path')
	    .attr('d', 'M0,-5L10,0L0,5');


	  markersize=5;
	  defs.append('svg:marker')
	    .attr('id', 'end-arrow')
	    .attr('viewBox', '0 -5 10 10')
	    .attr('refX', &quot;0&quot;)
	    .attr('refY', &quot;0&quot;)
	    .attr('markerWidth', markersize)
	    .attr('markerHeight', markersize)
	    .attr('orient', 'auto')
	    .attr('markerUnits','userSpaceOnUse')
	    .append('svg:path')
	    .attr('d', 'M0,-5L10,0L0,5');






	// x-axis
	svg.append(&quot;g&quot;)
	  .append(&quot;line&quot;)
	  .style(&quot;stroke&quot;,&quot;#000000&quot;)
	  .style('marker-end','url(#end-arrow-axis)')
	  .attr(&quot;x1&quot;, function(d) {
	    return 0-0.03*width;
	  })
	  .attr(&quot;y1&quot;, function(d) {
	    return height;
	  })
	  .attr(&quot;x2&quot;, function(d) {
	    return width;
	  })
	  .attr(&quot;y2&quot;, function(d) {
	    return height;
	  });

if(show_y){
	// y-axis
	svg.append(&quot;g&quot;)
	  .append(&quot;line&quot;)
	  .style(&quot;stroke&quot;,&quot;#000000&quot;)
	  .style('marker-end','url(#end-arrow-axis)')
	  .attr(&quot;x1&quot;, function(d) {
	    return width/2;
	  })
	  .attr(&quot;y1&quot;, function(d) {
	    return height;
	  })
	  .attr(&quot;x2&quot;, function(d) {
	    return width/2;
	  })
	  .attr(&quot;y2&quot;, function(d) {
	    return 0;
	  });	
	}


	// diracs
	for(var i=0; i&lt;x.length;i++){
		svg.append(&quot;g&quot;)
		  .append(&quot;line&quot;)
		  .style(&quot;stroke&quot;,&quot;#000000&quot;)
		  .style('marker-end','url(#end-arrow)')
		  .attr(&quot;x1&quot;, function(d) {
		    return width/2 + x[i].x*width;
		  })
		  .attr(&quot;y1&quot;, function(d) {
		    return height;
		  })
		  .attr(&quot;x2&quot;, function(d) {
		    return width/2 + x[i].x*width;
		  })
		  .attr(&quot;y2&quot;, function(d) {
		    return (1-x[i].w)*height;
		  });

		svg.append(&quot;g&quot;)
		  	.append(&quot;text&quot;)
		  	.text(x[i].t)
		  	.attr(&quot;transform&quot;,&quot;translate(&quot; + (width/2 + x[i].x*width) + &quot;, &quot; + (height + 17) + &quot;)&quot;)
		  	.attr(&quot;text-anchor&quot;,&quot;middle&quot;)

	}



	//null

	svg.append(&quot;g&quot;)
		.append(&quot;text&quot;)
		.text(&quot;0&quot;)
		.attr(&quot;transform&quot;,&quot;translate(&quot; + (width/2) + &quot;, &quot; + (height + 17) + &quot;)&quot;)
		.attr(&quot;text-anchor&quot;,&quot;middle&quot;)

	// path
	if (path){

		var line = d3.line()
		         .x(function(d) { return width/2 + d.x*width; })
                 .y(function(d) { return (1-d.w)*height; });
		svg.append(&quot;svg:path&quot;)
		    .attr(&quot;d&quot;, line(path))
		    .style(&quot;stroke&quot;,&quot;#000000&quot;)
		    .style(&quot;stroke-width&quot;,&quot;4&quot;)
		    .style(&quot;fill&quot;,&quot;none&quot;)
		    .style(&quot;opacity&quot;,0.2)
	}



}



dirac_plot(&quot;#dirac&quot;,[{x:0,w:0.7,t:&quot;&quot;}], false, null);
dirac_plot(&quot;#dirac_shift&quot;,[{x:0.25,w:0.7,t:&quot;y&quot;}], true, null);

var nd = 17;
var grid = [...Array(nd)].map((e,i)=&gt;{return (i - (nd-1)/2)/(1.5*nd)});
comb = [...Array(nd)].map((e,i)=&gt;{return {x:grid[i], w:0.7, t:&quot;&quot;}})
dirac_plot(&quot;#dirac_comb&quot;,comb, true, null);

wcomb = [...Array(nd)].map((e,i)=&gt;{return {x:grid[i], w:0.1*gaussian(grid[i],0,0.15), t:&quot;&quot;}})

var np = 101;
gauss = [...Array(np)].map((e,i)=&gt;{return {x:(i - (np-1)/2)/(1.5*np), w:0.1*gaussian((i - (np-1)/2)/(1.5*np),0,0.15), t:&quot;&quot;};})

dirac_plot(&quot;#dirac_wcomb&quot;,wcomb, true, gauss);
&lt;/script&gt;

&lt;p&gt;&lt;a href=&quot;https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm&quot;&gt;&lt;/a&gt;&lt;/p&gt;

&lt;div id=&quot;rad_to_s&quot; style=&quot;width:100px&quot;&gt;&lt;/div&gt;
&lt;div id=&quot;div1&quot;&gt;&lt;/div&gt;
&lt;div id=&quot;div2&quot;&gt;&lt;/div&gt;
&lt;!-- &lt;div id=&quot;system_dist_approx&quot;  style=&quot;width: 600px; height: 600px;&quot;&gt;&lt;/div&gt; --&gt;
&lt;!--&lt;div id=&quot;output_dist_approx&quot;  style=&quot;width: 600px; height: 600px;&quot;&gt;&lt;/div&gt;--&gt;

&lt;style&gt;
.slidecontainer {
    width: 100%; /* Width of the outside container */
}

/* The slider itself */
.slider {
    -webkit-appearance: none;  /* Override default CSS styles */
    appearance: none;
    width: 100%; /* Full-width */
    height: 25px; /* Specified height */
    background: #f0f0f0; /* Grey background */
    outline: none; /* Remove outline */
    opacity: 0.7; /* Set transparency (for mouse-over effects on hover) */
    -webkit-transition: .2s; /* 0.2 seconds transition on hover */
    transition: opacity .2s;
    margin-top:20px;
    margin-bottom:20px;
}

/* Mouse-over effects */
.slider:hover {
    opacity: 1; /* Fully shown on mouse-over */
}

/* The slider handle (use -webkit- (Chrome, Opera, Safari, Edge) and -moz- (Firefox) to override default look) */ 
.slider::-webkit-slider-thumb {
    -webkit-appearance: none; /* Override default look */
    appearance: none;
    width: 25px; /* Set a specific slider handle width */
    height: 25px; /* Slider handle height */
    background: #555555; /* Green background */
    cursor: pointer; /* Cursor on hover */
}

.slider::-moz-range-thumb {
    width: 25px; /* Set a specific slider handle width */
    height: 25px; /* Slider handle height */
    background: #555555; /* Green background */
    cursor: pointer; /* Cursor on hover */
}
&lt;/style&gt;

&lt;script&gt;



var slider = document.getElementById(&quot;myRange&quot;);

slider.oninput = function() {

	scene = scenes_name['race_track_mar_loc'];

    // delete strips
    scene.rt.delete_strip(&quot;inner&quot;);
    scene.rt.delete_strip(&quot;outer&quot;);


    // set strip domain
    scene.rt.set_strip_domain(parseInt(this.value));


    // create new bayes filter
    scene.bf = init_bayes_filter(scene.rc, scene.rt);

	// create strips
    scene.rt.init_strip(&quot;inner&quot;,get_output_dist_normalized(scene.rc, scene.rt, scene.rc.state) , scene.rt.strip_color.inner, scene.rt.strip_width.inner)
    scene.rt.init_strip(&quot;outer&quot;, normalize_vector(scene.bf.posterior), scene.rt.strip_color.outer, scene.rt.strip_width.outer)
    scene.restart()
}
&lt;/script&gt;</content><author><name></name></author><summary type="html">The process of Bayes filtering requires to solve integrals, that are in general intractable. One approach to circumvent this problem is the use of grid-based filtering. In this article, we will derive this method directly from the recursive equations of the Bayes filter.</summary></entry><entry><title type="html">Nonlinear filtering: Introduction</title><link href="http://localhost:4000/jekyll/update/2018/10/29/nf-intro.html" rel="alternate" type="text/html" title="Nonlinear filtering: Introduction" /><published>2018-10-29T18:04:07+09:00</published><updated>2018-10-29T18:04:07+09:00</updated><id>http://localhost:4000/jekyll/update/2018/10/29/nf-intro</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/10/29/nf-intro.html">&lt;p&gt;This post will start a series of articles that will treat common nonlinear filtering methods that are based on the Bayes filter. The motivation is to provide an intuitive understanding of these methods by deriving them directly from the general Bayes filter. This derivation is done in steps, that are supposed to be as atomic as possible. Furthermore, each nonlinear filtering method will be shown in action by providing an interactive example to play around with. This series will require some basic knowledge in math. Especially in linear algebra and probability theory.&lt;/p&gt;

&lt;!--more--&gt;
&lt;script src=&quot;https://d3js.org/d3.v5.min.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.plot.ly/plotly-latest.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/particle_filter.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/race_car.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/race_track.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/util.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/plot.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/scene.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/nonlinear_filter/discrete_bayes_filter.js&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;link rel=&quot;stylesheet&quot; type=&quot;text/css&quot; href=&quot;http://localhost:4000/assets/css/nonlinear_filter/style.css&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;





function draw_ssm(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 120;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 4;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;T-1) ind+=(t-T+1)


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }


    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}

    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);

  	var svg_w = margin_x + dist_x*(T-1) + 50;
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }



	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





&lt;/script&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Recently, I wrote an article about the &lt;a href=&quot;/jekyll/update/2018/10/10/kalman_filter.html&quot;&gt;derivation of the Kalman filter&lt;/a&gt;. When we are using the Kalman filter we assume that our models are linear Gaussian state space models. In this scenario, we are lucky because we can compute everything analytically in closed form. But at some point in time, we have to face the cruel truth that the real world is normally anything but linear. With nonlinear models, it is not possible to compute the equation of the Bayes filter in closed form, anymore. Nonetheless, we are still lucky, because there are many methods that provide approximations of the Bayes filter. Unfortunately, learning about those methods can be quite frustrating, many resources simply state the corresponding equations and are not providing any further explanations and intuitions. This series of blog posts is an attempt to fix this and provide derivations of these methods directly from the equations of the Bayes filter. Furthermore, presenting merely concatenations of equations can be very frustrating and tiring as well. Thus, the derivations are enriched with figures and examples to obtain a better understanding of what we are actually doing.
The current plan is to write articles about the following methods:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/jekyll/update/2018/10/29/nf-grid-based.html&quot;&gt;Grid-based filter&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Extended Kalman filter&lt;/li&gt;
  &lt;li&gt;Unscented Kalman filter&lt;/li&gt;
  &lt;li&gt;Particle filter&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this first post, I will take the chance to introduce the concept of the Bayes filter and to present the running example for the rest of this series.
Let’s get started!&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter&quot;&gt;Bayes filter&lt;/h2&gt;

&lt;p&gt;Let’s try to state the main idea of the Bayes filter in a compact manner.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;Bayes filter&lt;/strong&gt; is used to &lt;strong&gt;infer&lt;/strong&gt; the current state of a probabilistic state space model given all observations and inputs up to the current timestep and a prior distribution of the initial state.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We define our probabilistic state space model by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
x_{t+1} &amp;\sim p(x_{t+1}|x_{t}, u_{t}) \\

y_t &amp;\sim p(y_t|x_t) 
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;and an initial state distribution \(p(x_0)\). In the figure below, our model is visualized as a probabilistic graphical model.&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Knowledge about probabilistic graphical models is not required to be able to follow this series, but it can be quite helpful for understanding the dependencies between the state, input and output.&lt;/p&gt;

&lt;p&gt;Now that we have briefly introduced the state space model, we can formulate the objective of Bayes filters in a more rigorous way: We want to calculate \(p(x_t|y_{0:t},u_{0:t-1})\) given the prior initial distribution \(p(x_0)\), where the shorthand \(y_{n:m}\) stands for the set \(y_n,…,y_m\). With help of the basic rules of probability, we can find a way to calculate this expression in a recursive fashion. This recursion is composed of two distinct steps: the &lt;strong&gt;update&lt;/strong&gt; and &lt;strong&gt;predict&lt;/strong&gt; step.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Recursive formula of the Bayes filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .&lt;/script&gt;

  &lt;p&gt;The recursion is initialized with a prior distribution over the initial state \(p(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;We will use the definition above as our starting point for the derivations of the particular nonlinear filtering methods. But first, it’s time to introduce the example, that will stay with us for the rest of this series.&lt;/p&gt;

&lt;h2 id=&quot;interactive-example-race-track&quot;&gt;Interactive example: Race track&lt;/h2&gt;

&lt;p&gt;The example that will accompany us for the rest of this series, consists of a race car that can drive around a race track. The race car can be controlled by the discrete actions of &lt;em&gt;forward&lt;/em&gt;, &lt;em&gt;backward&lt;/em&gt; and &lt;em&gt;stop&lt;/em&gt;. After the action is taken some process noise is added. Therefore, we won’t know exactly where the race car will be after we have taken this action. Inside the race course is a tree, which serves as a natural landmark. The race car has a distance meter on board, which obtains noisy measurements of the distance to this tree. The following graphic shows the race car in action. Please use your keyboard to set the input of the race car: &lt;strong&gt;A&lt;/strong&gt; (Backward), &lt;strong&gt;S&lt;/strong&gt; (Stop),  &lt;strong&gt;D&lt;/strong&gt; (Forward). Alternatively, you can use the buttons below.&lt;/p&gt;

&lt;svg id=&quot;race_track_intro&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;
	// defines scenes
	n_scene = load_race_track(&quot;race_track_intro&quot;,&quot;http://localhost:4000&quot;);
	n_scene.mode = 0;
	n_scene.filter = null;
	n_scene.dur=fast_dur;
	n_scene.auto_start = true;

	n_scene.loaded = function(){
		document.getElementById(&quot;race_track_intro_input&quot;).style.display=&quot;block&quot;;
	}.bind(n_scene)

	n_scene.step= function(){
		this.rc.step(this.rc.current_input);
	}.bind(n_scene);

	n_scene.key_down = function(key){
		input = key_to_input(key);
		if(input&gt;=0 &amp;&amp; input &lt;=2){
			this.rc.current_input = input;
		}
	}.bind(n_scene)


	n_scene.on_click = function(key){
		ani(this);
	}.bind(n_scene)
	scenes_name['race_track_intro'] = n_scene;
	scenes.push(n_scene);
&lt;/script&gt;

&lt;div id=&quot;race_track_intro_input&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=0&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=1&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_intro'].rc.current_input=2&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;p&gt;Now that we have a good idea how the model will behave, you have the choice of either going directly to the first post about the grid-based filter or to learn more about the model of the dynamic \(p(x_{t+1}|x_{t}, u_{t})\) and observation \(p(y_t|x_t)\) of the example in the following section.&lt;/p&gt;

&lt;h2 id=&quot;simulation-model&quot;&gt;Simulation model&lt;/h2&gt;

&lt;p&gt;If you want to apply Bayes filters to real-life scenarios, a mathematical model of your system is needed. In general, you will have to learn it from data or derive it directly from the laws of physics. In our case we are lucky, because we are the designer of the real model and simply will use this exact model. Let’s start by taking a closer look at the system dynamic.&lt;/p&gt;

&lt;h1 id=&quot;system-dynamic&quot;&gt;System dynamic&lt;/h1&gt;

&lt;p&gt;The state \(x_t\) of the system is the current position on the race track. It is defined as the path length from the start to the current position following the race track. Having only the position without the velocity as a state representation is not very realistic, because it is not possible to model momentum. But for the sake of simplicity and visualisation, we will go without it. The input \(u_t\) is either \(-1\), \(0\) or \(1\) for backward, stop and forward.&lt;/p&gt;

&lt;p&gt;Our system dynamics are defined by a Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|x_t,u_t) = \mathcal{N}(x_{t+1}|\mu_s(x_t, u_t) ,\sigma_s^2(x_t) )&lt;/script&gt;

&lt;p&gt;with nonlinear mean  \(\mu_s(x_t, u_t)\) and variance \(\sigma_s^2(x_t)\). To obtain the mean of the next state \(x_{t+1}\) the input \(u_t\), which is weighted by \(v\) and \(b(\kappa)\), is simply added to the current state \(x_t\):&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_s(x_t, u_t) = x_t + b(\kappa)vu_t.&lt;/script&gt;

&lt;p&gt;The factor \(v\) is the velocity of the car, we will call it the step size. Intuitively, by multiplying \(u_t\) with the step size \(v\) you map the input from action space to the race track space. The weighting factor \(b(\kappa) = e^{- c\kappa}\), with the hand-tweaked parameter \(c\), is used to model a more realistic driving behavior that depends on the curvature&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\kappa(x_t) ={\frac {|L_x'(x_t)L_y''(x_t)-L_y'(x_t)L_x''(x_t)|}{\left(L_x'^2(x_t)+L_y'^2(x_t)\right)^{\frac {3}{2}}}}&lt;/script&gt;

&lt;p&gt;of the track at the current position \(x_t\). The function \(L(x_t)\) maps the current position of the car \(x_t\) to \((x,y)\) coordinates&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;L(x_t) =  \begin{pmatrix}
    L_x(x_t) \\
    L_y(x_t) \\
    \end{pmatrix}.&lt;/script&gt;

&lt;p&gt;Intuitively, if we are in a sharp curve the curvature is low and we drive faster. If we are on a more straight part of the track, the curvature is high and we drive faster.&lt;/p&gt;

&lt;p&gt;The variance of the next state \(x_{t+1}\) is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_s^2(x_t, u_t) = \left[db(\kappa)v\right]^2.&lt;/script&gt;

&lt;p&gt;The variance depends mainly on the curvature \(\kappa\) at the current position. If we have a low curvature, the mean of the next step will be larger. But if we take a larger step it is natural to assume, that we have a higher variance. The hand-tweaked parameter \(d\) and the step size \(v\) are fixed for the whole environment. Please be aware, that the variance is not depending on the input \(u_t\) itself, but only on the step size \(v\).&lt;/p&gt;

&lt;p&gt;If you move your mouse or your finger over the race track below, you will notice a small blue strip outside the race track. This is the probability density of our dynamic model \(p(x_{t+1}|x_t,u_t)\). Therefore, it shows the distribution over the next state \(x_{t+1}\) given the current state \(x_t\) and action \(u_t\). If you want to check out the distribution for other inputs, you can use again your keyboard or the buttons below the race track.&lt;/p&gt;

&lt;svg id=&quot;race_track_sys_dist&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;


	// defines scenes
	n_scene = load_race_track(&quot;race_track_sys_dist&quot;, &quot;http://localhost:4000&quot;);
	n_scene.mode = 3;
	n_scene.me_show_system = true;

	n_scene.me_show_observation_transposed = false;
	n_scene.me_show_observation = false;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	n_scene.rc.current_input = 0;

	n_scene.loaded = function(){
		document.getElementById(&quot;race_track_sys_dist_input&quot;).style.display=&quot;block&quot;;
		this.nearest = []
		this.nearest.pos = this.rc.state;
	}.bind(n_scene)


	n_scene.mouse_touch = function(coords){
		var min_dist = 100.0;
		this.nearest = this.rt.get_nearest_pos(coords);
		if(this.nearest.distance &lt; min_dist){
			this.rt.update_car(this.nearest.pos,this.dur, 0);
			this.rt.update_strip(&quot;outer&quot;, get_system_dist_normalized(this.rc, this.rt, this.nearest.pos, this.rc.current_input));
			this.rt.show_strip(&quot;outer&quot;);	
		}else{
			this.rt.hide_strip(&quot;outer&quot;);
		}

	}.bind(n_scene)

	n_scene.update_strip_sys = function(){
		if(this.rt.is_strip_visible(&quot;outer&quot;)){
			this.rt.update_strip(&quot;outer&quot;, get_system_dist_normalized(this.rc, this.rt, this.nearest.pos, this.rc.current_input));
		}
	}.bind(n_scene)


	n_scene.key_down = function(key){
		input = key_to_input(key);
		if(input&gt;=0 &amp;&amp; input &lt;=2){
			this.rc.current_input = input;
			this.update_strip_sys();
		}
		
		
	}.bind(n_scene)
	
	scenes_name['race_track_sys_dist'] = n_scene;
	scenes.push(n_scene);
&lt;/script&gt;

&lt;div id=&quot;race_track_sys_dist_input&quot; class=&quot;button_set&quot;&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=0;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;Backward&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=1;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;No action&lt;/div&gt;
&lt;div class=&quot;bt3 bt&quot; onclick=&quot;scenes_name['race_track_sys_dist'].rc.current_input=2;scenes_name['race_track_sys_dist'].update_strip_sys()&quot;&gt;Forward&lt;/div&gt;
 &lt;span class=&quot;stretch&quot;&gt;&lt;/span&gt;
&lt;/div&gt;

&lt;h1 id=&quot;observation-model&quot;&gt;Observation model&lt;/h1&gt;

&lt;p&gt;The race car has a distance meter on board, which will provide us with noisy measurements of the distance to the tree inside the race track, where the position of the tree in \((x,y)\) coordinates is defined as \(T = (T_x, T_y)\). As in the model of the system dynamics, we will model the uncertainty of the measurement with a Gaussian distribution&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|x_t) = \mathcal{N}(y_t| \mu_o(x_t), \sigma_o^2(x_t))&lt;/script&gt;

&lt;p&gt;with nonlinear mean  \(\mu_o(x_t)\) and variance \(\sigma_o^2(x_t)\).&lt;/p&gt;

&lt;p&gt;The mean of our observation is the exact distance to the tree&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mu_o(x_t) = d(L(x_t),T),&lt;/script&gt;

&lt;p&gt;where \(d(\cdot,\cdot)\) is defined as the Euclidean distance.&lt;/p&gt;

&lt;p&gt;The variance of our measuring device&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sigma_o^2(x_t) = \left[ad(L(x_t),T)\right]^2&lt;/script&gt;

&lt;p&gt;depends on the distance as well. The farther we are away from the tree, the more noise will be present in the signal. The parameter \(a\) is again hand-tweaked and constant.&lt;/p&gt;

&lt;p&gt;By moving with your mouse or finger over the race track below you will notice two things. Again, there appears a strip, this time inside the race track and in a brownish color. Furthermore, you will notice another light green strip at the trunk of the tree. Both strips are showing parts of our observation model \(p(y_t|x_t)\). The light green strip shows the probability of observing a distance measurement at our current position. Not surprisingly, we find the maximum probability density at the true distance of tree. Like stated above, the variance is varying depending on the distance to the tree. 
The brownish strip answers another question: Given the true distance \(d\) at our current position, what is the probability of obtaining this measurement \(d\) from another position on the race track. It represents  \(p(y_t=d|x_t)\) and is, therefore, a function of the race track \(x\). Please be aware, that this is not a proper probability density, because it is not integrating to 1.&lt;/p&gt;

&lt;svg id=&quot;race_track_obs_dist&quot; style=&quot;width:100%&quot; onclick=&quot;on_click()&quot;&gt;&lt;/svg&gt;

&lt;script&gt;

	// defines scenes
	n_scene = load_race_track(&quot;race_track_obs_dist&quot;, &quot;http://localhost:4000&quot;);
	n_scene.mode = 3;
	n_scene.me_show_system = false;
	n_scene.me_show_observation_transposed = true;
	n_scene.me_show_observation = true;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	// define particle filter 


	n_scene.mouse_touch = function(coords){
		var min_dist = 100.0;
		this.nearest = this.rt.get_nearest_pos(coords);
		if(this.nearest.distance &lt; min_dist){
			this.rt.update_car(this.nearest.pos,this.dur, 0);
			this.rt.update_strip(&quot;inner&quot;, get_output_dist_normalized(this.rc, this.rt, this.nearest.pos));
			this.rt.show_strip(&quot;inner&quot;);

			this.rt.update_dist_strip(normalize_vector(this.rc.output_dist_array(this.rt.dist_strip_domain, this.nearest.pos, 0)), this.nearest.pos, 0);
			this.rt.show_dist_strip();

		}else{
			this.rt.hide_strip(&quot;inner&quot;);
			this.rt.hide_dist_strip();
		}

	}.bind(n_scene)

	scenes.push(n_scene);
&lt;/script&gt;

&lt;p&gt;Now that we know the inner workings of our model, we are well prepared to start the series with the &lt;a href=&quot;/jekyll/update/2018/10/29/nf-grid-based.html&quot;&gt;grid-based filter&lt;/a&gt;.&lt;/p&gt;

&lt;h1 id=&quot;acknowledgement&quot;&gt;Acknowledgement&lt;/h1&gt;

&lt;p&gt;The vector graphics of the &lt;a href=&quot;https://www.freepik.com/free-photos-vectors/car&quot;&gt;car&lt;/a&gt; were created by &lt;a href=&quot;https://www.freepik.com/&quot;&gt;Freepik&lt;/a&gt;.&lt;/p&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/d3_graphical_model.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://localhost:4000/assets/js/svg_mathjax.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;new Svg_MathJax().install();&lt;/script&gt;</content><author><name></name></author><summary type="html">This post will start a series of articles that will treat common nonlinear filtering methods that are based on the Bayes filter. The motivation is to provide an intuitive understanding of these methods by deriving them directly from the general Bayes filter. This derivation is done in steps, that are supposed to be as atomic as possible. Furthermore, each nonlinear filtering method will be shown in action by providing an interactive example to play around with. This series will require some basic knowledge in math. Especially in linear algebra and probability theory.</summary></entry><entry><title type="html">The score-Fisher-information-KL connection</title><link href="http://localhost:4000/jekyll/update/2018/10/17/fisher.html" rel="alternate" type="text/html" title="The score-Fisher-information-KL connection" /><published>2018-10-17T18:04:07+09:00</published><updated>2018-10-17T18:04:07+09:00</updated><id>http://localhost:4000/jekyll/update/2018/10/17/fisher</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/10/17/fisher.html">&lt;p&gt;This article is a brief summary of some relationships between the log-likelihood, score, Kullback-Leibler divergence and Fisher information. No explanations, just pure math.
&lt;!--more--&gt;&lt;/p&gt;

&lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_SVG&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;link href=&quot;https://fonts.googleapis.com/css?family=Roboto&quot; rel=&quot;stylesheet&quot; /&gt;

&lt;script type=&quot;text/javascript&quot;&gt;

function getPosition(el) {
  var xPos = 0;
  var yPos = 0;
 
  while (el) {
    if (el.tagName == &quot;BODY&quot;) {
      // deal with browser quirks with body/window/document and page scroll
      var xScroll = el.scrollLeft || document.documentElement.scrollLeft;
      var yScroll = el.scrollTop || document.documentElement.scrollTop;
 
      xPos += (el.offsetLeft - xScroll + el.clientLeft);
      yPos += (el.offsetTop - yScroll + el.clientTop);
    } else {
      // for all other non-BODY elements
      xPos += (el.offsetLeft - el.scrollLeft + el.clientLeft);
      yPos += (el.offsetTop - el.scrollTop + el.clientTop);
    }
 
    el = el.offsetParent;
  }
  return {
    x: xPos,
    y: yPos
  };
}

function switch_tab(button){

    var p1 = getPosition(button)

	// get parent elements
	var bar = button.parentElement;
	var con = bar.parentElement;
	var tab_id = con.getAttribute(&quot;class&quot;).split(&quot; &quot;)[1];


	var c = bar.childNodes;
	var ci = 0;

	for (var i=0;i&lt;c.length;i++){

		if(c[i].tagName==&quot;H3&quot;){	
			if (c[i]===button){
				break;
			}
			ci++;
		}
	}
	// get tab id
	cons = document.querySelectorAll('.tab-container.' + tab_id);;

	for (var j=0;j&lt;cons.length;j++){

		// change bar
		var bar_j = cons[j].querySelector(&quot;.tab-bar&quot;);
		var buttons = bar_j.childNodes;
		var bi = 0;

		for (var i=0;i&lt;buttons.length;i++){
			if(buttons[i].tagName==&quot;H3&quot;){	
				if (bi == ci){
					buttons[i].className = &quot;tab-button-selected&quot;;
				}else{
					buttons[i].className = &quot;tab-button&quot;;
				}
				bi++;
			}
		}

		var bi = 0;

		for (var i=0;i&lt;cons[j].childNodes.length;i++){
			if(cons[j].childNodes[i].tagName==&quot;DIV&quot; &amp;&amp; cons[j].childNodes[i].className!=&quot;tab-bar&quot;){	
				if (bi == ci){
					cons[j].childNodes[i].className = &quot;tab-visible&quot;;
				}else{
					cons[j].childNodes[i].className = &quot;tab-invisible&quot;;
				}
				bi++;
			}
		}

	}

	span_cons = document.querySelectorAll('.span-container.' + tab_id );
	
	for (var j=0;j&lt;span_cons.length;j++){
		var bi = 0;
		for (var i=0;i&lt;span_cons[j].childNodes.length;i++){
			if(span_cons[j].childNodes[i].tagName==&quot;SPAN&quot;){	
				if (bi == ci){
					span_cons[j].childNodes[i].className = &quot;span-visible&quot;;
				}else{
					span_cons[j].childNodes[i].className = &quot;span-invisible&quot;;
				}
				bi++;
			}
		}
	}

	var p2 = getPosition(button)
	window.scrollBy(0,-p1.y+p2.y+15);





}

&lt;/script&gt;

&lt;style type=&quot;text/css&quot;&gt;


div.tab-container {

	width:100%;

	margin-bottom:15px;

}


div.tab-bar {
	display:inline-block;
	height:30px;

	border-bottom:solid 1px #ebebeb;
	border-left:solid 1px #ebebeb;
	border-right:solid 1px #ebebeb;
	padding:0;
	padding-bottom:2px;
	vertical-align: middle;
}


h3.tab-button-selected {
	height:100%;
	display:inline-block;
	max-width: 200px;
	padding-left:15px;
	padding-right:15px;
	border-top: 2px solid  #c90606;
	color:#c90606;
	border-bottom: 2px transparent;
	text-align:center;
	padding: 0 25px;
    line-height:30px;
    font-size:90%;
    font-weight: 500;
    text-transform: uppercase;
}

h3.tab-button {

	height:20px;
	display:inline-block;
	max-width: 200px;
	padding-left:15px;
	padding-right:15px;
	border-bottom: 2px transparent;
	border-top: 2px transparent;
	color:#757575;
	text-align:center;
	padding: 0 25px;
	line-height:0px;
	font-size:90%;
	font-weight: 500;
	font-family: 'Roboto', sans-serif;
	text-transform: uppercase;
}


div.tab-visible {

	width:100%;
	display:block;
	overflow: auto;
	background-color: #f7f7f7;
	border:solid 1px #ebebeb;

}

div.tab-invisible {

	width:100%;
	display:none;
	overflow: auto;
}

span.span-visible {
	display:inline-block;
}

span.span-invisible {
	display:none;
}

&lt;/style&gt;

&lt;h2 id=&quot;log-likelihood&quot;&gt;Log-likelihood&lt;/h2&gt;
&lt;p&gt;The log-likelihood is defined the logarithm of the likelihood&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\ell(x;\theta')  =  \log p(x|\theta').&lt;/script&gt;

&lt;p&gt;Let’s perform a &lt;strong&gt;Taylor approximation&lt;/strong&gt; of the log-likelihood \(\log p(x|\theta’)\) around the current estimate \(\theta\):&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
	$$\begin{align*}
 \log p(x|\theta') =&amp;amp; \log p(x|\theta')|_{\theta' =  \theta} +  \sum_i \left. \frac{\partial \log p(x|\theta')}{\partial \theta'_i} \right| _{\theta' = \theta}   (\theta'_i - \theta_i) \\ &amp;amp;+ \frac{1}{2}\sum_i\sum_j \left. \frac{\partial^2 \log p(x|\theta')}{\partial \theta'_i\partial \theta'_j}\right| _{\theta' = \theta}   (\theta'_i - \theta_i)(\theta'_j - \theta_j) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
	$$\begin{align*}
\log p(x|\theta') =&amp;amp; \log p(x|\theta')|_{\theta' = \theta} + \left. \nabla_{\theta'} \log p(x|\theta')^T\right| _{\theta' = \theta} (\theta' - \theta) \\ &amp;amp;+ \frac{1}{2}\left. (\theta' - \theta)^T\nabla^2_{\theta'} \log p(x|\theta')\right| _{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;linear&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;\(\frac{\partial}{\partial \theta’_i} \log p(x|\theta’)\)&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;\(\nabla_{\theta’} \log p(x|\theta’)\)&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$  \frac{\partial}{\partial \theta'_i} \log p(x|\theta') = \frac{1}{p(x|\theta')} \frac{\partial}{\partial \theta'_i} p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \nabla_{\theta'} \log p(x|\theta') = \frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta')  $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;by using the &lt;em&gt;log derivative trick&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left. \frac{\partial}{\partial \theta'_i} \log p(x|\theta') \right|_{\theta' = \theta} = \frac{1}{p(x|\theta)} \frac{\partial}{\partial \theta_i} p(x|\theta). $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \left. \nabla_{\theta'} \log p(x|\theta') \right|_{\theta' = \theta} = \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta).  $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;quadratic&lt;/strong&gt; term of the decomposition  &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial^2 \log p(x|\theta’)}{\partial \theta’_i\partial \theta’_j}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla^2_{\theta’} \log p(x|\theta’)\)
&lt;/span&gt;&lt;/span&gt; 
can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial^2 \log p(x|\theta')}{\partial \theta'_\partial \theta'_j} =&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right) \\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')} \frac{\partial}{\partial \theta'_i} p(x|\theta')\right) \\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')}\right) \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial }{\partial \theta'_j} \left(\frac{\partial}{\partial \theta'_i} p(x|\theta')\right)\\
=&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{1}{p(x|\theta')}\right) \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  \\
=&amp;amp; - \frac{1}{p(x|\theta')^2} \frac{\partial}{\partial \theta'_j} p(x|\theta') \frac{\partial}{\partial \theta'_i} p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  \\
=&amp;amp; - \frac{\partial}{\partial \theta'_j} \log p(x|\theta') \frac{\partial}{\partial \theta'_i} \log p(x|\theta') +    \frac{1}{p(x|\theta')} \frac{\partial^2 p(x|\theta')}{\partial \theta'_\partial \theta'_j}  
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla^2_{\theta'} \log p(x|\theta') =&amp;amp; \nabla_{\theta'} \nabla_{\theta'}^T \log p(x|\theta') \\
=&amp;amp; \nabla_{\theta'} \left(\frac{1}{p(x|\theta')}\nabla_{\theta'}^T p(x|\theta')\right) \\
=&amp;amp; \nabla_{\theta'} p(x|\theta')  \nabla_{\theta'}^T \left(\frac{1}{p(x|\theta')}\right) +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^T\nabla_{\theta'} p(x|\theta') \\
=&amp;amp;- \frac{1}{p(x|\theta')^2} \nabla_{\theta'} p(x|\theta') \nabla_{\theta'} p(x|\theta') ^T  +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^2 p(x|\theta') \\
=&amp;amp;- \nabla_{\theta'} \log p(x|\theta') \nabla_{\theta'} \log p(x|\theta') ^T  +  \frac{1}{p(x|\theta')}\nabla_{\theta'}^2 p(x|\theta')
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial^2 \log p(x|\theta')}{\partial \theta'_\partial \theta'_j}\right|_{\theta' = \theta} =&amp;amp; - \frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta) +    \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}  
\end{align*}.
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla^2_{\theta'} \log p(x|\theta')\right|_{\theta' = \theta} =&amp;amp;- \nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta) ^T  +  \frac{1}{p(x|\theta)}\nabla_{\theta}^2 p(x|\theta)
\end{align*}.
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, you can express the &lt;strong&gt;Taylor approximation&lt;/strong&gt; as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
	$$\begin{align*}
 \log p(x|\theta') =&amp;amp; \log p(x|\theta) +  \sum_i \frac{1}{p(x|\theta)} \frac{\partial}{\partial \theta_i} p(x|\theta)   (\theta'_i - \theta_i) \\ &amp;amp;+ \frac{1}{2} \sum_i\sum_j \left(- \nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta) ^T  +  \frac{1}{p(x|\theta)}\nabla_{\theta}^2 p(x|\theta)\right) (\theta'_i - \theta_i)(\theta'_j - \theta_j) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
	$$\begin{align*}
\log p(x|\theta') =&amp;amp; \log p(x|\theta) + \left. \nabla_{\theta'} \log p(x|\theta')^T\right| _{\theta' = \theta} (\theta' - \theta) \\ &amp;amp;+ \left. \frac{1}{2} (\theta' - \theta)^T\nabla^2_{\theta'} \log p(x|\theta')\right| _{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;mean&quot;&gt;Mean&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} 
 \mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')] &amp;=  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \log p(x|\theta')dx \\
&amp;= -H(\theta^*,\theta') \\
 \end{align*} %]]&gt;&lt;/script&gt;

&lt;h1 id=&quot;variance&quot;&gt;Variance&lt;/h1&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align*} \text{Var}(\log p(x|\theta')) &amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')])^2\right] \\
&amp;=  \mathbb{E}_{p(x|\theta^*)}\left[(\log p(x|\theta'))^2\right]-\mathbb{E}_{p(x|\theta^*)}[\log p(x|\theta')]^2 \\
&amp;=  \mathbb{E}_{p(x|\theta^*)}\left[(\log p(x|\theta'))^2\right]+H(\theta^*,\theta')^2 \\

\end{align*} %]]&gt;&lt;/script&gt;

&lt;h2 id=&quot;score&quot;&gt;Score&lt;/h2&gt;

&lt;p&gt;The score is defined as the derivative of the log-likelihood&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ V_i(x;\theta')  =  \frac{\partial}{\partial \theta'_i} \log p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ V(x;\theta')  =  \nabla_{\theta'} \log p(x|\theta') $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;mean-1&quot;&gt;Mean&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right] =  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \frac{\partial}{\partial \theta'_i} \log p(x|\theta')dx$$

&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right] =  \int \limits_{-\infty}^{\infty} p(x|\theta^*) \nabla_{\theta'} \log p(x|\theta')dx$$

&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;variance-1&quot;&gt;Variance&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
\begin{align*} \text{Var}\left(\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right) &amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\left(\frac{\partial}{\partial \theta'_i} \log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]\right)^2\right] \\
&amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right] +  \mathbb{E}_{p(x|\theta^*)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]^2 \\
\end{align*} 

&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
\begin{align*} \text{Var}\left(\nabla_{\theta'} \log p(x|\theta')\right) &amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\left(\nabla_{\theta'}\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\right)\left(\nabla_{\theta'}\log p(x|\theta')-\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\right)^T\right] \\
&amp;amp;=  \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\nabla_{\theta'} \log p(x|\theta')^T\right] +  \mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\mathbb{E}_{p(x|\theta^*)}\left[\nabla_{\theta'} \log p(x|\theta')\right]^T \\
\end{align*} 
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;kullback-leibler-divergence&quot;&gt;Kullback-Leibler divergence&lt;/h2&gt;

&lt;p&gt;The Kullback-Leibler divergence is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;D_\textrm{KL}(\theta||\theta') = \int \limits_{-\infty}^{\infty} p(x|\theta) \log \frac{p(x|\theta)}{p(x|\theta')} dx .&lt;/script&gt;

&lt;p&gt;Let’s perform a &lt;strong&gt;Taylor approximation&lt;/strong&gt; around the current estimate \(\theta\):&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} +  \sum_i \left.\frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i}\right|_{\theta' = \theta}   (\theta'_i - \theta_i)  \\ &amp;amp;+  \frac{1}{2}\sum_i\sum_j \left.\frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j}\right|_{\theta' = \theta}  (\theta'_i - \theta_i) (\theta'_j - \theta_j) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} + \left. \nabla_{\theta'}D_\textrm{KL}(\theta||\theta')^T\right|_{\theta' = \theta}(\theta' - \theta) \\
&amp;amp; +  \frac{1}{2}\left. (\theta' - \theta)^T\nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta')\right|_{\theta' = \theta}(\theta' - \theta) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;constant&lt;/strong&gt; term can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} =&amp;amp; 0.
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta')|_{\theta' = \theta} =&amp;amp;  0.
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;linear&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial D_\textrm{KL}(\theta||\theta’)}{\partial \theta’_i}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla_{\theta’}D_\textrm{KL}(\theta||\theta’)\)
&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i} =&amp;amp; \frac{\partial}{\partial \theta'_i}\left(\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta) dx -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx\right) \\
=&amp;amp; -\int \limits_{-\infty}^{\infty} p(x|\theta) \frac{\partial}{\partial \theta'_i}\log p(x|\theta') dx\\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\frac{1}{p(x|\theta')}\frac{\partial}{\partial \theta'_i} p(x|\theta') dx \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta'_i} \log p(x|\theta')\right]\\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta')}\frac{\partial}{\partial \theta'_i} p(x|\theta')\right].
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla_{\theta'}D_\textrm{KL}(\theta||\theta') =&amp;amp;  \nabla_{\theta'}\left(\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta) dx -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx\right) \\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'} \log p(x|\theta') dx \\
=&amp;amp;  -\int \limits_{-\infty}^{\infty} p(x|\theta)\frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta') dx \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta')}\nabla_{\theta'} p(x|\theta')\right].
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i} \right|_{\theta' = \theta} =&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i}p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \frac{\partial}{\partial \theta_i} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i} 1 \\
=&amp;amp; 0 
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla_{\theta'}D_\textrm{KL}(\theta||\theta') \right|_{\theta' = \theta} =&amp;amp;   - \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \nabla_{\theta} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta} 1 \\
=&amp;amp; 0 
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;strong&gt;quadratic&lt;/strong&gt; term &lt;span class=&quot;span-container scalar-vector&quot;&gt;&lt;span class=&quot;span-visible&quot;&gt;
\(\frac{\partial^2 D_\textrm{KL}(\theta||\theta’)}{\partial \theta’_i\partial \theta’_j}\)
&lt;/span&gt;&lt;span class=&quot;span-invisible&quot;&gt;
\(\nabla^2_ {\theta’}D_\textrm{KL}(\theta’||\theta’)\)
&lt;/span&gt;&lt;/span&gt; in this decomposition can be written as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j} =&amp;amp; \frac{\partial }{\partial \theta'_j} \left( \frac{\partial}{\partial \theta'_i} D_\textrm{KL}(\theta||\theta')\right) \\
=&amp;amp; -\frac{\partial }{\partial \theta'_j} \left( \int \limits_{-\infty}^{\infty} p(x|\theta)\frac{\partial}{\partial \theta'_i} \log p(x|\theta') dx \right) \\
=&amp;amp; - \int \limits_{-\infty}^{\infty} p(x|\theta)\frac{\partial^2  \log p(x|\theta')}{\partial \theta'_i  \partial \theta'_j}   dx \\
=&amp;amp; -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta')}{\partial \theta'_i  \partial \theta'_j}\right]
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta') =&amp;amp; \nabla_{\theta'} \nabla_{\theta'}^T D_\textrm{KL}(\theta||\theta') \\
=&amp;amp; -\nabla_{\theta'} \left( \int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'}^T \log p(x|\theta') dx \right) \\
=&amp;amp; - \int \limits_{-\infty}^{\infty} p(x|\theta)\nabla_{\theta'}^2 \log p(x|\theta') dx \\
=&amp;amp; -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'}^2 \log p(x|\theta')\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Evaluated at \(  \theta’ = \theta \) you receive&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\left. \frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta'_i\partial \theta'_j} \right|_{\theta' = \theta} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right]
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\left. \nabla^2_{\theta'}D_\textrm{KL}(\theta||\theta') \right|_{\theta' = \theta} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, you can express the &lt;strong&gt;Taylor approximation&lt;/strong&gt; as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; -\frac{1}{2}\sum_i\sum_j  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j} \right]  (\theta'_i - \theta_i) (\theta'_j - \theta_j) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
D_\textrm{KL}(\theta||\theta') =&amp;amp; -\frac{1}{2} (\theta' - \theta)^T\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right](\theta' - \theta) + ...
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;cross-entropy&quot;&gt;Cross entropy&lt;/h2&gt;

&lt;p&gt;The cross entropy is defined as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(\theta,\theta') = -\int \limits_{-\infty}^{\infty} p(x|\theta) \log p(x|\theta') dx .&lt;/script&gt;

&lt;p&gt;The cross entropy and the Kullback-Leibler differ only by the constant entropy \(H(\theta\). Therefore, the linear and quadratic terms of those are the same.&lt;/p&gt;

&lt;h2 id=&quot;fisher-information&quot;&gt;Fisher Information&lt;/h2&gt;
&lt;h1 id=&quot;the-fisher-definition-can-be-defined-as-&quot;&gt;The fisher definition can be defined as …&lt;/h1&gt;

&lt;h1 id=&quot;-expectation-of-the-squared-score&quot;&gt;… expectation of the squared score&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-variance-of-the-score&quot;&gt;… variance of the score&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij} &amp;amp;=  \text{Var}\left(\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right) \\
 &amp;amp;=  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] +  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right]^2 \\
\end{align*} 
 $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ 
\begin{align*}
\mathcal{I(\theta)} &amp;amp;=  \text{Var}\left(\nabla_{\theta'} \log p(x|\theta')\right) \\
&amp;amp;=  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\nabla_{\theta'} \log p(x|\theta')^T\right] +  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta'} \log p(x|\theta')\right]^T \\
\end{align*} 
 $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*} \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] =&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\frac{\partial}{\partial \theta_i}p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \frac{\partial}{\partial \theta_i} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \frac{\partial}{\partial \theta_i} 1 \\
=&amp;amp; 0 
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*} \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta)\right] =&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta)\right] \\
=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)}\nabla_{\theta} p(x|\theta) dx \\
=&amp;amp; \int \limits_{-\infty}^{\infty} \nabla_{\theta} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta) dx \\
=&amp;amp;  \nabla_{\theta} 1 \\
=&amp;amp; 0 
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;follows&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-curvature-of-the-kl-divergence&quot;&gt;… curvature of the KL-divergence&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij} =&amp;amp;\left. \frac{\partial^2 D_\textrm{KL}(\theta||\theta')}{\partial \theta_i\partial \theta_j} \right|_{\theta' = \theta}\\
 =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right] \\
=&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[-\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta) +    \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j}\right] \\
=&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] -    \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\mathcal{I(\theta)} =&amp;amp;\left. \nabla^2_{\theta}D_\textrm{KL}(\theta||\theta) \right|_{\theta = \theta}\\
 =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
 =&amp;amp; - \mathbb{E}_{p(x|\theta)}\left[-\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T +    \frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right] \\
=&amp;amp; \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] -    \mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;With&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
\mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_\partial \theta_j}\right]=&amp;amp; \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)} \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j} dx  \\
=&amp;amp; \int \limits_{-\infty}^{\infty}  \frac{\partial^2 p(x|\theta)}{\partial \theta_i\partial \theta_j} dx  \\
=&amp;amp;  \frac{\partial^2 }{\partial \theta_i\partial \theta_j}\int \limits_{-\infty}^{\infty} p(x|\theta)  dx  \\
=&amp;amp;  \frac{\partial^2 }{\partial \theta_i\partial \theta_j}1  \\
=&amp;amp; 0 .
\end{align*}
$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*} 
\mathbb{E}_{p(x|\theta)}\left[\frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta)\right]=&amp;amp;  \int \limits_{-\infty}^{\infty} p(x|\theta) \frac{1}{p(x|\theta)} \nabla^2_{\theta} \log p(x|\theta) dx  \\
=&amp;amp;  \int \limits_{-\infty}^{\infty}  \nabla^2_{\theta} \log p(x|\theta) dx  \\
=&amp;amp;  \nabla^2_{\theta}\int \limits_{-\infty}^{\infty} p(x|\theta)  dx  \\
=&amp;amp;  \nabla^2_{\theta}1   \\
=&amp;amp;0
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;follows&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\frac{\partial}{\partial \theta_j} \log p(x|\theta) \frac{\partial}{\partial \theta_i} \log p(x|\theta)\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta} \log p(x|\theta) \nabla_{\theta} \log p(x|\theta)^T\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h1 id=&quot;-curvature-of-the-cross-entropy&quot;&gt;… curvature of the cross entropy&lt;/h1&gt;

&lt;p&gt;Cross entropy and Kullback-Leibler divergence only differ by constant additive term. Therefore, curvature has to be identical.&lt;/p&gt;

&lt;h1 id=&quot;-expected-negative-curvature-of-log-likelihood&quot;&gt;… expected negative curvature of log-likelihood&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$
\begin{align*}
 \left[\mathcal{I(\theta)}\right]_{ij}  =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j}\right] \\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$
\begin{align*}
\mathcal{I(\theta)} =&amp;amp;  -\mathbb{E}_{p(x|\theta)}\left[\nabla_{\theta}^2 \log p(x|\theta)\right]\\
\end{align*}$$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;See &lt;em&gt;curvature of the KL-divergence&lt;/em&gt;.&lt;/p&gt;

&lt;h1 id=&quot;-expected-value-of-the-observed-information&quot;&gt;… expected value of the observed information&lt;/h1&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{I(\theta)}\right]_{ij} =  \mathbb{E}_{p(x|\theta)}\left[\left[\mathcal{J(\theta)}\right]_{ij}\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{I(\theta)} =  \mathbb{E}_{p(x|\theta)}\left[\mathcal{J(\theta)}\right] $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;where the observed information \(\mathcal{J(\theta)}\) is defined as&lt;/p&gt;

&lt;div class=&quot;tab-container scalar-vector&quot;&gt;&lt;div class=&quot;tab-visible&quot;&gt;
$$ \left[\mathcal{J(\theta)}\right]_{ij} =  -\frac{\partial^2  \log p(x|\theta)}{\partial \theta_i  \partial \theta_j} $$
&lt;/div&gt;&lt;div class=&quot;tab-invisible&quot;&gt;
$$ \mathcal{J(\theta)} =  -\nabla_{\theta}^2 \log p(x|\theta) $$
&lt;/div&gt;&lt;div class=&quot;tab-bar&quot;&gt;&lt;h3 class=&quot;tab-button-selected&quot; onclick=&quot;switch_tab(this)&quot;&gt;Scalar&lt;/h3&gt;&lt;h3 class=&quot;tab-button&quot; onclick=&quot;switch_tab(this)&quot;&gt;Vector&lt;/h3&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We note that this case has the same form as the curvature of the KL-divergence.&lt;/p&gt;</content><author><name></name></author><summary type="html">This article is a brief summary of some relationships between the log-likelihood, score, Kullback-Leibler divergence and Fisher information. No explanations, just pure math.</summary></entry><entry><title type="html">Derivation of the Kalman filter</title><link href="http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html" rel="alternate" type="text/html" title="Derivation of the Kalman filter" /><published>2018-10-10T19:35:07+09:00</published><updated>2018-10-10T19:35:07+09:00</updated><id>http://localhost:4000/jekyll/update/2018/10/10/kalman_filter</id><content type="html" xml:base="http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html">&lt;p&gt;The concept and the equations of the &lt;a href=&quot;https://en.wikipedia.org/wiki/Kalman_filter&quot;&gt;Kalman filter&lt;/a&gt; can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_inference&quot;&gt;Bayesian inference&lt;/a&gt;. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.
&lt;!--more--&gt;&lt;/p&gt;

&lt;script type=&quot;text/javascript&quot;&gt;
    function draw_ssm(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});
        state_ns.push({title: &quot;\\( x_&quot; + t +&quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:&quot;#FFFFFF&quot;});
        output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});


        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

  	nodes = input_ns.concat(state_ns).concat(output_ns);
    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_ind(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 120;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 4;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;T-1) ind+=(t-T+1)


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = (t==T-1) ? &quot;#e3e5feff&quot; :statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }


    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}

    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);

  	var svg_w = margin_x + dist_x*(T-1) + 50;
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_indi(svg){

      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 100;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	ind = &quot;t&quot;

      	if (t&lt;2) ind+=(t-2)
      	if (t&gt;2) ind+=&quot;+&quot;+(t-2)


      	input_ns.push({title: &quot;\\( u_{&quot; + ind + &quot;}\\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});

      	statefill = (t==2) ? &quot;#e3e5feff&quot; :statefill = &quot;#FFFFFF&quot;;
        state_ns.push({title: &quot;\\( x_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: &quot;\\( y_{&quot; + ind + &quot;} \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});



        edges.push({source: state_ns[t], target: output_ns[t], dash:&quot;&quot;})
        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

    estate_h = {x: state_ns[T-1].x + radius*4, y: state_ns[T-1].y}
    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    einput_h = {x: input_ns[T-1].x + 2*Math.sqrt(2)*radius, y: input_ns[T-1].y+ 2*Math.sqrt(2)*radius}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}


    edges.push({source: state_ns[T-1], target: estate_h, dash:&quot;5,5&quot;})
    edges.push({source: bstate_h, target: state_ns[0], dash:&quot;5,5&quot;})
    edges.push({source: input_ns[T-1], target: einput_h, dash:&quot;5,5&quot;})
    edges.push({source: binput_h, target: state_ns[0], dash:&quot;5,5&quot;})


  	nodes = input_ns.concat(state_ns).concat(output_ns);
    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

	function draw_ssm_obs(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t&lt;T;t++){


      	if (t&lt;T-1) input_ns.push({title: &quot;\\( u_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y , fill:&quot;#e3e5feff&quot;});
        state_ns.push({title: &quot;\\( x_&quot; + t +&quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y + dist_y, fill:&quot;#FFFFFF&quot;});

        if (t==0||t==4) output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
        if (t==2) {
        	output_ns.push({title: &quot;\\( z_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*(t+0.5) , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
			output_ns.push({title: &quot;\\( y_&quot; + t + &quot; \\)&quot;, type: &quot;prob&quot;, x: margin_x + dist_x*(t-0.5) , y: margin_y+ 2*dist_y, fill:&quot;#e3e5feff&quot;});
        }
        



        if (t&gt;0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:&quot;&quot;})
        }
      }

      edges.push({source: state_ns[0], target: output_ns[0], dash:&quot;&quot;})
      edges.push({source: state_ns[2], target: output_ns[1], dash:&quot;&quot;})
      edges.push({source: state_ns[2], target: output_ns[2], dash:&quot;&quot;})
      edges.push({source: state_ns[4], target: output_ns[3], dash:&quot;&quot;})

  	nodes = input_ns.concat(state_ns).concat(output_ns);

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);

    }
 &lt;/script&gt;

&lt;script src=&quot;//d3js.org/d3.v3.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;p&gt;First of all, let’s try to formulate the main idea of Kalman filtering in one sentence:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;p&gt;The &lt;strong&gt;Kalman filter&lt;/strong&gt; is used to &lt;strong&gt;infer&lt;/strong&gt; the current state of a &lt;strong&gt;linear Gaussian state space model&lt;/strong&gt; given all observations and inputs up to the current timestep and a Gaussian prior distribution of the initial state.&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Indeed, the process of Kalman filtering is simply Bayesian inference in the domain of linear Gaussian state space models. The information encoded in our formulation is sufficient to uniquely define what the Kalman filter should output. But it doesn’t tell us anything about how to compute it. In this article, we will find an efficient recursive method that will lead us to the familiar equations.&lt;/p&gt;

&lt;p&gt;Let’s get started with the derivation by defining &lt;em&gt;linear Gaussian state space models&lt;/em&gt; and &lt;em&gt;Bayesian inference&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&quot;linear-gaussian-state-space-models&quot;&gt;Linear Gaussian state space models&lt;/h2&gt;

&lt;p&gt;A linear Gaussian state space model is defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;= A_tx_t + B_t u_t + w_t \\ 
y_t &amp;= C_tx_t + v_t \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A_t\), input matrix \(B_t\), output matrix \(C_t\), Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).&lt;/p&gt;

&lt;p&gt;Alternatively, we can use the probability density functions&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}x_{t+1} &amp;\sim \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\\ 
 y_t &amp;\sim \mathcal{N}(y_t|C_tx_t, R_t)\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;to describe the system in a more compact fashion.&lt;/p&gt;

&lt;p&gt;Linear Gaussian state space models can also be described in the language of &lt;a href=&quot;https://en.wikipedia.org/wiki/Graphical_model&quot;&gt;probabilistic graphical models&lt;/a&gt; (or more precisely in the language of &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayesian_network&quot;&gt;Bayesian networks&lt;/a&gt;). The figure below shows such a model up to time \(T = 4\).&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Every node represents a random variable and the edges are representing conditional dependencies between the respective nodes. Random variables, that are observed (or given) are shaded in light blue. In our case this is the output \(y_t\) and the input \(u_t\). The state \(x_t\) is not observed (or latent).&lt;/p&gt;

&lt;h2 id=&quot;bayesian-inference&quot;&gt;Bayesian inference&lt;/h2&gt;

&lt;p&gt;In simplest terms Bayesian inference tries to update a hypothesis/belief of something, that is not directly observable, in the face of new information by using &lt;a href=&quot;https://en.wikipedia.org/wiki/Bayes%27_theorem&quot;&gt;Bayes’ rule&lt;/a&gt;. A bit more formal: The goal is to update the prior distribution \(p(x)\) given new data \(\mathcal{D}\) to obtain the posterior distribution \(p(x|\mathcal{D})\) with help of Bayes rule&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\mathcal{D}) = \frac{p(\mathcal{D}|x)p(x)}{p(\mathcal{D})}&lt;/script&gt;

&lt;p&gt;with likelihood \(p(\mathcal{D}|x)\) and evidence \(p(\mathcal{D})\).&lt;/p&gt;

&lt;p&gt;This idea is very general and can be applied to dynamical models quite easily. The most common inference tasks in dynamical models are filtering, smoothing and prediction. These methods differ only in the form of the posterior distribution.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Filtering&lt;/strong&gt;: What is my belief about the &lt;strong&gt;current state&lt;/strong&gt; \(x_t\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; 	$$ p(x_t|y_0,...,y_t,u_0,...,u_{t-1})  $$ &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Smoothing&lt;/strong&gt;: What is my belief about &lt;strong&gt;all states&lt;/strong&gt; \(x_t, … ,x_0\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; $$ p(x_t,...,x_0|y_0,...,y_t,u_0,...,u_{t-1}) $$ &lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: What is my belief about the &lt;strong&gt;next state&lt;/strong&gt; \(x_{t+1}\) given all observations and inputs?&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;big_eq&quot;&gt; $$ p(x_{t+1}|y_0,...,y_t,u_0,...,u_{t-1}) $$ &lt;/div&gt;

&lt;p&gt;The name Kalman &lt;em&gt;filter&lt;/em&gt; reveals, that we will be interested in the filtering problem. Therefore, we want to infer the current state \(x_t\) based on all recent observations \(y_0,…,y_t\) and inputs \(u_0,…,u_{t-1}\).
Now that we have defined what we are looking for, let’s try to find a way to efficiently calculate it. We will start by finding a recursive method for &lt;em&gt;general&lt;/em&gt; dynamical models defined by the probabilistic graphical model above.&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter-for-state-space-models&quot;&gt;Bayes filter for state space models&lt;/h2&gt;

&lt;p&gt;We have the task to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \). For this purpose only the structure of the graphical model will matter: it governs the conditional dependencies.
To unclutter the notation we will use \(\Box_{n:m}\) for \(\Box_n,…,\Box_m\).&lt;/p&gt;

&lt;p&gt;With help of &lt;strong&gt;Bayes’ rule&lt;/strong&gt; we can rewrite the formula as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t,y_{0:t-1},u_{0:t-1})p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.&lt;/script&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;If you are not very familiar with Bayes’ rule this can be quite confusing. There are much more moving parts than in the very simple definition. Nonetheless, there is an intuitive explanation.
It is Bayes’ rule applied in a world, where we already observed \(\mathcal{W}\) in the past (every term is conditioned on \(\mathcal{W}\)):&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x|\mathcal{D},\mathcal{W}) = \frac{p(\mathcal{D}|x,\mathcal{W})p(x|\mathcal{W})}{p(\mathcal{D}|\mathcal{W})}.&lt;/script&gt;

  &lt;p&gt;In our case \(x:=x_t \), \(\mathcal{D}:=y_t \) and \(\mathcal{W}:=(y_{0:t-1},u_{0:t-1}) \).&lt;/p&gt;

&lt;/div&gt;
&lt;p&gt;We note that \(y_t\) is independent of \(y_{0:t-1}\) and  \(u_{0:t-1}\) given \(x_t\). It follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.&lt;/script&gt;

&lt;div class=&quot;extra_box&quot;&gt;
  &lt;p&gt;This conditional independence property is not obvious as well. When it comes to conditional dependencies, it is always a good idea to look at the graphical model.&lt;/p&gt;

  &lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm_ind(this);&quot;&gt;&lt;/svg&gt;
  &lt;p&gt;In the figure above we notice that the node \(x_t\) is shaded (observed). This node blocks the way of \(y_{0:t-1}\) and  \(u_{0:t-1}\) to \(y_t\). We have proven the conditional independence &lt;em&gt;visually&lt;/em&gt;. You can learn more about conditional independence in probabilistic graphical models in &lt;a href=&quot;https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book&quot;&gt;Pattern Recognition and Machine Learning&lt;/a&gt; (Chapter 8.2).&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;The denominator is simply the integral of the numerator&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(y_t|y_{0:t-1},u_{0:t-1}) = \int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) dx_t .&lt;/script&gt;

&lt;p&gt;Great! We successfully expressed our equation in simpler terms. In return, we obtained the new expression \(p(x_t|y_{0:t-1},u_{0:t-1})\), which we have to calculate as well. Using marginalization we can express it as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t,x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;We can split the expression in the integral with product rule, which leads to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1},y_{0:t-1},u_{0:t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;Note that \(x_t\) is independent of \(y_{0:t-1}\) and \(u_{0:t-2}\) given \(x_{t-1}\). Furthermore, \(x_{t-1}\) is independent of \(u_{t-1}\). We obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1}, u_{t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-2}) dx_{t-1}.&lt;/script&gt;

&lt;p&gt;We note that \(p(x_{t-1}|y_{0:t-1},u_{0:t-2})\) has the same form as our expression we started from only shifted by one time step. Our recursive formula is complete!&lt;/p&gt;

&lt;p&gt;Let’s summarize our results!&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Bayes filter for state space models&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .&lt;/script&gt;

  &lt;p&gt;The recursion is started with the prior distribution over the initial state \(p(x_0)\).&lt;/p&gt;

&lt;/div&gt;

&lt;p&gt;Up to this point, we assumed that we obtain exactly one observation at every timestep. This rather limiting assumption is violated in many real-life scenarios. Multiple or even no observations per timestep are possible. This behavior is exemplified in the probabilistic graphical model below.&lt;/p&gt;

&lt;svg class=&quot;pgm_centered&quot; onload=&quot;draw_ssm_obs(this);&quot;&gt;&lt;/svg&gt;

&lt;p&gt;Fortunately, handling these cases is very simple. For every observation we make, we calculate the update step with the newest estimate available. Furthermore, it is not necessary that the observations are coming from the same output function (illustrated by the outputs \(y_2\) and \(z_2\) at \(t=2\)). &lt;a href=&quot;https://en.wikipedia.org/wiki/Information_integration&quot;&gt;Information integration/fusion&lt;/a&gt; is very natural in Bayesian inference.&lt;/p&gt;

&lt;p&gt;Nice! We just derived the equations of the Bayes filter for general state space models!
Now let’s translate this into the linear state space scenario.&lt;/p&gt;

&lt;h2 id=&quot;bayes-filter-in-linear-gaussian-state-space-models&quot;&gt;Bayes filter in linear Gaussian state space models&lt;/h2&gt;

&lt;p&gt;Let’s start by identifying the probability distributions we already know:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &amp;= \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t) \\
p(y_t|x_t) &amp;=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Furthermore, we assume that the prior distribution of the initial state is Gaussian as well. All probability distributions in our model are Gaussian. Therefore, the distributions \(p(x_t|y_{0:t-1},u_{0:t-1})\) and \(p(x_t|y_{0:t},u_{0:t-1})\) will also be in form of Gaussian distributions, because our recursive formula is only using marginalization and Bayes’ rule, which are closed under Gaussian distributions. In the context of Kalman filtering, these are normally defined by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
p(x_t|y_{0:t},u_{0:t-1}) &amp;:= \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t}) \\
p(x_t|y_{0:t-1},u_{0:t-1}) &amp;:= \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) .
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Please note, that these distributions are still implicitly dependent on the inputs and outputs. The mean and the covariance are a &lt;em&gt;sufficient statistic&lt;/em&gt; of the in- and outputs.&lt;/p&gt;

&lt;p&gt;The index \(\Box_{n|m}\) of the parameters indicates that the state at time \(n\) is estimated, based on the outputs upto time \(m\).
The expression \(\hat x_{t|t}\) is called the &lt;em&gt;updated&lt;/em&gt; state estimate and \( P_{t|t}\) the &lt;em&gt;updated&lt;/em&gt; error covariance. Moreover, \(\hat x_{t|t-1}\) is called the &lt;em&gt;predicted&lt;/em&gt; state estimate and \( P_{t|t-1}\) the &lt;em&gt;predicted&lt;/em&gt; error covariance.&lt;/p&gt;

&lt;p&gt;In summary, these are the equations for the Bayes filter in linear Gaussian state space models:&lt;/p&gt;
&lt;div class=&quot;important_box&quot;&gt;

  &lt;p&gt;&lt;strong&gt;Prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

  &lt;p&gt;&lt;strong&gt;Update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;/div&gt;

&lt;p&gt;Let’s try to simplify these equations!&lt;/p&gt;

&lt;h3 id=&quot;prediction-step&quot;&gt;Prediction step&lt;/h3&gt;

&lt;p&gt;We will start with the prediction step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.&lt;/script&gt;

&lt;p&gt;In order to find a closed form solution of this integral, we could simply plug in the corresponding expressions of the Gaussian distributions and solve the integral. Fortunately, Marc Toussaint already gathered the most important &lt;a href=&quot;https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf&quot;&gt;Gaussian identities&lt;/a&gt;, which will lighten our workload a lot.  To find an expression for our prediction step we can simply use the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ).&lt;/script&gt;

&lt;p&gt;By comparison with our expression, we see that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat x_{t+1|t} =  A_t \hat x_{t|t} + B_tu_t,&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t+1|t} = Q_t + A_t P_{t|t} A_t^T  .&lt;/script&gt;

&lt;h3 id=&quot;update-step&quot;&gt;Update step&lt;/h3&gt;

&lt;p&gt;We will start to simplify the update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}&lt;/script&gt;

&lt;p&gt;by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A &amp; A^TF^T\\FA &amp; B + FA^TF^T\end{matrix}\right) . %]]&gt;&lt;/script&gt;

&lt;p&gt;Then again, this joint distribution can be rewritten as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D &amp; F\\F^T &amp; E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) . %]]&gt;&lt;/script&gt;

&lt;p&gt;We can combine the two previous equations to the following expression&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .&lt;/script&gt;

&lt;p&gt;By comparison with the numerator of our update step, we obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})\mathcal{N}(y_{t}|C_tx_{t}, R_t ) = \mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t}),  P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;At a first glance, this is not looking like a simplification at all. Conceptually, we only transformed&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}.&lt;/script&gt;

&lt;p&gt;If we look closely at the final expression, we see that \(p(y)\) is canceling out. Therefore, the result is simply the remaining part&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t}),P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).&lt;/script&gt;

&lt;p&gt;If our reasoning is correct the denominator should be equal to \(\mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)\), which was canceled out. The denominator can be simplified with the &lt;em&gt;propagation&lt;/em&gt; formula (Formula 37, Toussaint)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|C_t\hat x_{t|t-1}, R_t + C_tP_{t|t-1}C_t^T ).&lt;/script&gt;

&lt;p&gt;Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.&lt;/p&gt;

&lt;p&gt;Let’s summarize our results:&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Bayes filter in linear Gaussian state space models&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Bayes filter in linear Gaussian state space models consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T  \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t|t} &amp;= \hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t}) \\ 
P_{t|t} &amp;= P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}.  \end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;
&lt;p&gt;That’s it! We derived the equations of the Bayes filter in linear Gaussian state space models, which is nothing else but the good old Kalman filter.
In the next section, we will split these equations up to finally obtain the formulation normally used for the Kalman filter.&lt;/p&gt;

&lt;h2 id=&quot;kalman-filter&quot;&gt;Kalman filter&lt;/h2&gt;

&lt;p&gt;In order to obtain the familiar equations of the Kalman filter we have to define&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Innovation&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;z_t = y_{t}-C_t\hat x_{t|t-1}&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Innovation covariance&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;S_t = R_t + C_tP_{t|t-1}C_t^T&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Optimal Kalman gain&lt;/strong&gt;&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;K_t = P_{t|t-1}C_t^TS_t^{-1}&lt;/script&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;extra_box&quot;&gt;

  &lt;p&gt;&lt;strong&gt;What is the meaning of \(z_t\) and \(S_t\)?&lt;/strong&gt;&lt;/p&gt;

  &lt;p&gt;The denominator of the update step is&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(y_{t}|C_t\hat x_{t|t-1},R_t + C_tP_{t|t-1}^TC_t^T)&lt;/script&gt;

  &lt;p&gt;and can be transformed by (Formula 34, Toussaint)&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(x|a,A) = \mathcal{N}(x+f|a+f,A)&lt;/script&gt;

  &lt;p&gt;to obtain the expression&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathcal{N}(y_{t} - C_t\hat x_{t|t-1}|0,R_t + C_tP_{t|t-1}^TC_t^T) = \mathcal{N}(z_t|0,S_t).&lt;/script&gt;

  &lt;p&gt;Therefore, the innovation \(z_t\) is the deviation of the expected output and the observed output.
The random variable \(z_t\) has a Gaussian distribution with zero mean and variance \(S_t\).&lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Let’s plug these definitions into the equations of our update step&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat x_{t|t} = \hat x_{t|t-1} + \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}(\underbrace{y_{t}-C_t\hat x_{t}}_{z_t})&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P_{t|t} = P_{t|t-1} - \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}P_{t|t-1} .&lt;/script&gt;

&lt;p&gt;This leads us to the final equations of the Kalman filter.&lt;/p&gt;

&lt;div class=&quot;important_box&quot;&gt;
  &lt;h1&gt;Equations of the Kalman filter&lt;/h1&gt;

  &lt;p&gt;The recursive formula for the Kalman filter consists of the &lt;strong&gt;prediction step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}\hat x_{t+1|t} &amp;=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &amp;= Q_t + A_t P_{t|t} A_t^T \end{align} %]]&gt;&lt;/script&gt;

  &lt;p&gt;and the &lt;strong&gt;update step&lt;/strong&gt;&lt;/p&gt;

  &lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
z_t &amp;= y_{t}-C_t\hat x_{t|t-1}\\
S_t &amp;= R_t + C_tP_{t|t-1}C_t^T\\
K_t &amp;= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &amp;= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &amp;= (I - K_tC_t)P_{t|t-1}.
\end{align} %]]&gt;&lt;/script&gt;

&lt;/div&gt;

&lt;h2 id=&quot;summary&quot;&gt;Summary&lt;/h2&gt;

&lt;p&gt;This article presented the derivation of the Kalman filter from first principles using Bayesian inference. The goal was to derive the Kalman filter in a clear and straightforward fashion. The steps were designed to be as atomic as possible, in order to be comprehensible for readers, who are not so familiar with the tools we used. Summarized, the derivation was performed in the following four subsequent steps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We realized, that we have to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \).&lt;/li&gt;
  &lt;li&gt;Derived the recursive equations of the Bayes filter to efficiently calculate this distribution.&lt;/li&gt;
  &lt;li&gt;Inserted the corresponding distributions of the linear Gaussian state space model.&lt;/li&gt;
  &lt;li&gt;Added some “sugar” to obtain the usual equations of the Kalman filter.&lt;/li&gt;
&lt;/ol&gt;

&lt;script src=&quot;http://localhost:4000/assets/js/d3_graphical_model.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;http://localhost:4000/assets/js/svg_mathjax.js&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot; src=&quot;https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG&quot;&gt;&lt;/script&gt;

&lt;script type=&quot;text/x-mathjax-config&quot;&gt;

var mq = window.matchMedia( &quot;(max-width: 570px)&quot; );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  &quot;HTML-CSS&quot;: { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

&lt;/script&gt;

&lt;script type=&quot;text/javascript&quot;&gt;new Svg_MathJax().install();&lt;/script&gt;</content><author><name></name></author><summary type="html">The concept and the equations of the Kalman filter can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using Bayesian inference. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.</summary></entry></feed>