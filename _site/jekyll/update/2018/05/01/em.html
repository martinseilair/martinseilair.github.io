<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.4.0 -->
<title>Inference | Your awesome title</title>
<meta name="generator" content="Jekyll v3.8.0" />
<meta property="og:title" content="Inference" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<meta property="og:description" content="Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/05/01/em.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/05/01/em.html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-05-01T18:04:07+09:00" />
<script type="application/ld+json">
{"description":"Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/05/01/em.html","headline":"Inference","dateModified":"2018-05-01T18:04:07+09:00","datePublished":"2018-05-01T18:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/05/01/em.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Inference</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-05-01T18:04:07+09:00" itemprop="datePublished">May 1, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script src="https://d3js.org/d3.v4.min.js"></script>

<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>Recently I thought a lot about statistical inference and it’s connections to Linear Algebra/Functional Analysis. Why should we maximize the likelihood? Why should we use EM? And what do both have in common.</p>

<p>All the questions that I had centered around one observation: If marginalization is nothing else but a linear transformation of the input distribution, why shouldn’t we just use the inverse operator to get the input distribution back?</p>

<h1>Marginal distribution</h1>

<p>The marginal distribution \(p(y)\) is defined as</p>

<script type="math/tex; mode=display">p(y) = \int_x p(y|x)p(x)dx</script>

<p>in the continuous case and as</p>

<script type="math/tex; mode=display">p(y) = \sum_x p(y|x)p(x)</script>

<p>in the discrete case. To those who have some experience Linear Algebra or Functional Analysis, these expressions could look familiar. Indeed it has the same functional form as an <a href="https://jekyllrb.com/docs/home">integral transform</a></p>

<script type="math/tex; mode=display">(Tf)(y) = \int_x K(y,x)f(x)dx</script>

<p>in the continuous case and an <a href="https://jekyllrb.com/docs/home">matrix multiplication</a> / linear map</p>

<script type="math/tex; mode=display">y_i = \sum_x A_{ij} x_i</script>

<p>in the discrete case.</p>

<h1>Infering \(p(x)\)</h1>

<p>Let’s define some kind of running example. Let’s say you have some arbitrary system that has \(N\) discrete states. The starting state \(x\) of the system is distributed according to \(p(x)\). You have a probabilistic dynamical model, that transforms the start state \(x\) and gives the next state \(y\). This model is described by \(p(y|x)\). Now we observe samples from the state \(y_n\) sampled from</p>

<script type="math/tex; mode=display">p(y) = \sum_x p(y|x)p(x) .</script>

<p>In my naivity I thought it was a good idea to use Bayes Theorem for this. I thought: I don’t want a point estimate of the state with the maximum likelihood, but a distribution over all states. After some more thinking and plotting I noticed, that this is not a good idea. Because the result was in the limit of infinite samples the same as maximum likelihood. In the following I want to sketch my path of reasoning.</p>

<p>Bayes Theorem is defined as</p>

<script type="math/tex; mode=display">p(x|y) = \frac{p(y|x)p(x)}{p(y)}  .</script>

<p>The derivation makes use of the chain rule of probability, that states that \(p (x,y) = p(x|y)p(y) = p(y|x)p(x) \). By rearranging the terms you finally arrive at Bayes Theorem. The methods of Bayesian inference are all based on this formula. At the beginning you choose a prior distribution  \( p(x) \) , which can be used to encode prior beliefs about \(x\).
Let’s draw our first sample \(y_0\) from \(p(y)\) and see what happens.</p>

<p>The main operation happening is the point wise multiplication of \(p(y=y_0|x)\) and \(p(x)\). In the case of discrete \(x\) this will be a point wise mulitplication of vectors. In the case of continuous \(x\) this will be a point wise multiplication of functions. If you take again the analogy to Linear Algebra, \(p(y=y_0|x)\) would be the \(y_0\)th row in the matrix \(p(y|x)\). So you are essentially multiplying the prior vector with the row of \(p(y|x)\) corresponding to your sample.</p>

<p>Finally you normalize to obtain a valid probability distribution. The normalizer is therefore the sum over \(x\) of the point wise product \( \sum_x p(y=y_0|x)p(x) \). The proabability distribution you will receive is called the posterior distribution. Your prior belief was transformed to the posterior belief. To be ready for the next sample, you just “rename” yo\the posterior to prior and start over again. This sequential process can be described as</p>

<script type="math/tex; mode=display">p(x|Y) = \frac{\prod_{n = 0}^N p(y=y_n|x) p(x)}{Z}</script>

<p>where \(Z\) is simply the normalizer. So it seems, that Bayes Method is all about sequential pointwise mulitpliation of rows of \(p(y|x)\). 
But what happens if our number of samples \(N\) goes to \(\infty\)? Let’s see.</p>

<p>First concentrate on the product</p>

<script type="math/tex; mode=display">\prod_{n = 0}^N p(y=y_n|x)</script>

<p>and use the identity \(f = ((f)^\frac{1}{n})^n\) to get</p>

<script type="math/tex; mode=display">\left(\left(\prod_{n = 0}^N p(y=y_n\|x)\right)^\frac{1}{n}\right)^n .</script>

<p>Use another identity \(f = \exp(\log(f))\) to arrive at</p>

<script type="math/tex; mode=display">\left(\exp\left(\log\left(\left(\prod_{n = 0}^N p(y=y_n\|x)\right)^\frac{1}{n}\right)\right)\right)^n .</script>


  </div><a class="u-url" href="/jekyll/update/2018/05/01/em.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Your awesome title</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Your awesome title</li><li><a class="u-email" href="mailto:your-email@example.com">your-email@example.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li><li><a href="https://www.twitter.com/jekyllrb"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg> <span class="username">jekyllrb</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
