<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Observability: A Bayesian perspective | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Observability: A Bayesian perspective" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability." />
<meta property="og:description" content="Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability." />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-07T01:04:07+09:00" />
<script type="application/ld+json">
{"description":"Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability.","@type":"BlogPosting","url":"/jekyll/update/2018/11/07/observability.html","headline":"Observability: A Bayesian perspective","dateModified":"2018-11-07T01:04:07+09:00","datePublished":"2018-11-07T01:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"/jekyll/update/2018/11/07/observability.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1541580342343322721"><link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Ikigai" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-128910439-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Observability: A Bayesian perspective</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-11-07T01:04:07+09:00" itemprop="datePublished">Nov 7, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Observability is an important concept of classical control theory. Quite often it is motivated by abstract concepts, that are not intuitive at all. In this article, we will take a look at observability from a Bayesian perspective and will find a natural interpretation of observability.
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script></p>

<p>Let’s begin by stating the definition of <a href="https://en.wikipedia.org/wiki/Observability">observability</a> from classical control theory.</p>
<div class="important_box">
  Formally, a system is said to be observable, if for any possible sequence of state and control vectors (the latter being variables whose values one can choose), the current state (the values of the underlying dynamically evolving variables) can be determined in finite time using only the outputs.
</div>

<p>We can easily translate this definition into the language of Bayesian inference:</p>

<div class="important_box">
  A system is said to be observable if for any possible initial state and sequence of control vectors, the probability mass of the posterior of the current state will collapse into a single point in finite time.
</div>

<p>Normally, we are using the idea of observability in the setting of deterministic time-invariant linear state space models, which are defined by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}x_{t+1} &= Ax_t + B u_t \\ 
y_t &= Cx_t  \end{align} %]]></script>

<p>with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A\), input matrix \(B\) and output matrix \(C\).</p>

<p>Based on the methods shown in the last post about <a href="/jekyll/update/2018/11/06/linalg-gaussian.html">linear algebra with Gauss and Bayes</a>, we can reformulate these deterministic equations with Gaussian distributions</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|Ax_t + Bu_t, \delta I)</script>

<p>and</p>

<script type="math/tex; mode=display">\mathcal{N}(y_t|Cx_t, \delta I)</script>

<p>where \(\delta \to 0\).</p>

<p>Now that we arrived at a probabilistic description, we can use Bayesian inference to infer the current state \(x_t\). In particular, we are interested in the uncertainty of our estimate of the current state: Our system will be observable if the covariance of the estimate will go to zero.</p>

<h1 id="derivation">Derivation</h1>

<p>First of all, we are defining a Gaussian prior distribution of the initial state \(x_0\)</p>

<script type="math/tex; mode=display">p(x_0) = \mathcal{N}(x_0|0,I).</script>

<p>The choice of the mean and covariance are actually arbitrary, as long as the covariance is positive definite.</p>

<p>Now let’s plug our distributions into the equations of the Bayes filter, which are described by the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .</script>

<p>Fortunately, we already know how to do inference in linear Gaussian state space models. We can simply use the equations of the Kalman filter and obtain the following equations for the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A \hat x_{t|t} + Bu_t \\ 
P_{t+1|t} &= \delta I + A P_{t|t} A^T  \end{align} %]]></script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C^T(\delta I + CP_{t|t-1}C^T)^{-1}(y_{t}-C\hat x_{t|t-1}) \\ 
P_{t|t} &= P_{t|t-1} - P_{t|t-1}C^T (\delta I + CP_{t|t-1}C^T)^{-1}CP_{t|t-1} .\end{align} %]]></script>

<p>If this was too fast, please check out the earlier blog post on <a href="/jekyll/update/2018/10/10/kalman_filter.html">Kalman filtering</a>.</p>

<p>Observability depends only on the covariance of the estimates \( P \). Therefore, the question of observability of a linear state space model is reduced to the question, if the equations</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
P_{t+1|t} &= \delta I + A P_{t|t} A^T \\ 
P_{t|t} &=(I-P_{t|t-1}C^T(\delta I + CP_{t|t-1}C^T)^{-1}C)P_{t|t-1}  
\end{align} %]]></script>

<p>are going to transform an arbitrary positive definite initial covariance matrix \(P_0\) to 0.</p>
<div class="extra_box">
  <p>When we combine the prediction and update to a single equation</p>

  <script type="math/tex; mode=display">P_{t+1} = \delta I + A (I-P_{t}C^T(\delta I + CP_{t}C^T)^{-1}C)P_{t} A^T</script>

  <p>and look very closely we can identify the <a href="https://en.wikipedia.org/wiki/Algebraic_Riccati_equation#Context_of_the_discrete-time_algebraic_Riccati_equation">discrete-time algebraic Ricatti equation</a></p>

  <script type="math/tex; mode=display">P_{t+1} = \delta I + AP_{t} A^T  - AP_{t}C^T(\delta I + CP_{t}C^T)^{-1}CP_{t} A^T.</script>

</div>
<p>Let’s try to interpret what our two equations are doing with the covariance estimate \(P\). As a mental model, it is helpful to imagine the particular covariance matrices as subspaces.
We begin with our prior variance \(P_0\). We have selected our prior variance in such a way, that it describes the entire state space.</p>

<p>We are starting by taking an update step. The update step can be interpreted as calculating the intersection of the prior subspace and the subspace defined by all points \(x\) that map to the observed output \(y_0\). We will call this last subspace the <em>inverse subspace</em>.
We took the intersection of the whole state space and <em>inverse subspace</em>. As a result, our posterior will be simply the inverse subspace.
Let’s see what happens, if we take the prediction step. If we assume that \(A\) has full rank, the dimensionality of the subspace will remain the same. Depending on the matrix \(A\) two things can happen:</p>

<ol>
  <li>The transformation won’t change the subspace, but only the representation of the subspace. It is an <a href="https://en.wikipedia.org/wiki/Invariant_subspace">invariant subspace</a> with respect to transformation \(A\).</li>
  <li>The transformation is changing the subspace.</li>
</ol>

<p>Depending on these two cases we will have two cases for the next update step:</p>

<p><strong>The transformation didn’t change the subspace.</strong> In this case, the update step would have no effect, because we are intersecting again with the <em>same</em> inverse subspace. Formally, after the prediction step our posterior would still be the orthogonal <a href="https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Projectors">projector</a> onto the kernel of \(C\)</p>

<script type="math/tex; mode=display">P_{0|0} = I - C^+C.</script>

<p>We know that \(C(I - C^+C) = 0\) and \((I - C^+C)C^T = 0\), therefore, our update step simplifies to</p>

<script type="math/tex; mode=display">P_{t|t} =P_{t|t-1}-\underbrace{P_{t|t-1}C^T}_{0}(\delta I + \underbrace{CP_{t|t-1}}_{0}C^T)^{-1}\underbrace{CP_{t|t-1}}_{0} =  P_{t|t-1}.</script>

<p>It seems, that we can’t rid of this <em>unobservable</em> subspace. Therefore, we have <strong>no</strong> observability.</p>

<p><strong>The transformation did change the subspace.</strong> In this case, the intersection with the inverse subspace will again have an effect. The dimensionality of the posterior subspace will get smaller.</p>

<p>We have to repeat the process of prediction and updating until the subspace of our posterior has either dimension zero or the prediction step is again not changing our subspace.
In the first case, we have no uncertainty: The system is observable. In the second case, we identified a invariant subspace. Therefore, the system is not observable.</p>

<h1 id="summary">Summary</h1>

<p>In this post, we looked at the concept of observability from a Bayesian standpoint. We found an intuitive way to reason about the effect of the update step and prediction step in terms of subspaces, described by covariance matrices.</p>


  </div><a class="u-url" href="/jekyll/update/2018/11/07/observability.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
