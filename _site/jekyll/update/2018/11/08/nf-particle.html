<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Nonlinear filtering: Particle filter | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Nonlinear filtering: Particle filter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example." />
<meta property="og:description" content="This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example." />
<link rel="canonical" href="https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html" />
<meta property="og:url" content="https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-11-08T07:04:07+09:00" />
<script type="application/ld+json">
{"description":"This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example.","@type":"BlogPosting","url":"https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html","headline":"Nonlinear filtering: Particle filter","dateModified":"2018-11-08T07:04:07+09:00","datePublished":"2018-11-08T07:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://martinseilair.github.io/jekyll/update/2018/11/08/nf-particle.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1543041753852430753"><link type="application/atom+xml" rel="alternate" href="https://martinseilair.github.io/feed.xml" title="Ikigai" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-128910439-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Nonlinear filtering: Particle filter</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-11-08T07:04:07+09:00" itemprop="datePublished">Nov 8, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This article, treating the derivation of the particle filter, marks the last part of the nonlinear filtering series. We will derive the particle filter algorithm directly from the equations of the Bayes filter. In the end, we will have the opportunity to play around with the particle filter with our toy example. <!--more-->If you haven’t read the <a href="/jekyll/update/2018/10/29/nf-intro.html">introduction</a>, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind the particle filter.
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script></p>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/util.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />

<link rel="stylesheet" type="text/css" href="https://martinseilair.github.io/assets/css/nonlinear_filter/style.css" />

<script type="text/javascript">


	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>

<div class="important_box">
  <p>The <strong>particle filter</strong> is a sample-based approximation of the Bayes filter. It is used to <strong>infer</strong> the current state of an arbitrary probabilistic state space model given all observations and inputs up to the current timestep and a prior distribution of the initial state.</p>
</div>

<h2 id="derivation">Derivation</h2>

<p>In this section, we will develop the method of particle filtering directly from the Bayes filter and mean field particle methods.</p>

<p>We learned that the recursive equations of the Bayes filter consist of the 
 <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\,dx_t}</script>

<p>and that the integrals, contained in these equations, are in general intractable. The main idea of the particle filter is to approximate the Bayes filter by approximating the current posterior \(p(x_{t+1}|y_{0:t},u_{0:t})\) and \(p(x_t|y_{0:t},u_{0:t-1})\) with <a href="https://en.wikipedia.org/wiki/Empirical_measure">empirical measures</a>  \(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})\) and \(\hat{p}(x_t|y_{0:t},u_{0:t-1})\). Let’s take a brief look at empirical measures.</p>

<div class="important_box">
  <h1>Empirical measure</h1>
  <p>The empirical measure of \(p(x)\) is defined as</p>

  <script type="math/tex; mode=display">p_N(x) = \frac{1}{N}\sum_{i=1}^{N} \delta_{x_i}(x),</script>

  <p>where \(\delta_{x_i}(x)\) is an abbreviation for the shifted <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta function</a> \(\delta(x-x_i)\) and \(x_{1:N}\) are \(N\) samples from \(p(x)\).
If the number of samples goes to infinity, the empirical measure will <a href="https://en.wikipedia.org/wiki/Almost_surely">almost surely</a> converge to the distribution \(p(x)\). The following figure shows the distribution \(p(x)\) in red and the empirical measure \(p_N(x)\) in black.</p>

  <div style="text-align:center; width:100%"><div id="dirac_plot" style="width:75%;display:inline-block;"></div></div>
  <script>
function load_em_meas(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = sample_gmm(mix, gs, n_sam, dom);
	for (var i = 0; i < n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#dirac_plot", samples, [dat], dom, 0.2, false, 0.7);
}
load_em_meas();
</script>

  <p>Please be aware that the lines, representing the Dirac delta functions, would actually be infinitely high. But in order to visualize it, we set the length of the lines to be the area under the corresponding Dirac delta function. In our case, this area will be \(\frac{1}{N}\).</p>

  <p>Up until now, we assumed that each Dirac delta function is weighted uniformly. We can also define a <strong>weighted empirical measure</strong> by</p>

  <script type="math/tex; mode=display">p_N(x) =\sum_{i=1}^{N} w_i\delta_{x_i}(x),</script>

  <p>with \(\sum_{i=1}^{N}w_i = 1\).</p>

</div>

<p>In the following, we will call the tuple \((x_i, w_i)\), corresponding to the position and weight of the \(i\)th Dirac delta function, a <strong>particle</strong>. Now that we have an idea about empirical measures, we can directly start our derivation with the update step.</p>

<h1 id="update-step">Update step</h1>

<p>The first step is to replace the posterior distribution \(p(x_t|y_{0:t},u_{0:t-1})\) with the empirical measure \(\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) \). This empirical measure could look like this:</p>

<div style="text-align:center; width:100%"><div id="prior_plot" style="width:75%;display:inline-block;"></div></div>
<script>
var n_sam = 50;
var mix = [0.8,0.2];
var gs = [[1.5,0.6],[4.0,0.6]];
var dom = [0.0, 5.0];
var n_plot = 1000;
var gdata= [];
var samples = sample_gmm(mix, gs, n_sam, dom);
var dat = [];
function load_prior_meas(){

	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#prior_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas();
</script>

<p>We plug our empirical measure into the update step and obtain</p>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\int_{x_t} p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) dx_t} .</script>

<p>We are multiplying the empirical measure and the likelihood function pointwise in the numerator. This pointwise multiplication can be understood graphically:</p>

<div style="text-align:center; width:100%"><div id="prior_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_prior_meas_mal(){
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#prior_mal_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas_mal();
</script>

<script type="math/tex; mode=display">*</script>

<div style="text-align:center; width:100%"><div id="likelihood_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_likelihood_meas_mal(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = [];
	for (var i = 0; i < n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	
	create_dirac_plot("#likelihood_mal_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_likelihood_meas_mal();
</script>

<script type="math/tex; mode=display">=</script>

<div style="text-align:center; width:100%"><div id="posterior_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_posterior_meas_mal(){
	var sw = samples.map((e)=>{return {x:e, w:gmm(e, mix, gs)};})

	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#posterior_mal_plot", sw, [dat], dom, 0.2, false, 0.2);
}
load_posterior_meas_mal();
</script>

<p>If we look closely at the denominator, we see that we are integrating a function which is weighted by a Dirac delta function. Therefore, we can use the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function#Translation">sifting property</a> of the Dirac delta function to obtain</p>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)},</script>

<p>where we canceled out the factor \(\frac{1}{N}\). As a result, we have a <em>weighted</em> empirical measure</p>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) =\sum_{i=1}^{N} w_i\delta_{\xi_t^i}(x_t),</script>

<p>with weights</p>

<script type="math/tex; mode=display">w_i = \frac{p(y_t|x_t=\xi_t^i))}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)}.</script>

<p>This leads us directly to the important resampling step.</p>

<h1 id="resampling-step">Resampling step</h1>

<p>In the resampling step, we are replacing our weighted empirical measure with an (unweighted) empirical measure. This is equivalent to sampling from a <a href="https://en.wikipedia.org/wiki/Categorical_distribution">categorical distribution</a>, where our weights are defining the probability mass function. Therefore, the probability of drawing particles with high weights is higher than drawing particles with low weights. As a result, it is possible that particles with low weights are not drawn at all and particles with high weights are drawn multiple times. 
You may ask, why we are doing this weird resampling step? The main idea is quite simple:
We only have a limited number of particles, therefore, we want to discard hypotheses that have low probability. Otherwise, weights of some of the particles could get close to zero and would become useless.</p>

<p>It can also be interpreted as a selection step in the context of <a href="https://en.wikipedia.org/wiki/Evolutionary_algorithm">evolutionary algorithms</a>. Only the <em>fittest</em> particles are able to produce the next generation of particles.</p>

<p>As a result of the resampling step, we will obtain \(N\) particles \((\hat{\xi} _ t^{i},\frac{1}{N})_{1:N}\), which were drawn from</p>

<script type="math/tex; mode=display">\hat{\xi}_t^i \sim \hat{q}(x_t|y_{0:t},u_{0:t-1}).</script>

<h1 id="prediction-step">Prediction step</h1>
<p>Let’s look at the prediction step and replace the posterior with our empirical measure from the update step:</p>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})\frac{1}{N}\sum_{i=1}^N \delta_{\hat{\xi}_t^i}(x_t) dx_{t}.</script>

<p>By changing the order of the sum and integral</p>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N\int_{x_{t}} p(x_{t+1}|x_{t}, u_{t}) \delta_{\hat{\xi}_t^i}(x_t) dx_{t},</script>

<p>we note that we can use the sifting property again and finally obtain</p>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N p(x_{t+1}|x_{t} = \hat{\xi}_t^i, u_{t}).</script>

<p>Therefore, the distribution \(\hat{q}(x_{t+1}|y_{0:t},u_{0:t})\) is a weighted sum of <em>continuous</em> distributions over \(x_{t+1}\), which is shown schematically in the following figure.</p>

<div style="text-align:center; width:100%"><div id="pred_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_pred_meas(){
	var n_sam = 50;
	var mix = [0.2, 0.2, 0.2, 0.2];
	var gs = [[1.5,0.6], [4.0,0.3], [2.5,0.3], [3.0,0.4]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var samples = [];
	var datdat = [];

	var sum_data = [...Array(n_plot)].map((e,i)=>{return {x:dom[1]*i/n_plot,y:0}});

	for (var j = 0; j < mix.length; j++) {
		var dat = [];
		var gdata= [];
		dat.color = "red";
		for (var i = 0; i < n_plot; i++) {
			var x = dom[1]*i/n_plot;	
			var v = mix[j]*gaussian(x,gs[j][0],gs[j][1] );
			gdata.push({x:x, y:v});
			sum_data[i].y+=v;
		}
		dat.gdata = gdata;
		datdat.push(dat);
	}

	var dat = [];
	dat.color="blue";
	dat.gdata=sum_data;
	datdat.push(dat);



	create_dirac_plot("#pred_plot", samples, datdat, dom, 0.2, false, 0.7);
}
load_pred_meas();
</script>

<p>We note, that even if we have an empirical measure as current belief distribution, we will obtain a continuous distribution for our new belief.</p>

<p>Fortunately, we already know how to obtain an empirical measure of a continous distribution: We simply have to sample from it. But how can we sample from our new belief?</p>

<p>First, we take a sample from our current empirical posterior distribution. Based on this sample we can sample from the system distribution to obtain a sample from the new posterior. This sampling procedure samples exactly from our new continuous posterior distribution.</p>

<p>Nice! We are essentially done, we expressed the update and prediction step in terms of empirical measures.</p>

<p>But one thing seems a little bit weird. In order to obtain the empirical measure of the new  posterior in the prediction step, we have to sample from the current posterior, that was obtained from the resampling step. But in the resampling step, we already sampled from our posterior distribution. Therefore, we can take the particles of the current posterior directly as our samples.</p>

<p>Let’s summarize the algorithm of the particle filter.</p>

<div class="important_box">
  <h1>Algorithm of the particle filter</h1>

  <p>The algorithm is <strong>initialized</strong> with a set of \(N\) particles \((\xi_0^{i},\frac{1}{N})_{1:N}\).</p>

  <p>We are taking the <strong>update step</strong> and obtain the distribution</p>

  <script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\frac{1}{N}\sum_{i=1}^N p(y_t|x_t=\xi_t^i)},</script>

  <p>which is equivalent to the particles \((\xi _ t^{i},\frac{p(y_t|x_t=\xi_t^i)}{\sum_{i=1}^N p(y_t|x_t=\xi_t^i)})_{1:N}\).</p>

  <p>In the <strong>resampling step</strong>, we are taking \(N\) samples</p>

  <script type="math/tex; mode=display">\hat{\xi}_t^i \sim \hat{q}(x_t|y_{0:t},u_{0:t-1})</script>

  <p>and obtain new particles \((\hat{\xi} _ t^{i},\frac{1}{N})_{1:N}\).</p>

  <p>Finally, we are taking the <strong>prediction step</strong>. We obtain the distribution</p>

  <script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N p(x_{t+1}|x_{t} = \hat{\xi}_t^i, u_{t})</script>

  <p>and sample from it to get new particles \((\xi _ {t+1}^{i},\frac{1}{N})_{1:N}\).</p>

  <p>The process starts again with the <strong>update step</strong>.</p>

</div>

<p>Now that we are finally done with the derivation, let’s see how the particle filter performs in our toy example.</p>

<h1 id="example">Example</h1>

<script>

		// defines scenes
	n_scene = load_race_track("race_track_particle", "https://martinseilair.github.io");
	n_scene.mode = 2;
	//n_scene.filter = "particle";
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;

	n_scene.t = 1;

	n_scene.ids = ["race_track_particle_timestep", "race_track_particle_likelihood", "race_track_particle_update","race_track_particle_predict","race_track_particle_resampling" ];

	n_scene.take_observation = true;


	n_scene.loaded = function(){
		document.getElementById("race_track_particle_likelihood").style.display="block";
		this.rt.hide_strip("inner");
		this.pf = init_particle_filter(this.rc, this.rt)


        var inner_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style("background-color")), d3.rgb('#ff834d'), d3.rgb('#8e3323')]);
		this.rt.init_strip("inner", get_output_dist_normalized(this.rc, this.rt, this.rc.state), inner_color, 60);
		

        this.restart = function(){
            for (var i=0; i<this.ids.length;i++){

                document.getElementById(this.ids[i]).style.display="none";
            }
            document.getElementById("race_track_particle_likelihood").style.display="block";
            this.rc.reset();
            this.t = 1;

            //this.bf.reset();
            this.pf.reset()

            this.rt.hide_strip("inner");
            this.rt.treeg.style("opacity",1.0)
			this.take_observation = true;
        }




        this.rt.set_restart_button(this.restart.bind(this))



        this.toogle_observation = function(){
			if(this.take_observation){
				this.rt.treeg.style("opacity",0.2)
				this.take_observation = false;
				if(this.t% 5 ==1){
					document.getElementById("race_track_particle_likelihood").style.display="none";
					document.getElementById("race_track_particle_timestep").style.display="block";
					this.t = 4;
				}
			}else{
				this.rt.treeg.style("opacity",1.0)
				this.take_observation = true;
			}
        	
        }

		this.rt.tree_click = this.toogle_observation.bind(this)




	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;


		for (var i=0; i<this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display="none";
		}
		


		if(this.t % 5 == 0){
			this.rc.step(scene.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById("race_track_particle_predict").style.display="block";
		}if(this.t % 5 == 1){
	    	
			//this.rt.update_strip("outer", get_system_dist_normalized(scene.rc, scene.rt, scene.rc.state, scene.rc.current_input));

			this.pf.predict(this.last_input);
			if(this.take_observation){
				document.getElementById("race_track_particle_likelihood").style.display="block";
			}else{
				document.getElementById("race_track_particle_timestep").style.display="block";
				this.t=4;
			}
			
		}else if(this.t % 5 == 2){
			this.rt.show_strip("inner");
			this.output = this.rc.output_dist_sample(0);
			this.rt.update_strip("inner", get_output_dist_normalized_from_distance(this.rc, this.rt, this.output));
			document.getElementById("race_track_particle_update").style.display="block";
		}else if(this.t % 5 == 3){
	    	this.pf.update(this.output, 0);
	    	document.getElementById("race_track_particle_resampling").style.display="block";
		}else if(this.t % 5 == 4){
			this.rt.hide_strip("inner");
			this.pf.ancestor_sampling();
			document.getElementById("race_track_particle_timestep").style.display="block";
		}



	}.bind(n_scene);

	scenes_name["race_track_particle"] = n_scene;
	scenes.push(n_scene);
</script>

<svg id="race_track_particle" style="width:100%" onclick="on_click()"></svg>

<div id="race_track_particle_timestep" class="button_set">
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=0;scenes_name['race_track_particle'].step();">Backward</div>
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=1;scenes_name['race_track_particle'].step();">No action</div>
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=2;scenes_name['race_track_particle'].step();">Forward</div>
 <span class="stretch"></span>
</div>

<div id="race_track_particle_predict" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Predict step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_likelihood" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Observe</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_update" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Update step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_resampling" class="button_set" onclick="scenes_name['race_track_particle'].step();">
<div class="bt1  bt">Resampling</div>
  <span class="stretch"></span>
</div>

<p>On the outside of the race track, you will notice uniformly distributed blue dots. These dots are our particles and, therefore, represent a set of hypotheses of the position of the race car. By pressing the <strong>OBSERVE</strong> button two things will happen: first, we will take a measurement to the distance of the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the <strong>UPDATE STEP</strong> button, we will perform our update step. The size of the blue dots will change according to their weighting. By pressing the <strong>RESAMPLING</strong> button, we are performing the resampling step. As a result, you will notice that only the dots with high weights remains. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the <strong>PREDICT STEP</strong> button. You will see that the dots will change accordingly to your chosen action. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.</p>

<p>If you want to reset the environment, just press the reset button in the bottom left corner or press the <strong>R</strong> button on your keyboard.
As before you can control the car by using your keyboard: <strong>A</strong> (Backward), <strong>S</strong> (Stop),  <strong>D</strong> (Forward) or the buttons below the race track.</p>

<h1 id="acknowledgement">Acknowledgement</h1>

<p>The vector graphics of the <a href="https://www.freepik.com/free-photos-vectors/car">car</a> were created by <a href="https://www.freepik.com/">Freepik</a>.</p>


  </div><div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://ikigai-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/jekyll/update/2018/11/08/nf-particle.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
