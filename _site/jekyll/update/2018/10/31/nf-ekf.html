<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Nonlinear filtering: Extended Kalman filter | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Nonlinear filtering: Extended Kalman filter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter." />
<meta property="og:description" content="This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter." />
<link rel="canonical" href="https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html" />
<meta property="og:url" content="https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-31T23:04:07+09:00" />
<script type="application/ld+json">
{"description":"This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter.","@type":"BlogPosting","url":"https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html","headline":"Nonlinear filtering: Extended Kalman filter","dateModified":"2018-10-31T23:04:07+09:00","datePublished":"2018-10-31T23:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://martinseilair.github.io/jekyll/update/2018/10/31/nf-ekf.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1542018238110391177"><link type="application/atom+xml" rel="alternate" href="https://martinseilair.github.io/feed.xml" title="Ikigai" /><script>
if(!(window.doNotTrack === "1" || navigator.doNotTrack === "1" || navigator.doNotTrack === "yes" || navigator.msDoNotTrack === "1")) {
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-128910439-1', 'auto');
  ga('send', 'pageview');
}
</script>
  
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Nonlinear filtering: Extended Kalman filter</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-31T23:04:07+09:00" itemprop="datePublished">Oct 31, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>This article is the second part of the nonlinear filtering series, where we will derive the extended Kalman filter with non-additive and additive noise directly from the recursive equations of the Bayes filter.  <!--more--> If you haven’t read the <a href="/jekyll/update/2018/10/29/nf-intro.html">introduction</a>, I would recommend to read it first. Before we dive into the derivation, let’s try to state the main idea behind extended Kalman filter.
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script></p>

<script src="https://cdn.plot.ly/plotly-latest.min.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/particle_filter.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/race_car.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/race_track.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/util.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/plot.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/scene.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/ekf.js"></script>

<script src="https://martinseilair.github.io/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />

<link rel="stylesheet" type="text/css" href="https://martinseilair.github.io/assets/css/nonlinear_filter/style.css" />

<script type="text/javascript">


	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>

<div class="important_box">
  <p>The <strong>extended Kalman filter</strong> approximates the Bayes filter by linearizing the system and observation equations.</p>
</div>

<h2 id="derivation">Derivation</h2>

<p>We will start the derivation directly from the recursive equations of the Bayes filter with the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t}p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) \,dx_t} .</script>

<p>The extended Kalman filter is normally formulated with nonlinear functions with additive noise. In this article, we directly derive the general case for non-additive noise and obtain the extended Kalman filter as a special case. Therefore, our equations of the system are</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 x_{t+1} &= f(x_t, u_t, w_t) \\
 y_t &= h(x_k, v_k).
 \end{align} %]]></script>

<p>with Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).</p>

<p>In our formula of the Bayes filter, we can’t find any functions \(f(x_t, u_t, w_t)\) or \(h(x_k, v_k)\). Therefore, our first step will be to express these functions as probability distributions \(p(x_{t+1}|x_{t}, u_{t})\) and \(p(y_t|x_t)\). The next box will show how to achieve this in general. <strong>Warning:</strong> Distributions are very weird and the following treatment is <strong>not rigorous</strong>.</p>
<div class="extra_box">
  <p>We want to calculate \(p(y|x)\) given the function \(y=f(x,z)\) and \(p(z)\).
We can express the deterministic function \(y=f(x,z)\) as probability distribution</p>

  <script type="math/tex; mode=display">p(y|x,z) = \delta(y-f(x,z)),</script>

  <p>with the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta function</a> \(\delta(x)\).</p>

  <p>In order to calculate \(p(y|x)\) we can simply marginalize out \(z\):</p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(y|x) &= \int_z p(y|x,z)p(z)\, dx \\
 &= \int_z \delta(y-f(x,z))p(z)\, dx 
\end{align} %]]></script>

  <p>By using the composition rule of the Dirac delta function, we can express this as</p>

  <script type="math/tex; mode=display">p(y|x) = \sum_i \frac{p(z_i)}{\left|\det\nabla_z f(x,z)|_{z_i}\right|},</script>

  <p>where the sum goes over all \(z_i\) which satisfy the equation \(y = f(x,z)\).</p>
</div>

<p>In our case, we can express our system function \(x_{t+1} = f(x_t,u_t,w_t)\) as a probability distribution</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \sum_i \frac{p(w_i)}{\left|\det\nabla_w f(x_t, u_t, w_t)|_{w_i}\right|},</script>

<p>where the sum goes over all \(w_i\) which satisfy \(x_{t+1} = f(x_t,u_t,w_t)\).</p>

<p>Similarly, we can express our observation function  \(y_t = h(x_k, v_k)\) as probability distribution</p>

<script type="math/tex; mode=display">p(y_t|x_t) = \sum_i \frac{p(v_i)}{\left|\det\nabla_v h(x_t, v_t)|_{v_i}\right|}</script>

<p>where the sum goes over all \(v_i\) which satisfy \(x_{t+1} = h(x_t,v_t)\).</p>

<p>Even if we are assuming Gaussian process and measurement noise, the resulting distributions of our model will, in general, be non-Gaussian.
This is where the extended Kalman filter comes into play. It approximates our probability distributions by using linearization.
In my limited scope, linearization of a probability distribution makes no sense. But what is linearized instead?</p>

<p>Instead of linearizing our probability distributions we will do this with our deterministic functions before we express them as probability distributions.</p>

<p>In order to linearize, we are performing a first-order Taylor expansion</p>

<script type="math/tex; mode=display">f(x_t, u_t, w_t) \approx f(x_t, u_t, w_t)|_{x_t=\hat x_{t|t},u_t=u,w_t=0} + \nabla_{x_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}(x_t - x)+ \nabla_{u_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}(u_t - u) + \nabla_{w_t} f(x_t, u_t, w_t)^T|_{x_t=\hat x_{t|t},u_t=u,w_t=0}w_t</script>

<p>around the current state estimate \(x_t = \hat x_{t|t}\), current input \(u_t=u\) and zero process noise \(w_t=0\).</p>

<p>To unclutter the notation, we define \(A_t = \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) , \(B_t = \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) and \(L_t = \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\) to finally obtain the much cleaner representation</p>

<script type="math/tex; mode=display">\hat{f}(x_t, u_t, w_t) = f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u)  + L_tw_t.</script>

<p>Now we are ready to transform our linearized deterministic system function into a probability distribution. We will use the same formula as above, but replace the true function \(f(x_t, u_t, w_t)\) with \(\hat{f}(x_t, u_t, w_t)\), and arrive at</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \sum_i \frac{\mathcal{N}(w_i|0,Q_t)}{\left|\det\nabla_w \hat{f}(x_t, u_t, w_t)|_{w_i}\right|},</script>

<p>where the sum is over all \(w_i\) which satisfy \(x_{t+1} = \hat{f}(x_t, u_t, w_t)\). Let’s try to simplify this expression! First, we notice that if the matrix \(L_t\) is invertible, then there is exactly one \(w\) that satisfies \(x_{t+1} = \hat{f}(x_t, u_t, w_t)\). We can express it as</p>

<script type="math/tex; mode=display">w = L_t^{-1}\left(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)\right).</script>

<p>Note, that if the martrix \(L_t\) is not invertible, we would have to integrate over the whole null space and the sum would become an integral.</p>

<p>Next, let’s look at the denominator. We can express the derivative of \(\hat{f}(x_t, u_t, w_t)\) with respect to \(w_t\) as</p>

<script type="math/tex; mode=display">\nabla_w \hat{f}(x_t, u_t, w_t) =\nabla_w( f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u)  + L_tw_t) = L_t.</script>

<p>Let’s plug in the information our new information about \(w\) and the derivative:</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \frac{\mathcal{N}(L_t^{-1}\left(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)\right)|0,Q_t)}{\left|\det L_t\right|}.</script>

<p>We apply the transformation identity (Formula 35, <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf">Toussaint</a>):</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \frac{\frac{1}{\left|\det L_t^{-1}\right|}\mathcal{N}(x_{t+1} - f(\hat x_{t|t}, u, 0) - A_t(x_t - \hat x_{t|t})- B_t(u_t - u)|0,L_tQ_tL_t^T)}{\left|\det L_t\right|}</script>

<p>And apply it once more, to obtain our final result:</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \mathcal{N}(x_{t+1}|f(\hat x_{t|t}, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u),L_tQ_tL_t^T).</script>

<p>We are done! The linearization of our function has lead us back to Gaussianity!</p>

<p>With the same strategy, we obtain for our observation model</p>

<script type="math/tex; mode=display">p(y_t|x_t) = \mathcal{N}(y_T|h(\hat x_{t|t-1}, 0) + C_t(x_t - \hat x_{t|t-1}),M_tQ_tM_t^T),</script>

<p>with \(C_t = \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\) and \(M_t = \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\)</p>

<p>Our linearization led to a Gaussian system and observation model. Therefore, the distribution of the updated state estimate</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) := \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t})</script>

<p>and the distribution of the predicted state estimate</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t-1},u_{0:t-1}) := \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})</script>

<p>will be Gaussians as well.</p>

<p>Now we are ready to plug our surrogate into the equations of the Bayes filter:</p>
<div class="important_box">
  <p><strong>Prediction step</strong></p>

  <script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) + A_t(x_t - \hat x_{t|t})+ B_t(u_t - u),L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.</script>

  <p><strong>Update step</strong></p>

  <script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(x, 0) C_t(x_t - \hat x_{t|t-1}) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(x, 0) C_t(x_t - \hat x_{t|t-1}) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}</script>

</div>

<p>Let’s try to simplify these equations!</p>

<h3 id="prediction-step">Prediction step</h3>

<p>We will start with the prediction step</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}\left(x_{t+1}\middle| f(x, u, 0) - A_t\hat x_{t|t} - B_tu + A_tx_t + B_tu_t,L_t^TQ_tL_t\right) \mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.</script>

<p>To find an expression for our prediction step we can simply use the <em>propagation</em> formula (Formula 37, <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf">Toussaint</a>)</p>

<script type="math/tex; mode=display">\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ).</script>

<p>By comparison with our expression, we see that</p>

<script type="math/tex; mode=display">\hat x_{t+1|t} = f(x_{t|t}, u_t, 0) -A_t \hat x_{t|t} - B_tu_t + A_t \hat x_{t|t} + B_tu_t = f(x_{t|t}, u_t, 0)</script>

<script type="math/tex; mode=display">P_{t+1|t} = L_t^TQ_tL_t + A_t P_{t|t} A_t^T  .</script>

<h3 id="update-step">Update step</h3>

<p>We will start to simplify the update step</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t\hat x_{t|t-1} + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t\hat x_{t|t-1} + C_tx_t ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}</script>

<p>by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf">Toussaint</a>)</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) . %]]></script>

<p>Then again, this joint distribution can be rewritten as</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D & F\\F^T & E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) . %]]></script>

<p>We can combine the two previous equations to the following expression</p>

<script type="math/tex; mode=display">\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .</script>

<p>By comparison with the numerator of our update step, we obtain</p>

<script type="math/tex; mode=display">\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1},M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0) + C_t x_{t|t-1} -C_t\hat x_{t|t-1}),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}),</script>

<p>which simplifies to</p>

<script type="math/tex; mode=display">\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).</script>

<p>We applied the same trick as in the <a href="/jekyll/update/2018/10/10/kalman_filter.html">derivation of the Kalman filter</a>: Conceptually, we only transformed</p>

<script type="math/tex; mode=display">\frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}.</script>

<p>If we look closely at the final expression, we see that \(p(y)\) is canceling out. Therefore, the result is simply the remaining part</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)),  P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).</script>

<p>If our reasoning is correct the denominator should be equal to \(\mathcal{N}(y_{t}|h(x, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)\), which was canceled out. The denominator can be simplified with the <em>propagation</em> formula (Formula 37, <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf">Toussaint</a>)</p>

<script type="math/tex; mode=display">\int_{x_{t}}\mathcal{N}\left(y_{t}\middle| h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t x_{t|t-1} ,M_t^TR_tM_t\right) \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|h(\hat x_{t|t-1}, 0) - C_t x_{t|t-1} + C_t\hat x_{t|t-1}, M_t^TR_tM_t + C_tP_{t|t-1}C_t^T ) = \mathcal{N}(y_{t}|h(\hat x_{t|t-1}, 0),M_t^TR_tM_t + C_tP_{t|t-1}C_t^T).</script>

<p>Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.</p>

<p>Let’s summarize our results:</p>

<div class="important_box">
  <h1>Extended Kalman filter with non-additive noise</h1>

  <p>The recursive formula for the extended Kalman filter with non-additive noise consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} %]]></script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C_t^T(M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-h(\hat x_{t|t-1}, 0)) \\ 
P_{t|t} &= P_{t|t-1} - P_{t|t-1}C_t^T (M_t^TR_tM_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}  \end{align} %]]></script>

  <p>with</p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
A_t &= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
L_t &= \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
M_t &= \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}.
\end{align} %]]></script>

</div>
<p>That’s it! We derived the equations of the extended Kalman filter. To bring the equations in a more implementation friendly form, we are restating the extended Kalman filter as:</p>

<div class="important_box">
  <h1>Extended Kalman filter with non-additive noise</h1>

  <p>The recursive formula for the extended Kalman filter with non-additive noise consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t, 0) \\ 
P_{t+1|t} &= L_t^TQ_tL_t + A_t P_{t|t} A_t^T   \end{align} %]]></script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
z_t &= y_{t}-h(\hat x_{t|t-1}, 0)\\
S_t &= M_t^TR_tM_t + C_tP_{t|t-1}C_t^T\\
K_t &= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &= (I - K_tC_t)P_{t|t-1}
\end{align} %]]></script>

  <p>with</p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
A_t &= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
L_t &= \nabla_{w_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
M_t &= \nabla_{v_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}.
\end{align} %]]></script>

</div>

<p>As promised we will also look at the special case with <strong>additive</strong> noise. Therefore, our functions will look like:</p>

<script type="math/tex; mode=display">f(x_t, u_t, w_t) = \bar{f}(x_t, u_t) + w_t.</script>

<script type="math/tex; mode=display">h(x_t, v_t) = \bar{h}(x_t) + v_t.</script>

<p>In this case, the matrix \(L_t\) and \(M_t\) will be identity matrices:</p>

<script type="math/tex; mode=display">L_t = \nabla_{w_t} f(x_t, u_t, w_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{w_t} \bar{f}(x_t, u_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{w_t} w_t}_{I}|_{x_t=x,v_t=0} = I</script>

<script type="math/tex; mode=display">M_t = \nabla_{v_t} h(x_t, v_t)|_{x_t=x,v_t=0} = \underbrace{\nabla_{v_t} \bar{h}(x_t)}_{0}|_{x_t=x,v_t=0} + \underbrace{\nabla_{v_t} v_t}_{I}|_{x_t=x,v_t=0} = I.</script>

<p>Finally, we can state the equations of the extended Kalman filter with additive noise.</p>

<div class="important_box">
  <h1>Extended Kalman filter with additive noise</h1>

  <p>The recursive formula for the extended Kalman filter with additive noise consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &= f(x_{t|t}, u_t) \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T   \end{align} %]]></script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
z_t &= y_{t}-h(\hat x_{t|t-1})\\
S_t &= R_t + C_tP_{t|t-1}C_t^T\\
K_t &= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &= (I - K_tC_t)P_{t|t-1}
\end{align} %]]></script>

  <p>with</p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
A_t &= \nabla_{x_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
B_t &= \nabla_{u_t} f(x_t, u_t, w_t)^T|_ {x_t=\hat x_{t|t},u_t=u,w_t=0}\\
C_t &= \nabla_{x_t} h(x_t, v_t)^T|_ {x_t=\hat x_{t|t-1},v_t=0}\\
\end{align} %]]></script>

</div>

<h2 id="example">Example</h2>

<p>Enough of the dry theory! Let’s play around with the grid-based filter in our race track example.</p>

<svg id="race_track_mar_loc" style="width:100%" onclick="on_click()"></svg>
<script>


	n_scene = load_race_track("race_track_mar_loc","https://martinseilair.github.io");
	n_scene.mode = 2;
	n_scene.filter = "";
	n_scene.dur=slow_dur;
	// define particle filter 

	n_scene.auto_start = false;

	n_scene.t = 1;
	n_scene.take_observation = true;
	n_scene.ids = ["race_track_mar_loc_likelihood", "race_track_mar_loc_update","race_track_mar_loc_timestep", "race_track_mar_loc_predict" ];

	n_scene.loaded = function(){

		var outer_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style("background-color")), d3.rgb('#006eff'), d3.rgb('#00028e')]);
		var inner_color = d3.piecewise(d3.interpolateRgb, [d3.rgb(this.rt.svg.style("background-color")), d3.rgb('#ff834d'), d3.rgb('#8e3323')]);

		this.ekf = init_ekf_1D(this.rc, this.rc.state, 3*this.rc.sigma_s_no_cache(this.rc.state));
		this.rt.init_strip("inner", get_output_dist_normalized(this.rc, this.rt, this.rc.state), inner_color, 60);
		this.rt.init_strip("outer", get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt), outer_color, 60);



		document.getElementById("race_track_mar_loc_likelihood").style.display="block";
		this.rt.hide_strip("inner");


		this.restart = function(){
			for (var i=0; i<this.ids.length;i++){

				document.getElementById(this.ids[i]).style.display="none";
			}
			document.getElementById("race_track_mar_loc_likelihood").style.display="block";
			this.rc.reset();
			this.t = 1;

			

			//this.bf.reset();
			this.ekf.reset(this.rc.state, this.rc.sigma_o_no_cache(this.rc.state))
			this.rt.hide_strip("inner");
			this.rt.show_strip("outer");
			this.rt.update_strip("outer", get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));
			this.rt.treeg.style("opacity",1.0)
			this.take_observation = true;
		}


		this.rt.set_restart_button(this.restart.bind(this))

		this.toogle_observation = function(){
			if(this.take_observation){
				this.rt.treeg.style("opacity",0.2)
				this.take_observation = false;
				if(this.t% 5 ==1){
					document.getElementById("race_track_mar_loc_likelihood").style.display="none";
					document.getElementById("race_track_mar_loc_timestep").style.display="block";
					this.t = 3;
				}
			}else{
				this.rt.treeg.style("opacity",1.0)
				this.take_observation = true;
			}
        	
        }

		this.rt.tree_click = this.toogle_observation.bind(this)


	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;
		for (var i=0; i<this.ids.length;i++){

			document.getElementById(this.ids[i]).style.display="none";
		}


		if(this.t % 4 == 0){
			//CHOOSE ACTION
			this.rc.step(this.rc.current_input);
			this.last_input = this.rc.current_input;
			document.getElementById("race_track_mar_loc_predict").style.display="block";
			this.rt.hide_strip("inner");
		}else if(this.t % 4 == 1){
			// PREDICT
			this.ekf.predict(this.last_input);

			// trim ekf posterior
			if(this.ekf.posterior_mu<0){
				this.ekf.posterior_mu+=this.rt.track_length;
			}
			this.ekf.posterior_mu = this.ekf.posterior_mu % this.rt.track_length;
			this.rt.update_strip("outer", get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));
			if(this.take_observation){
				document.getElementById("race_track_mar_loc_likelihood").style.display="block";
			}else{
				document.getElementById("race_track_mar_loc_timestep").style.display="block";
				this.t=3;
			}
		}else if(this.t % 4 == 2){
			// OBSERVE
			this.rt.show_strip("inner");
			this.output = scene.rc.output_dist_sample(0);
			var likelihood = this.ekf.get_likelihood(this.output,this.ekf.posterior_mu)
			this.rt.update_strip("inner", get_gaussian_circ_normalized(this.rt.strip_pos, likelihood.mu, likelihood.sigma, this.rt));

			document.getElementById("race_track_mar_loc_update").style.display="block";
		}else if(this.t % 4 == 3){
			// UPDATE

			this.ekf.update(this.output);

			this.rt.update_strip("outer", get_gaussian_circ_normalized(this.rt.strip_pos, this.ekf.posterior_mu, this.ekf.posterior_sigma, this.rt));

			document.getElementById("race_track_mar_loc_timestep").style.display="block";
		}

	}.bind(n_scene);

	scenes_name["race_track_mar_loc"] = n_scene;
	scenes.push(n_scene);

</script>

<div id="race_track_mar_loc_timestep" class="button_set">
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();">Backward</div>
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();">No action</div>
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();">Forward</div>
 <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_predict" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_mar_loc'].step();">Predict step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_likelihood" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_mar_loc'].step();">Observe</div>
  <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_update" class="button_set" onclick="scenes_name['race_track_mar_loc'].step();">
<div class="bt1  bt">Update step</div>
  <span class="stretch"></span>
</div>

<p>On the outside of the race track, you will notice a blue colored strip. This strip represents our current posterior of the current position of the race car. We will start with a Gaussian prior distribution around the true mean. By pressing the <strong>OBSERVE</strong> button two things will happen: first, we will take a measurement of the distance to the tree and second, we will display the likelihood for this observed distance on the brown strip inside the race track. By pressing the <strong>UPDATE STEP</strong> button, we will perform our update step and show the resulting posterior at the outer strip. Now we are ready for the next time step. Take an action, by pressing the corresponding button below the race track. After the step is performed, you have to update your posterior by pressing the <strong>PREDICT STEP</strong> button. You will see that the outer strip will change accordingly. Now we finished one full cycle of the filtering process and are ready to start a new cycle by taking a measurement.</p>

<p>What if our distance meter is not working anymore? By either clicking on the tree or pressing the <strong>W</strong> button on your keyboard, you can turn off your measurement device. Therefore, the observation and update step will be skipped. The tree will become opaque, if your measurement device is turned off.</p>

<p>If you want to reset the environment, just press the reset button in the bottom left corner or press the <strong>R</strong> button on your keyboard.
As before you can control the car by using your keyboard: <strong>A</strong> (Backward), <strong>S</strong> (Stop),  <strong>D</strong> (Forward) or the buttons below the race track.</p>

<p>We see, that it is actually working pretty well. But one thing seems particularly weird: At certain positions on the race track, the brownish inner strip (our likelihood) seems to be uniformly distributed. This is not a bug, but a shortcoming of the linearization. We will always experience this behavior, if the part of the road at our current posterior mean is pointing only in <em>tangential</em> direction, with the tree as the center. In other words: A small change in position wouldn’t change the distance. When we are linearizing, we assume this local behavior applies for the whole system and we won’t get any new information out of our measurement. To get an intuitive understanding of this, you can imagine two parallel lines. Our car is driving along one of the parallel lines and we take the nearest distance to the other line as a measurement. This measurement would be uninformative because the distance to the parallel line is always the same.</p>

<p>Are you curious about the derivation of unscented Kalman filter? Then you should definitely check out the next article of the nonlinear filter series covering the <a href="/jekyll/update/2018/11/07/nf-ukf.html">unscented Kalman filter</a>. See you there!</p>

<h1 id="acknowledgement">Acknowledgement</h1>

<p>The vector graphics of the <a href="https://www.freepik.com/free-photos-vectors/car">car</a> were created by <a href="https://www.freepik.com/">Freepik</a>.</p>

<p><a href="https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm"></a></p>

<div id="rad_to_s" style="width:100px"></div>
<div id="div1"></div>
<div id="div2"></div>
<!-- <div id="system_dist_approx"  style="width: 600px; height: 600px;"></div> -->
<!--<div id="output_dist_approx"  style="width: 600px; height: 600px;"></div>-->
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG"></script>

<script type="text/x-mathjax-config">

var mq = window.matchMedia( "(max-width: 570px)" );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  "HTML-CSS": { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

</script>


  </div><div id="disqus_thread"></div>
<script>

(function() { // DON'T EDIT BELOW THIS LINE
var d = document, s = d.createElement('script');
s.src = 'https://ikigai-1.disqus.com/embed.js';
s.setAttribute('data-timestamp', +new Date());
(d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript><a class="u-url" href="/jekyll/update/2018/10/31/nf-ekf.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
