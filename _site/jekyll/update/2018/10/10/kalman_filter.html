<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Derivation of the Kalman filter | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Derivation of the Kalman filter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The concept and the equations of the Kalman filter can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using Bayesian inference. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory." />
<meta property="og:description" content="The concept and the equations of the Kalman filter can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using Bayesian inference. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory." />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-10T19:35:07+09:00" />
<script type="application/ld+json">
{"description":"The concept and the equations of the Kalman filter can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using Bayesian inference. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html","headline":"Derivation of the Kalman filter","dateModified":"2018-10-10T19:35:07+09:00","datePublished":"2018-10-10T19:35:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/10/10/kalman_filter.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1541397713378653663"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ikigai" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Derivation of the Kalman filter</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-10T19:35:07+09:00" itemprop="datePublished">Oct 10, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>The concept and the equations of the <a href="https://en.wikipedia.org/wiki/Kalman_filter">Kalman filter</a> can be quite confusing at the beginning. Often the assumptions are not stated clearly and the equations are just falling from the sky. This post is an attempt to derive the equations of the Kalman filter in a systematic and hopefully understandable way using <a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a>. It addresses everyone, who wants to get a deeper understanding of the Kalman filter and is equipped with basic knowledge of linear algebra and probability theory.
<!--more--></p>

<script type="text/javascript">
    function draw_ssm(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t<T;t++){


      	if (t<T-1) input_ns.push({title: "\\( u_" + t + " \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y , fill:"#e3e5feff"});
        state_ns.push({title: "\\( x_" + t +" \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y + dist_y, fill:"#FFFFFF"});
        output_ns.push({title: "\\( y_" + t + " \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});


        edges.push({source: state_ns[t], target: output_ns[t], dash:""})
        if (t>0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:""})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:""})
        }
      }

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

  	nodes = input_ns.concat(state_ns).concat(output_ns);
    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_ind(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 120;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 4;

      for (var t = 0; t<T;t++){


      	ind = "t"

      	if (t<T-1) ind+=(t-T+1)


      	if (t<T-1) input_ns.push({title: "\\( u_{" + ind + "}\\)", type: "prob", x: margin_x + dist_x*t , y: margin_y , fill:"#e3e5feff"});

      	statefill = (t==T-1) ? "#e3e5feff" :statefill = "#FFFFFF";
        state_ns.push({title: "\\( x_{" + ind + "} \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: "\\( y_{" + ind + "} \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});



        edges.push({source: state_ns[t], target: output_ns[t], dash:""})
        if (t>0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:""})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:""})
        }
      }


    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}

    edges.push({source: bstate_h, target: state_ns[0], dash:"5,5"})
    edges.push({source: binput_h, target: state_ns[0], dash:"5,5"})


  	nodes = input_ns.concat(state_ns).concat(output_ns);

  	var svg_w = margin_x + dist_x*(T-1) + 50;
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

    function draw_ssm_indi(svg){

      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 100;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t<T;t++){


      	ind = "t"

      	if (t<2) ind+=(t-2)
      	if (t>2) ind+="+"+(t-2)


      	input_ns.push({title: "\\( u_{" + ind + "}\\)", type: "prob", x: margin_x + dist_x*t , y: margin_y , fill:"#e3e5feff"});

      	statefill = (t==2) ? "#e3e5feff" :statefill = "#FFFFFF";
        state_ns.push({title: "\\( x_{" + ind + "} \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y + dist_y, fill:statefill});
        output_ns.push({title: "\\( y_{" + ind + "} \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});



        edges.push({source: state_ns[t], target: output_ns[t], dash:""})
        if (t>0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:""})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:""})
        }
      }

    estate_h = {x: state_ns[T-1].x + radius*4, y: state_ns[T-1].y}
    bstate_h = {x: state_ns[0].x - radius*4, y: state_ns[0].y}
    einput_h = {x: input_ns[T-1].x + 2*Math.sqrt(2)*radius, y: input_ns[T-1].y+ 2*Math.sqrt(2)*radius}
    binput_h = {x: state_ns[0].x - 2*Math.sqrt(2)*radius, y: state_ns[0].y- 2*Math.sqrt(2)*radius}


    edges.push({source: state_ns[T-1], target: estate_h, dash:"5,5"})
    edges.push({source: bstate_h, target: state_ns[0], dash:"5,5"})
    edges.push({source: input_ns[T-1], target: einput_h, dash:"5,5"})
    edges.push({source: binput_h, target: state_ns[0], dash:"5,5"})


  	nodes = input_ns.concat(state_ns).concat(output_ns);
    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);
    }

	function draw_ssm_obs(svg){


      var radius = 30;
      var dist_x = 120;
      var dist_y = 120;
      var margin_x = 50;
      var margin_y = 50;
      var markersize = 10;

      var input_ns = [];
      var state_ns = [];
      var output_ns = [];
      var edges = [];
      var T = 5;

      for (var t = 0; t<T;t++){


      	if (t<T-1) input_ns.push({title: "\\( u_" + t + " \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y , fill:"#e3e5feff"});
        state_ns.push({title: "\\( x_" + t +" \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y + dist_y, fill:"#FFFFFF"});

        if (t==0||t==4) output_ns.push({title: "\\( y_" + t + " \\)", type: "prob", x: margin_x + dist_x*t , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});
        if (t==2) {
        	output_ns.push({title: "\\( z_" + t + " \\)", type: "prob", x: margin_x + dist_x*(t+0.5) , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});
			output_ns.push({title: "\\( y_" + t + " \\)", type: "prob", x: margin_x + dist_x*(t-0.5) , y: margin_y+ 2*dist_y, fill:"#e3e5feff"});
        }
        



        if (t>0) {
        	edges.push({source: state_ns[t-1], target: state_ns[t], dash:""})
        	edges.push({source: input_ns[t-1], target: state_ns[t], dash:""})
        }
      }

      edges.push({source: state_ns[0], target: output_ns[0], dash:""})
      edges.push({source: state_ns[2], target: output_ns[1], dash:""})
      edges.push({source: state_ns[2], target: output_ns[2], dash:""})
      edges.push({source: state_ns[4], target: output_ns[3], dash:""})

  	nodes = input_ns.concat(state_ns).concat(output_ns);

    var svg_w = 2*margin_x + dist_x*(T-1);
    var svg_h = 2*margin_y + 2*dist_y;

    create_graph(d3.select(svg), nodes, edges, radius, markersize, svg_w, svg_h);

    }
 </script>

<script src="//d3js.org/d3.v3.js" charset="utf-8"></script>

<p>First of all, let’s try to formulate the main idea of Kalman filtering in one sentence:</p>

<div class="important_box">
  <p>The <strong>Kalman filter</strong> is used to <strong>infer</strong> the current state of a <strong>linear Gaussian state space model</strong> given all observations and inputs up to the current timestep and a Gaussian prior distribution of the initial state.</p>
</div>

<p>Indeed, the process of Kalman filtering is simply Bayesian inference in the domain of linear Gaussian state space models. The information encoded in our formulation is sufficient to uniquely define what the Kalman filter should output. But it doesn’t tell us anything about how to compute it. In this article, we will find an efficient recursive method that will lead us to the familiar equations.</p>

<p>Let’s get started with the derivation by defining <em>linear Gaussian state space models</em> and <em>Bayesian inference</em>.</p>

<h2 id="linear-gaussian-state-space-models">Linear Gaussian state space models</h2>

<p>A linear Gaussian state space model is defined by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}x_{t+1} &= A_tx_t + B_t u_t + w_t \\ 
y_t &= C_tx_t + v_t \end{align} %]]></script>

<p>with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A_t\), input matrix \(B_t\), output matrix \(C_t\), Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).</p>

<p>Alternatively, we can use the probability density functions</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}x_{t+1} &\sim \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\\ 
 y_t &\sim \mathcal{N}(y_t|C_tx_t, R_t)\end{align} %]]></script>

<p>to describe the system in a more compact fashion.</p>

<p>Linear Gaussian state space models can also be described in the language of <a href="https://en.wikipedia.org/wiki/Graphical_model">probabilistic graphical models</a> (or more precisely in the language of <a href="https://en.wikipedia.org/wiki/Bayesian_network">Bayesian networks</a>). The figure below shows such a model up to time \(T = 4\).</p>

<svg class="pgm_centered" onload="draw_ssm(this);"></svg>

<p>Every node represents a random variable and the edges are representing conditional dependencies between the respective nodes. Random variables, that are observed (or given) are shaded in light blue. In our case this is the output \(y_t\) and the input \(u_t\). The state \(x_t\) is not observed (or latent).</p>

<h2 id="bayesian-inference">Bayesian inference</h2>

<p>In simplest terms Bayesian inference tries to update a hypothesis/belief of something, that is not directly observable, in the face of new information by using <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a>. A bit more formal: The goal is to update the prior distribution \(p(x)\) given new data \(\mathcal{D}\) to obtain the posterior distribution \(p(x|\mathcal{D})\) with help of Bayes rule</p>

<script type="math/tex; mode=display">p(x|\mathcal{D}) = \frac{p(\mathcal{D}|x)p(x)}{p(\mathcal{D})}</script>

<p>with likelihood \(p(\mathcal{D}|x)\) and evidence \(p(\mathcal{D})\).</p>

<p>This idea is very general and can be applied to dynamical models quite easily. The most common inference tasks in dynamical models are filtering, smoothing and prediction. These methods differ only in the form of the posterior distribution.</p>

<ul>
  <li><strong>Filtering</strong>: What is my belief about the <strong>current state</strong> \(x_t\) given all observations and inputs?</li>
</ul>

<div class="big_eq"> 	$$ p(x_t|y_0,...,y_t,u_0,...,u_{t-1})  $$ </div>

<ul>
  <li><strong>Smoothing</strong>: What is my belief about <strong>all states</strong> \(x_t, … ,x_0\) given all observations and inputs?</li>
</ul>

<div class="big_eq"> $$ p(x_t,...,x_0|y_0,...,y_t,u_0,...,u_{t-1}) $$ </div>

<ul>
  <li><strong>Prediction</strong>: What is my belief about the <strong>next state</strong> \(x_{t+1}\) given all observations and inputs?</li>
</ul>

<div class="big_eq"> $$ p(x_{t+1}|y_0,...,y_t,u_0,...,u_{t-1}) $$ </div>

<p>The name Kalman <em>filter</em> reveals, that we will be interested in the filtering problem. Therefore, we want to infer the current state \(x_t\) based on all recent observations \(y_0,…,y_t\) and inputs \(u_0,…,u_{t-1}\).
Now that we have defined what we are looking for, let’s try to find a way to efficiently calculate it. We will start by finding a recursive method for <em>general</em> dynamical models defined by the probabilistic graphical model above.</p>

<h2 id="bayes-filter-for-state-space-models">Bayes filter for state space models</h2>

<p>We have the task to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \). For this purpose only the structure of the graphical model will matter: it governs the conditional dependencies.
To unclutter the notation we will use \(\Box_{n:m}\) for \(\Box_n,…,\Box_m\).</p>

<p>With help of <strong>Bayes’ rule</strong> we can rewrite the formula as</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t,y_{0:t-1},u_{0:t-1})p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.</script>

<div class="extra_box">
  <p>If you are not very familiar with Bayes’ rule this can be quite confusing. There are much more moving parts than in the very simple definition. Nonetheless, there is an intuitive explanation.
It is Bayes’ rule applied in a world, where we already observed \(\mathcal{W}\) in the past (every term is conditioned on \(\mathcal{W}\)):</p>

  <script type="math/tex; mode=display">p(x|\mathcal{D},\mathcal{W}) = \frac{p(\mathcal{D}|x,\mathcal{W})p(x|\mathcal{W})}{p(\mathcal{D}|\mathcal{W})}.</script>

  <p>In our case \(x:=x_t \), \(\mathcal{D}:=y_t \) and \(\mathcal{W}:=(y_{0:t-1},u_{0:t-1}) \).</p>

</div>
<p>We note that \(y_t\) is independent of \(y_{0:t-1}\) and  \(u_{0:t-1}\) given \(x_t\). It follows</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})}.</script>

<div class="extra_box">
  <p>This conditional independence property is not obvious as well. When it comes to conditional dependencies, it is always a good idea to look at the graphical model.</p>

  <svg class="pgm_centered" onload="draw_ssm_ind(this);"></svg>
  <p>In the figure above we notice that the node \(x_t\) is shaded (observed). This node blocks the way of \(y_{0:t-1}\) and  \(u_{0:t-1}\) to \(y_t\). We have proven the conditional independence <em>visually</em>. You can learn more about conditional independence in probabilistic graphical models in <a href="https://www.microsoft.com/en-us/research/people/cmbishop/#!prml-book">Pattern Recognition and Machine Learning</a> (Chapter 8.2).</p>
</div>
<p>The denominator is simply the integral of the numerator</p>

<script type="math/tex; mode=display">p(y_t|y_{0:t-1},u_{0:t-1}) = \int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1}) dx_t .</script>

<p>Great! We successfully expressed our equation in simpler terms. In return, we obtained the new expression \(p(x_t|y_{0:t-1},u_{0:t-1})\), which we have to calculate as well. Using marginalization we can express it as</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t,x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.</script>

<p>We can split the expression in the integral with product rule, which leads to</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1},y_{0:t-1},u_{0:t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-1}) dx_{t-1}.</script>

<p>Note that \(x_t\) is independent of \(y_{0:t-1}\) and \(u_{0:t-2}\) given \(x_{t-1}\). Furthermore, \(x_{t-1}\) is independent of \(u_{t-1}\). We obtain</p>

<script type="math/tex; mode=display">p(x_t|y_{0:t-1},u_{0:t-1}) = \int_{x_{t-1}} p(x_t|x_{t-1}, u_{t-1})p(x_{t-1}|y_{0:t-1},u_{0:t-2}) dx_{t-1}.</script>

<p>We note that \(p(x_{t-1}|y_{0:t-1},u_{0:t-2})\) has the same form as our expression we started from only shifted by one time step. Our recursive formula is complete!</p>

<p>Let’s summarize our results!</p>

<div class="important_box">
  <h1>Bayes filter for state space models</h1>

  <p>The recursive formula for the Bayes filter in state space models consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .</script>

  <p>The recursion is started with the prior distribution over the initial state \(p(x_0)\).</p>

</div>

<p>Up to this point, we assumed that we obtain exactly one observation at every timestep. This rather limiting assumption is violated in many real-life scenarios. Multiple or even no observations per timestep are possible. This behavior is exemplified in the probabilistic graphical model below.</p>

<svg class="pgm_centered" onload="draw_ssm_obs(this);"></svg>

<p>Fortunately, handling these cases is very simple. For every observation we make, we calculate the update step with the newest estimate available. Furthermore, it is not necessary that the observations are coming from the same output function (illustrated by the outputs \(y_2\) and \(z_2\) at \(t=2\)). <a href="https://en.wikipedia.org/wiki/Information_integration">Information integration/fusion</a> is very natural in Bayesian inference.</p>

<p>Nice! We just derived the equations of the Bayes filter for general state space models!
Now let’s translate this into the linear state space scenario.</p>

<h2 id="bayes-filter-in-linear-gaussian-state-space-models">Bayes filter in linear Gaussian state space models</h2>

<p>Let’s start by identifying the probability distributions we already know:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &= \mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t) \\
p(y_t|x_t) &=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align} %]]></script>

<p>Furthermore, we assume that the prior distribution of the initial state is Gaussian as well. All probability distributions in our model are Gaussian. Therefore, the distributions \(p(x_t|y_{0:t-1},u_{0:t-1})\) and \(p(x_t|y_{0:t},u_{0:t-1})\) will also be in form of Gaussian distributions, because our recursive formula is only using marginalization and Bayes’ rule, which are closed under Gaussian distributions. In the context of Kalman filtering, these are normally defined by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x_t|y_{0:t},u_{0:t-1}) &:= \mathcal{N}(x_{t}|\hat x_{t|t}, P_{t|t}) \\
p(x_t|y_{0:t-1},u_{0:t-1}) &:= \mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) .
 \end{align} %]]></script>

<p>Please note, that these distributions are still implicitly dependent on the inputs and outputs. The mean and the covariance are a <em>sufficient statistic</em> of the in- and outputs.</p>

<p>The index \(\Box_{n|m}\) of the parameters indicates that the state at time \(n\) is estimated, based on the outputs upto time \(m\).
The expression \(\hat x_{t|t}\) is called the <em>updated</em> state estimate and \( P_{t|t}\) the <em>updated</em> error covariance. Moreover, \(\hat x_{t|t-1}\) is called the <em>predicted</em> state estimate and \( P_{t|t-1}\) the <em>predicted</em> error covariance.</p>

<p>In summary, these are the equations for the Bayes filter in linear Gaussian state space models:</p>
<div class="important_box">

  <p><strong>Prediction step</strong></p>

  <script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.</script>

  <p><strong>Update step</strong></p>

  <script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}</script>

</div>

<p>Let’s try to simplify these equations!</p>

<h3 id="prediction-step">Prediction step</h3>

<p>We will start with the prediction step</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|\hat x_{t+1|t}, P_{t+1|t})  = \int_{x_t}\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, Q_t)\mathcal{N}(x_t|\hat x_{t|t}, P_{t|t}) dx_t.</script>

<p>In order to find a closed form solution of this integral, we could simply plug in the corresponding expressions of the Gaussian distributions and solve the integral. Fortunately, Marc Toussaint already gathered the most important <a href="https://ipvs.informatik.uni-stuttgart.de/mlr/marc/notes/gaussians.pdf">Gaussian identities</a>, which will lighten our workload a lot.  To find an expression for our prediction step we can simply use the <em>propagation</em> formula (Formula 37, Toussaint)</p>

<script type="math/tex; mode=display">\int_{y}\mathcal{N}(x|a + Fy, A)\mathcal{N}(y|b,B) dx_t = \mathcal{N}(x|a + Fb, A + FBF^T ).</script>

<p>By comparison with our expression, we see that</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T.
\end{align} %]]></script>

<h3 id="update-step">Update step</h3>

<p>We will start to simplify the update step</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \frac{\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})}{\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t}}</script>

<p>by focussing on the numerator first. We notice that we can rewrite it as a joint distribution (Formula 39, Toussaint)</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) . %]]></script>

<p>Then again, this joint distribution can be rewritten as</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}d\\e \end{matrix},\begin{matrix}D & F\\F^T & E\end{matrix}\right) = \mathcal{N}(y|e,E)\mathcal{N}(x|d + F^TE^{-1}(y-e),D - F^T E^{-1}F) . %]]></script>

<p>We can combine the two previous equations to the following expression</p>

<script type="math/tex; mode=display">\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}(y|b + Fa,B + FA^TF^T) \mathcal{N}(x|a + A^TF^T(B + FA^TF^T)^{-1}(y-b -Fa),A - A^TF^T (B + FA^TF^T)^{-1}FA) .</script>

<p>By comparison with the numerator of our update step, we obtain</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1})\mathcal{N}(y_{t}|C_tx_{t}, R_t ) = \mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)  \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}),  P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).</script>

<p>At a first glance, this is not looking like a simplification at all. Conceptually, we only transformed</p>

<script type="math/tex; mode=display">\frac{p(y|x)p(x)}{p(y)} \to \frac{p(y,x)}{p(y)} \to \frac{p(x|y)p(y)}{p(y)}.</script>

<p>If we look closely at the final expression, we see that \(p(y)\) is canceling out. Therefore, the result is simply the remaining part</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t}|\hat x_{t|{t}}, P_{t|t} ) = \mathcal{N}(x_{t}|\hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}),P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}).</script>

<p>If our reasoning is correct the denominator should be equal to \(\mathcal{N}(y_{t}|C_t\hat x_{t|t},R_t + C_tP_{t|t-1}C_t^T)\), which was canceled out. The denominator can be simplified with the <em>propagation</em> formula (Formula 37, Toussaint)</p>

<script type="math/tex; mode=display">\int_{x_{t}}\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) dx_{t} =  \mathcal{N}({y_{t}}|C_t\hat x_{t|t-1}, R_t + C_tP_{t|t-1}C_t^T ).</script>

<p>Yay! We see, that the denominator is exactly the same as the canceled factor in the numerator.</p>

<p>Let’s summarize our results:</p>

<div class="important_box">
  <h1>Bayes filter in linear Gaussian state space models</h1>

  <p>The recursive formula for the Bayes filter in linear Gaussian state space models consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T  \end{align} %]]></script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}) \\ 
P_{t|t} &= P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}.  \end{align} %]]></script>

</div>
<p>That’s it! We derived the equations of the Bayes filter in linear Gaussian state space models, which is nothing else but the good old Kalman filter.
In the next section, we will split these equations up to finally obtain the formulation normally used for the Kalman filter.</p>

<h2 id="kalman-filter">Kalman filter</h2>

<p>In order to obtain the familiar equations of the Kalman filter we have to define</p>

<ul>
  <li>
    <p><strong>Innovation</strong></p>

    <script type="math/tex; mode=display">z_t = y_{t}-C_t\hat x_{t|t-1}</script>
  </li>
  <li>
    <p><strong>Innovation covariance</strong></p>

    <script type="math/tex; mode=display">S_t = R_t + C_tP_{t|t-1}C_t^T</script>
  </li>
  <li>
    <p><strong>Optimal Kalman gain</strong></p>

    <script type="math/tex; mode=display">K_t = P_{t|t-1}C_t^TS_t^{-1}</script>
  </li>
</ul>

<div class="extra_box">

  <p><strong>What is the meaning of \(z_t\) and \(S_t\)?</strong></p>

  <p>The denominator of the update step is</p>

  <script type="math/tex; mode=display">\mathcal{N}(y_{t}|C_t\hat x_{t|t-1},R_t + C_tP_{t|t-1}^TC_t^T)</script>

  <p>and can be transformed by (Formula 34, Toussaint)</p>

  <script type="math/tex; mode=display">\mathcal{N}(x|a,A) = \mathcal{N}(x+f|a+f,A)</script>

  <p>to obtain the expression</p>

  <script type="math/tex; mode=display">\mathcal{N}(y_{t} - C_t\hat x_{t|t-1}|0,R_t + C_tP_{t|t-1}^TC_t^T) = \mathcal{N}(z_t|0,S_t).</script>

  <p>Therefore, the innovation \(z_t\) is the deviation of the expected output and the observed output.
The random variable \(z_t\) has a Gaussian distribution with zero mean and variance \(S_t\).</p>
</div>

<p>Let’s plug these definitions into the equations of our update step</p>

<script type="math/tex; mode=display">\hat x_{t|t} = \hat x_{t|t-1} + \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}(\underbrace{y_{t}-C_t\hat x_{t|t-1}}_{z_t})</script>

<script type="math/tex; mode=display">P_{t|t} = P_{t|t-1} - \underbrace{P_{t|t-1}C_t^T(\underbrace{R_t + C_tP_{t|t-1}C_t^T}_{S_t})^{-1}}_{K_t}P_{t|t-1} .</script>

<p>This leads us to the final equations of the Kalman filter.</p>

<div class="important_box">
  <h1>Equations of the Kalman filter</h1>

  <p>The recursive formula for the Kalman filter consists of the <strong>prediction step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T \end{align} %]]></script>

  <p>and the <strong>update step</strong></p>

  <script type="math/tex; mode=display">% <![CDATA[
\begin{align}
z_t &= y_{t}-C_t\hat x_{t|t-1}\\
S_t &= R_t + C_tP_{t|t-1}C_t^T\\
K_t &= P_{t|t-1}C_t^TS_t^{-1} \\
\hat x_{t|t} &= \hat x_{t|t-1} + K_t z_t\\
P_{t|t} &= (I - K_tC_t)P_{t|t-1}.
\end{align} %]]></script>

</div>

<h2 id="summary">Summary</h2>

<p>This article presented the derivation of the Kalman filter from first principles using Bayesian inference. The goal was to derive the Kalman filter in a clear and straightforward fashion. The steps were designed to be as atomic as possible, in order to be comprehensible for readers, who are not so familiar with the tools we used. Summarized, the derivation was performed in the following four subsequent steps:</p>

<ol>
  <li>We realized, that we have to calculate \( p(x_{t}|y_0,…,y_t,u_0,…,u_{t-1}) \).</li>
  <li>Derived the recursive equations of the Bayes filter to efficiently calculate this distribution.</li>
  <li>Inserted the corresponding distributions of the linear Gaussian state space model.</li>
  <li>Added some “sugar” to obtain the usual equations of the Kalman filter.</li>
</ol>

<script src="http://localhost:4000/assets/js/d3_graphical_model.js"></script>

<script type="text/javascript" src="http://localhost:4000/assets/js/svg_mathjax.js"></script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG"></script>

<script type="text/x-mathjax-config">

var mq = window.matchMedia( "(max-width: 570px)" );
if (!mq.matches) {
    MathJax.Hub.Config({
	  CommonHTML: { linebreaks: { automatic: true } },
	  "HTML-CSS": { linebreaks: { automatic: true } },
	         SVG: { linebreaks: { automatic: true } }
	}); 
} 

</script>

<script type="text/javascript">new Svg_MathJax().install();</script>


  </div><a class="u-url" href="/jekyll/update/2018/10/10/kalman_filter.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
