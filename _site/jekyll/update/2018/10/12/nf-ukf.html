<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Nonlinear filtering: Unscented Kalman filter | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Nonlinear filtering: Unscented Kalman filter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Intro" />
<meta property="og:description" content="Intro" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/10/12/nf-ukf.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/10/12/nf-ukf.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-12T18:04:07+09:00" />
<script type="application/ld+json">
{"description":"Intro","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/10/12/nf-ukf.html","headline":"Nonlinear filtering: Unscented Kalman filter","dateModified":"2018-10-12T18:04:07+09:00","datePublished":"2018-10-12T18:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/10/12/nf-ukf.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1540973150664162091"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ikigai" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Nonlinear filtering: Unscented Kalman filter</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-12T18:04:07+09:00" itemprop="datePublished">Oct 12, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Intro
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script></p>

<script src="http://localhost:4000/assets/js/nonlinear_filter/particle_filter.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/race_car.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/race_track.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/util.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/plot.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/scene.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />

<link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/nonlinear_filter/style.css" />

<script type="text/javascript">


// mit keys oder button steuerbar
// strips ein und ausblendbar
// weights ein und ausblendbar
// update resample predict manuell oder langsam automatisch (weiter button)
// update resample predict button (hier macht input keinen sinn, außer man hat 3 button für predict)
// mit maus car position festlegen (geringster abstand)


// herangehensweise

// 1. auto fährt 
// 2. Vorstellung der system und beobachtungsfunktion (plot)
// 3. mit maus car position festlegen, entsprechende verteilung innen und außen anzeigen
// 3a. Bayes filter approximierung außen posterior innen beobachtung (update prediction weiter)
// 4. standbild: particle anzeigen
// 5. standbild: update step (5 sek vorher 5 sek nachher) (prob strip innen anzeigen)
// 6. standbild: resampling (5 sek vorher 5 sek nachher)
// 7. standbild: predict (5 sek vorher 5 sek nachher)
// 8. update resample predict manuell (weiter button)
// 9. update resample predict automatisch (geschwindigkeit einstellbar) (steuerung über pfeiltasten)
// 10. zwei trees

	// SITE NOT LOADED!!!

	// input modes
	// 0: Automatisch langsam; sequential
	// 1: Set input per  A = backward, S = no movement, D = forward; one step
	// 2: Set input per  A = backward, S = no movement, D = forward; sequential
	// 3: Mouse exploring
	// 4: No input

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>

<h2 id="unscented-kalman-filter">Unscented Kalman filter</h2>

<p>The unscented Kalman filter is another approximation of the Bayes filter with Gaussian belief states.</p>

<p>When we are doing the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>with</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &= \mathcal{N}(x_{t+1}|f(x_t, u_t), Q_t) \\
p(y_t|x_t) &=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align} %]]></script>

<p>with a Gaussian belief distribution xxx we will receive a non-Gaussian function as a result. The first idea of the unscented Kalman filter is, that we want a Gaussian posterior in the end. In other words we are only intrested in the mean and variance of the true posterior. We try to approximate the real posterior with a Gaussian. In this scenario, we have two problems:</p>

<ol>
  <li>The integral is intractable, because of the non-linear sytem dynamic \(f(x_t,u_t)\).</li>
  <li>It is intractable to calculate the mean and covariance of the resulting posterior</li>
</ol>

<p>How can we solve this problem?</p>

<p>One solution is sampling. We could simply obtain the empricial distribution by sampling from our Gaussian. Then we replace our belief with the emprical distribution in the predict step. The integral will become a sum. Please be aware, that the resulting function will still be a continuous function. Nonetheless, we know that this function is a mixture of gaussians, where each component is a Gaussian with mean \(f(\xi^i_t,u_t)\) and variance \(Q_t\). When we are forming our empirical distrobution our samples are weighted by \(w^i  = \frac{1}{N}\).</p>

<p>By assuming an arbitray weighting of the samples the resulting mean is</p>

<script type="math/tex; mode=display">\hat{\mu} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}[x_{t+1}]</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i p(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \sum_{i=1}^N w^i\int_{x_{t+1}}  w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \sum_{i=1}^N w^i f(\xi^i_t, u_t)</script>

<p>and the variance</p>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[(x_{t+1}-\hat{\mu})(x_{t+1}-\hat{\mu})^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T - x_{t+1}\hat{\mu}^T - \hat{\mu}x_{t+1}^T + \hat{\mu}\hat{\mu}^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}\hat{\mu}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}x_{t+1}^T\right] + \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}\hat{\mu}^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T - \hat{\mu}\hat{\mu}^T + \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^ip(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = Q_t + \sum_{i=1}^N w^i f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T - \hat{\mu}\hat{\mu}^T</script>

<p>Because of this we can find the true mean and variance of the continuous posterior. In the limit with inifinitley many samples this procedure will procduce the correct mean and variance.</p>

<p>Great, we have a nice method! But there is another problem. Sampling can be very inefficient. Maybe there is a smarter way.</p>

<p>The second big idea of the unscented Kalman filter is, that we are replacing the our belief distribution with a surrogate distribution, which is distributoin containing weighted samples.</p>

<p>We choose this surrogate distribution in such a way, that it has the same mean and variance as the Gaussian belief distribution. This distribution is not unique.</p>

<p>For our samples we use</p>

<script type="math/tex; mode=display">\xi_t^0 = \mu_t</script>

<script type="math/tex; mode=display">\xi_t^i = \mu_t + \sqrt{n+\lambda}L i=1,..,n</script>

<script type="math/tex; mode=display">\xi_t^i = \mu_t - \sqrt{n+\lambda}L i=n+1, ..., 2n</script>

<p>with weights</p>

<script type="math/tex; mode=display">w_c^0 = \frac{\lambda}{n + \lambda}</script>

<script type="math/tex; mode=display">w_m^0 = \frac{\lambda}{n + \lambda} + (1-\alpha^2+\beta)</script>

<script type="math/tex; mode=display">w_m^i =   \frac{\lambda}{2(n + \lambda)}</script>

<p>Why do they choose another Wc0?????????</p>

<script type="math/tex; mode=display">D_{KL}(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})||\mathcal{N}(x_{t+1}|\hat{\mu},\hat{\Sigma}))</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .</script>

<h2 id="unscented-kalman-filter-1">Unscented Kalman filter</h2>

<p>The unscented Kalman filter was born by intuition. It seems that it is not derived from the general Bayes filter but from the Kalman filter</p>

<p>Bayes filter -&gt; Linear Gaussian models -&gt; Kalman filter</p>

<p>Kalman filter -&gt; Non-linear models -&gt; Unscented Kalman filter</p>

<p>In this post I will try to derive the UKF from first principles. There is one catch, to obtain the true UKF you have to do some very nasty approximations, will incorporate in the derivation afterwards.</p>

<p>The UKF is based around the unscented transform. If you want to transform a Gaussian distribution</p>

<script type="math/tex; mode=display">p(x) = \mathcal{N}(y|\mu, \Sigma)</script>

<p>thorugh a nonlinear function, it will give you an estimate of the resulting mean and covariance.</p>

<p>For developing the equations I would state the UT in another form. The nonlinear equations is in our case a Gaussian distribution with constant variance and nonlinear mean:</p>

<script type="math/tex; mode=display">p(y|x) = \mathcal{N}(y|f(x),B)</script>

<p>. Now, the big difference is that we dont want to estimate the distribution of the output \(p(x)\), but the distribution over the joint probability.</p>

<p>The first step is to replace the Gaussian distribution \( p(x) = \mathcal{N}(y|\mu, \Sigma) \) with a surrogate distribution. This surrogate distribution is a weighted dirac distribution.</p>

<p>For our samples we use</p>

<script type="math/tex; mode=display">\chi^0 = \mu</script>

<script type="math/tex; mode=display">\chi^i = \mu + a_iL_i  i=1,..,n</script>

<script type="math/tex; mode=display">\chi^i = \mu - a_iL_{i-n}  i=n+1, ..., 2n</script>

<p>With the property</p>

<script type="math/tex; mode=display">\mu= \sum_{i} w^i \chi^i</script>

<script type="math/tex; mode=display">\Sigma = \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T</script>

<p>where</p>

<script type="math/tex; mode=display">\sum_{i} w^i = 1</script>

<script type="math/tex; mode=display">a_i = a_{i+n} i=1,..,n</script>

<script type="math/tex; mode=display">w_i = w_{i+n} i=1,..,n</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
 \Sigma &= \sum_{i} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
&= w^0 (\chi^0-\mu)(\chi^0-\mu)^T + \sum_{i=1}^{n} w^i (\chi^i-\mu)(\chi^i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\chi^i-\mu)(\chi^i-\mu)^T \\
 &= w^0 (\mu-\mu)(\mu-\mu)^T + \sum_{i=1}^{n} w^i (\mu + a_iL_i-\mu)(\mu + a_iL_i-\mu)^T + \sum_{i=n+1}^{2n} w^i (\mu - a_iL_{i-n}-\mu)(\mu - a_iL_{i-n}-\mu)^T \\
&= \sum_{i=1}^{n} w^i (a_iL_i)(a_iL_i)^T + \sum_{i=n+1}^{2n} w^i (- a_iL_{i-n})(- a_iL_{i-n})^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=n+1}^{2n} w^i a_i^2 L_{i-n}L_{i-n}^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^{i+n} a_{i+n}^2 L_{i}L_{i}^T \\
&= \sum_{i=1}^{n} w^i a_i^2 L_iL_i^T + \sum_{i=1}^{n} w^i a_i^2 L_{i}L_{i}^T \\
&= \sum_{i=1}^{n} 2w^i a_i^2 L_iL_i^T \\
&\stackrel{!}{=} \sum_{i=1}^{n} L_iL_i^T = LL^T = \Sigma \\
\end{align} %]]></script>

<p>It follows that \(w^i = \frac{1}{2a_i^2} \) has to hold for \(i=1,..,n\).</p>

<p>From \(\sum_{i} w^i = 1\) follows that</p>

<script type="math/tex; mode=display">w^0  = 1 - \sum_{i=1}^{n}\frac{1}{2a_i^2}</script>

<p>Given the deviations \(a_i\) we can uniquely determine the corresponding weights.</p>

<p>Because of the symmetry of the points and the fact that the weights sum to 1, the first property is always fulfilled. You have to choose the weights based on \({a_i}_{1:2n}\) to fulfill the second property.</p>

<p>In total our surrogate distribution is defined as:</p>

<script type="math/tex; mode=display">\hat{p}(x) = \sum_{i} w^i \delta_{\chi^i}(x).</script>

<p>Now we can write down the joint probability based on \(\hat{p}(x)\) and  \(p(y|x)\)</p>

<script type="math/tex; mode=display">\hat{p}(x,y) = p(y|x)\hat{p}(x) = \mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x).</script>

<p>This joint probability will look like slices of the \(p(y|x) \).</p>

<p>Now comes the crucial part. Now we are calculating the mean and covariance of this surrogate joint probability. lets start with the mean</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{\mu} &= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy \\ 
&= \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \,dx\,dy \\ 
&= \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\begin{pmatrix}
    x  \\
    y \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy \\ 
&= \sum_{i} w^i \int\limits_{x}\mathcal{N}(y|f(\chi^i),B)\begin{pmatrix}
    \chi^i \\
    y \\
    \end{pmatrix} \,dy \\ 
&= \sum_{i} w^i \begin{pmatrix}
    \chi^i \\
    f(\chi^i) \\
    \end{pmatrix} =  \begin{pmatrix}
    \sum_{i} w^i \chi^i \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}\\
&=  \begin{pmatrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{pmatrix}. 
\end{align} %]]></script>

<p>Lets calculate the covariance</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
\hat{\Sigma} &= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    x-\hat{\mu}_x  \\
    y-\hat{\mu}_y \\
    \end{pmatrix}\begin{pmatrix}
    x-\mu  \\
    y-\hat{\mu} \\
    \end{pmatrix}^T \,dx\,dy \\ 
&= \iint\limits_{x\,y}\hat{p}(x,y) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy \\ 
&= \iint\limits_{x\,y}\mathcal{N}(y|f(x),B)\sum_{i} w^i \delta_{\chi^i}(x) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dx\,dy \\ 
&= \sum_{i} w^i \iint\limits_{x\,y}\mathcal{N}(y|f(x),B) \begin{pmatrix}
    (x-\hat{\mu}_x)(x-\hat{\mu}_x)^T & (x-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(x-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \delta_{\chi^i}(x) \,dx\,dy \\ 
&= \sum_{i} w^i \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T & (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T \\
    (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T & (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T \\
    \end{pmatrix} \,dy \\ 
&= \sum_{i} w^i \begin{pmatrix}
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T \,dy& \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (\chi^i-\hat{\mu}_x)(y-\hat{\mu}_y)^T\,dy\\
    \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T\,dy & \int\limits_{y}\mathcal{N}(y|f(\chi^i),B) (y-\hat{\mu}_y)(y-\hat{\mu}_y)^T\,dy \\
    \end{pmatrix}  \\ 

&= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& (\chi^i-\hat{\mu}_x)\left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right]^T\\
    \left[\int\limits_{y}\mathcal{N}(y|f(\chi^i),B)y\,dy -\hat{\mu}_y\right](\chi^i-\hat{\mu}_x)^T &  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&= \sum_{i} w^i \begin{pmatrix}
    (\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& (\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    (f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  (f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&=  \begin{pmatrix}
    \sum_{i} w^i(\chi^i-\hat{\mu}_x)(\chi^i-\hat{\mu}_x)^T& \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
&=  \begin{pmatrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{pmatrix}  \\ 
\end{align} %]]></script>

<p>In total we get for the estimated surrogate joint probability</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{p}(x,y) = \mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{matrix},\begin{matrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{matrix}\right) %]]></script>

<p>This is the result of this variant of the UT transform. Lets look how this goes into the Bayes filter and with the <strong>prediction step</strong>:</p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}.</script>

<p>This is equation is nothing else but marginalization</p>

<script type="math/tex; mode=display">p(y) = \int_x p(y|x)p(x)dx</script>

<p>we can also write it as</p>

<script type="math/tex; mode=display">p(y) = \int_x p(x,y)dx</script>

<p>and have identified our joint probability distribution, that we can approximate with our variant of the UT transform.</p>

<p>It is very important to notice, that in this case you dont have to calculate the whole joint Gaussian probability, because you are marginalizing afterwards.</p>

<script type="math/tex; mode=display">% <![CDATA[
\int\limits_{x}\mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    a  \\
    b \\
    \end{matrix}, \begin{matrix}
    A & C \\
    C^T & B\\
    \end{matrix} \right) \,dx = \mathcal{N}(y|b,B) %]]></script>

<p>Now lets look at the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t} p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})\,dx_t} .</script>

<p>We see that we have a joint probability in the numerator and a marginalization of the same joint probability in the denominator.</p>

<script type="math/tex; mode=display">p(y|x) = \frac{p(x,y)}{\int_{x}p(x,y)\,dx} .</script>

<p>We can calculate this analytically with our estimate of the surrogate function and obtain directly a Gaussian posterior.</p>

<p>We can calculate the joint probability for the Linear Gaussian case and then do a coefficient comparison. To obtain the corresponding filtering equations.</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(y_{t}|C_tx_{t}, R_t )\mathcal{N}(x_{t}|\hat x_{t|t-1}, P_{t|t-1}) = \mathcal{N}\left(\begin{matrix}x_t \\y_t\end{matrix}\middle|\begin{matrix}\hat x_{t|t-1}\\C_t\hat x_{t|t-1} \end{matrix},\begin{matrix}P_{t|t-1} & P_{t|t-1}^TC_t^T\\C_tP_{t|t-1} & R_t + C_tP_{t|t-1}^TC_t^T\end{matrix}\right) . %]]></script>

<p>and</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{p}(x,y) = \mathcal{N}\left(\begin{matrix}
    x  \\
    y \\
    \end{matrix}\middle|\begin{matrix}
    \mu \\
    \sum_{i} w^i f(\chi^i) \\
    \end{matrix},\begin{matrix}
    \Sigma & \sum_{i} w^i(\chi^i-\hat{\mu}_x)(f(\chi^i) -\hat{\mu}_y)^T\\
    \sum_{i} w^i(f(\chi^i) -\hat{\mu}_y)(\chi^i-\hat{\mu}_x)^T &  \sum_{i} w^i(f(\chi^i)-\hat{\mu}_y)(f(\chi^i)-\hat{\mu}_y)^T + B \\
    \end{matrix}\right) %]]></script>

<p>To obtain the known equations of the UKF you are choosing specific values \(a_i\) and when you calculate the update function you are cheating by using the sigma points from the last time step instead from the current timestep. But you still take the covariance of the current timestep for the calculation.</p>

<p><a href="https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm"></a></p>

<div id="rad_to_s" style="width:100px"></div>
<div id="div1"></div>
<div id="div2"></div>
<!-- <div id="system_dist_approx"  style="width: 600px; height: 600px;"></div> -->
<!--<div id="output_dist_approx"  style="width: 600px; height: 600px;"></div>-->


  </div><a class="u-url" href="/jekyll/update/2018/10/12/nf-ukf.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
