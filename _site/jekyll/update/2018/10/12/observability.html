<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Observability: A Bayesian perspective | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Observability: A Bayesian perspective" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!" />
<meta property="og:description" content="A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/10/12/observability.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/10/12/observability.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-12T18:04:07+09:00" />
<script type="application/ld+json">
{"description":"A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/10/12/observability.html","headline":"Observability: A Bayesian perspective","dateModified":"2018-10-12T18:04:07+09:00","datePublished":"2018-10-12T18:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/10/12/observability.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1541478288426615231"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ikigai" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Observability: A Bayesian perspective</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-12T18:04:07+09:00" itemprop="datePublished">Oct 12, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script></p>

<div class="important_box">
  Formally, a system is said to be observable if, for any possible sequence of state and control vectors (the latter being variables whose values one can choose), the current state (the values of the underlying dynamically evolving variables) can be determined in finite time using only the outputs.
</div>

<p>We can formulate this help of Bayesian inference:</p>

<div class="important_box">
  A system is said to be observable if, for any possible sequence of state and control vectors, the probability mass of the posterior of the current state will collapse into a single point in finite time using only the outputs.
</div>

<p>A deterministic linear Gaussian state space model is defined by</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}x_{t+1} &= A_tx_t + B_t u_t \\ 
y_t &= C_tx_t  \end{align} %]]></script>

<p>with state \(x_t\), output \(y_t\), input \(u_t\), system matrix \(A_t\), input matrix \(B_t\) and output matrix \(C_t\).</p>

<p>From the Kalman filter we know how to do inference in linear Gaussian state space models. The equation of the</p>

<p><strong>prediction step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T  \end{align} %]]></script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C_t^T(R_t + C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}) \\ 
P_{t|t} &= P_{t|t-1} - P_{t|t-1}C_t^T (R_t + C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1} \end{align} %]]></script>

<p>contain Gaussian process noise \( w_t \sim \mathcal{N}(w_t|0, Q_t) \) and Gaussian observation noise \( v_t \sim \mathcal{N}(v_t|0, R_t) \).</p>

<p>In a deterministic model, as the name suggests is no noise. Conceptually we set \(Q_t=0\) and \(R_t=0\). It is also quite easy to imagine, that the probability mass will be concentrated on a single value. But mathematically it gets problematic. We would have to enter the strange world of distributions/measure theory. For our purpose it is much safer to imagine, that \(Q_t\) and \(R_t\) will go to zero and we are interested in the limiting case.</p>

<p>From this perspective we can look again at the equations of the Kalman filter. The effect of taking the limit \(Q_t \to 0\) is not really harmful. We are simply transforming the metric defined by \(P_{t|t}\)</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &= Q_t + A_t P_{t|t} A_t^T  \end{align} %]]></script>

<p>In the case of the update step, we have to be more careful. First of all we can rewrite it as</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}y_{t} + (I - P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}C_t)\hat x_{t|t-1} \\ 
P_{t|t} &= (I - P_{t|t-1}C_t^T (C_tP_{t|t-1}C_t^T)^{-1}C_t)P_{t|t-1}.  \end{align} %]]></script>

<p>In the determinstic case we know, that \(y_t = C_tx_t\). If we plug this in we obtain</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{C_t}(x_{t}-\hat x_{t|t-1}) \\ 
P_{t|t} &= P_{t|t-1} - P_{C_t}P_{t|t-1},  \end{align} %]]></script>

<p>where \(P_{C_t} = P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}C_t\) is a orthogonal projection matrix with the inner product defined by \(P_{t|t-1}\).</p>

<p>For the sake of symmetry we could also write</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{C_t}(x_{t}-\hat x_{t|t-1}) \\ 
P_{t|t} &= P_{t|t-1} + P_{C_t}(0-P_{t|t-1}),  \end{align} %]]></script>

<p>because we know, that the true state itself has covariance zero. How can we interpret this? 
Lets think about an \(2\times 2\) matrix \(A_t\) and \(2\times 1\) matrix \(C_t\)
By observing \(y_t\) we gain new information. We know, that \(y_T\) was generated by</p>

<script type="math/tex; mode=display">y_t = C_tx_t.</script>

<p>Now we can ask the question which points will map to this particular \(y_t\). In our case these points will lay on a line. What have we done? Effectively we have inferred where our true state could be:
It has the on this line. We cut our full two dimensional Gaussian multivariate with and the only thing that remains is this slice, that we will be Gaussian again. Based on this new knowledge, how do we choose our new mean and variance?</p>

<p>Our eqauations above speak a clear language: We are performing an orthogonal projection from our current mean on the line of possible values. Intuitively, the new mean will be the closest to our old mean. What happens with the covariance? Essentially the same thing. If we imagine our covariance as two vectors, we can see that the vectors are also projected down to the line of possible \(x_t\).</p>

<p>If we would have no measurement noise, we would now have a singular covariance matrix. Therefore, it describes a degenerate multivariate Gaussian distribution.</p>

<p>An application of our prediction step would simply transform our degenerate distribution. It translates, scales and rotates it.</p>

<p>What happens, if we do an update step with a degenerate distribution? Intuitively, we are only transforming the valid subspace of the covariance matrix into the matrix. Now we are restricting the possible space not on the nullspace of our transformation, but also on the possible space.</p>

<script type="math/tex; mode=display">P_{t|t-1} - P_{C_t}P_{t|t-1}</script>

<script type="math/tex; mode=display">I - P_{C_0}</script>

<script type="math/tex; mode=display">AA^T - AP_{C_0}A^T</script>

<script type="math/tex; mode=display">AA^T - AP_{C_0}A^T - P_{C_1}(AA^T - AP_{C_0}A^T) = AA^T - AP_{C_0}A^T - P_{C_1}AA^T + P_{C_1}AP_{C_0}A^T</script>

<script type="math/tex; mode=display">A^2A^{2T} - A^2P_{C_0}A^{2T} - AP_{C_1}AA^{2T} + AP_{C_1}AP_{C_0}A^{2T}</script>

<script type="math/tex; mode=display">P_{t|t} = P_{t|t-1} - P_{t|t-1}C_t^T (C_tP_{t|t-1}C_t^T)^{-1}C_tP_{t|t-1}</script>

<script type="math/tex; mode=display">P_{t|t} = I - C_t^T (C_tC_t^T)^{-1}C_t</script>

<script type="math/tex; mode=display">P_{t|t} = AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T</script>

<script type="math/tex; mode=display">P_{t|t} = AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T - (AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T)C_t^T (C_t(AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T)C_t^T)^{-1}C_t(AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T)</script>

<script type="math/tex; mode=display">P_{t|t} = AA^T - A^TC_t^T (C_tC_t^T)^{-1}C_tA^T - (AA^TC_t^T  - A^TC_t^T (C_tC_t^T)^{-1}C_tA^TC_t^T )(C_tAA^TC_t^T - C_tA^TC_t^T (C_tC_t^T)^{-1}C_tA^TC_t^T)^{-1}(C_tAA^T - C_tA^TC_t^T (C_tC_t^T)^{-1}C_tA^T)</script>

<p>Furthermore, in order to perform Bayesian inference we need a prior distribution of the initial state \(\mathcal{N}(x_0|\mu, \Sigma)\).</p>

<p>We can describe our deterministic state space models in a probabilistic form using the Dirac delta function</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \delta(x_{t+1} - A_tx_t - B_t u_t)</script>

<script type="math/tex; mode=display">p(y_t|x_t) = \delta(x_{t+1} - C_tx_t).</script>

<p>This can also be formulated with the help of Gaussians distributions</p>

<script type="math/tex; mode=display">\mathcal{N}(x_{t+1}|A_tx_t + B_t u_t, \Sigma)</script>

<script type="math/tex; mode=display">\mathcal{N}(y_{t}|C_tx_t, \Sigma),</script>

<p>where \(\Sigma \to 0\).</p>

<p>The recursive formula for the Bayes filter in state space models consists of the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .</script>

<p>Let’s take a moment and think about which form our posterior distribution will have by performing the first prediction step starting with the prior \(p(x_0)\):</p>

<script type="math/tex; mode=display">p(x_{1}|u_{0}) = \int_{x_{0}} \delta(x_{1} - A_0x_0 - B_0 u_0)\mathcal{N}(x_0|\mu, \Sigma) dx_{0}.</script>

<p>We note, that we have a composed the Dirac delta function with a function \(\delta(g(x_0))\) with the function \(g(x_0) = x_{1} - A_0x_0 - B_0 u_0\).</p>

<p>Therefore, we can also write the integral as</p>

<script type="math/tex; mode=display">p(x_{1}|u_{0}) = \int_{g^{-1}(0)} \frac{\mathcal{N}(x_0|\mu, \Sigma)}{|\det(\nabla_{x_0} g(x)|_{x=x_0})|} dx_{0}.</script>

<p>The derivative \(\nabla_{x_0}g(x_0)\) is</p>

<script type="math/tex; mode=display">\nabla_{x_0}g(x_0) = \nabla_{x_0}\left[x_{1} - A_0x_0 - B_0 u_0\right] = -A_0.</script>

<p>Which elements are contained in the set \(g^{-1}(0) \)?</p>

<p>Linear algebra can help us with this regard.</p>

<script type="math/tex; mode=display">g(x_0) = x_{1} - A_0x_0 - B_0 u_0 = 0</script>

<script type="math/tex; mode=display">A_0x_0 = x_{1} - B_0 u_0</script>

<p>Which set of elements \(x\) satisfy the equation</p>

<script type="math/tex; mode=display">Ax = b,</script>

<p>with \(A=A_0\), \(x=x_0\) and \(b=x_{1} - B_0 u_0\)?</p>

<p>In general,</p>

<script type="math/tex; mode=display">x = x_p + x_h</script>

<p>will satisfy this eqaution, where \(x_p\) is a particular solution and \(x_h\) is coming from the space of the homogeneous solution</p>

<script type="math/tex; mode=display">Ax_h = 0.</script>

<p>All elements \(x_h\) form the null space of A.</p>

<p>If we assume, that A is invertibl. The nullspace will only contain the element \(x=0\).</p>

<script type="math/tex; mode=display">g^{-1}(0) = A_0^{-1}(x_1 - B_0u_0)</script>

<p>We can plug our results in the integral above</p>

<script type="math/tex; mode=display">p(x_{1}|u_{0}) = \frac{\mathcal{N}(A_0^{-1}(x_1 - B_0u_0)|\mu, \Sigma)}{|\det(A_0)|} .</script>

<script type="math/tex; mode=display">p(x_{1}|u_{0}) = \frac{\frac{1}{|\det(A_0)|}\mathcal{N}(x_1 - B_0u_0|A_0\mu, A_0^T\Sigma A_0)}{|\det(A_0)|}.</script>

<script type="math/tex; mode=display">p(x_{1}|u_{0}) = \mathcal{N}(x_1 |A_0\mu + B_0u_0, A_0^T\Sigma A_0).</script>

<h1 id="joint-probability-of-gaussian-with-conditional-gaussian">Joint probability of Gaussian with conditional Gaussian</h1>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) . %]]></script>

<script type="math/tex; mode=display">\frac{1}{\sqrt{(2\pi)^k \det(A)}}\exp(-\frac{1}{2}(x-a)^TA^{-1}(x-a))\frac{1}{\sqrt{(2\pi)^k \det(B)}}\exp(-\frac{1}{2}(y-(b + Fx)))^TB^{-1}(y-(b + Fx))))</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}(x-a)^TA^{-1}(x-a))\exp(-\frac{1}{2}(y-(b + Fx)))^TB^{-1}(y-(b + Fx))))</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}(x-a)^TA^{-1}(x-a) -\frac{1}{2}(y-(b + Fx)))^TB^{-1}(y-(b + Fx))))</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[x^TA^{-1}x - x^TA^{-1}a - a^TA^{-1}x + a^TA^{-1}a + y^TB^{-1}y - y^TB^{-1}(b + Fx) - (b + Fx)^TB^{-1}y + (b + Fx)^TB^{-1}(b + Fx) \right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[x^TA^{-1}x - x^TA^{-1}a - a^TA^{-1}x + a^TA^{-1}a + y^TB^{-1}y - y^TB^{-1}b- y^TB^{-1}Fx - b^TB^{-1}y- Fx^TB^{-1}y + b^TB^{-1}b+ b^TB^{-1}Fx+x^TF^TB^{-1}b+ x^TF^TB^{-1}Fx \right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
x^TA^{-1}x + x^TF^TB^{-1}Fx  
- x^TA^{-1}a - a^TA^{-1}x + a^TA^{-1}a + 
y^TB^{-1}y - y^TB^{-1}b - b^TB^{-1}y + b^TB^{-1}b
- y^TB^{-1}Fx - Fx^TB^{-1}y + b^TB^{-1}Fx  +  x^TF^TB^{-1}b\right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
x^T(A^{-1} + F^TB^{-1}F)x  - x^TA^{-1}a - a^TA^{-1}x + a^TA^{-1}a 
\underbrace{- x^TF^TB^{-1}Fa + x^TF^TB^{-1}Fa}_{0}
\underbrace{- a^TF^TB^{-1}Fx + a^TF^TB^{-1}Fx}_{0}
\underbrace{- a^TF^TB^{-1}a + a^TF^TB^{-1}Fa}_{0}
+ y^TB^{-1}y - y^TB^{-1}b - b^TB^{-1}y + b^TB^{-1}b
- y^TB^{-1}Fx - Fx^TB^{-1}y + b^TB^{-1}Fx  +  x^TF^TB^{-1}b
\underbrace{+ a^TF^TB^{-1}y - a^TF^TB^{-1}y}_{0}
\underbrace{+ y^TB^{-1}Fa - y^TB^{-1}Fa}_{0}
\underbrace{+ a^TF^TB^{-1}Fa - a^TF^TB^{-1}Fa}_{0}
\underbrace{+ b^TB^{-1}Fa - b^TB^{-1}Fa}_{0}
\underbrace{+ a^TF^TB^{-1}b - a^TF^TB^{-1}b}_{0}
\right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
x^T(A^{-1} + F^TB^{-1}F)x  - x^T(A^{-1} + F^TB^{-1}F)a - a^T(A^{-1} + F^TB^{-1}F)x + a^T(A^{-1} + F^TB^{-1}F)a 
+ x^TF^TB^{-1}Fa
+ a^TF^TB^{-1}Fx
- a^TF^TB^{-1}a
+ y^TB^{-1}y 
- y^TB^{-1}b - y^TB^{-1}Fa
- b^TB^{-1}y - a^TF^TB^{-1}y
+ b^TB^{-1}b + b^TB^{-1}Fa + a^TF^TB^{-1}b + a^TF^TB^{-1}Fa
- y^TB^{-1}Fx - x^TF^TB^{-1}y + b^TB^{-1}Fx  +  x^TF^TB^{-1}b
+a^TF^TB^{-1}y
+y^TB^{-1}Fa 
 - a^TF^TB^{-1}Fa
- b^TB^{-1}Fa 
- a^TF^TB^{-1}b 
\right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
(x-a)^T(A^{-1} + F^TB^{-1}F)(x-a)
+ x^TF^TB^{-1}Fa
+ a^TF^TB^{-1}Fx
- a^TF^TB^{-1}a
- y^TB^{-1}Fx - x^TF^TB^{-1}y + b^TB^{-1}Fx  +  x^TF^TB^{-1}b
+a^TF^TB^{-1}y
+y^TB^{-1}Fa 
 - a^TF^TB^{-1}Fa
- b^TB^{-1}Fa 
- a^TF^TB^{-1}b 
+ y^TB^{-1}y 
- y^TB^{-1}(b+Fa)
- (b+Fa)^TB^{-1}y 
+ (b+Fa)^TB^{-1}(b+Fa)
\right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
(x-a)^T(A^{-1} + F^TB^{-1}F)(x-a)

- x^TF^TB^{-1}y
+ x^TF^TB^{-1}Fa
+ x^TF^TB^{-1}b

+ a^TF^TB^{-1}y
- a^TF^TB^{-1}Fa
- a^TF^TB^{-1}b 

- y^TB^{-1}Fx  
+ a^TF^TB^{-1}Fx
+ b^TB^{-1}Fx  

+ y^TB^{-1}Fa 
- a^TF^TB^{-1}a
- b^TB^{-1}Fa 

+ (y - (b+Fa))^TB^{-1}(y - (b+Fa)) 
\right])</script>

<script type="math/tex; mode=display">\exp(-\frac{1}{2}\left[
(x-a)^T(A^{-1} + F^TB^{-1}F)(x-a)
- (x-a)^TF^TB^{-1}(y - (b+Fa)) 
- (y - (b+Fa))^TB^{-1}F(x-a)  
+ (y - (b+Fa))^TB^{-1}(y - (b+Fa)) 
\right])</script>

<script type="math/tex; mode=display">% <![CDATA[
\exp\left(\begin{pmatrix}x - a\\ y - (b + Fa) \end{pmatrix}^T\begin{pmatrix}A^{-1} + F^TB^{-1}F & -F^TB^{-1}\\-B^{-1}F & B^{-1}\end{pmatrix}\begin{pmatrix}x-a\\y-(b + Fa) \end{pmatrix}\right) %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\exp\left(\begin{pmatrix}x - a\\ y - (b + Fa) \end{pmatrix}^T\begin{pmatrix}A^{-1} + F^TB^{-1}F & -F^TB^{-1}\\-B^{-1}F & B^{-1}\end{pmatrix}\begin{pmatrix}x-a\\y-(b + Fa) \end{pmatrix}\right) %]]></script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix}
A^{-1} 	+ F^TB^{-1}F							& -F^TB^{-1}\\
-B^{-1}F 										& 	B^{-1}
\end{pmatrix}^{-1} 
= \begin{pmatrix}
C_1^{-1} 										& (A^{-1} + F^TB^{-1}F)^{-1}F^TB^{-1}C_2^{-1}\\
C_2^{-1}B^{-1}F(A^{-1}+ F^TB^{-1}F)^{-1} 		& C_2^{-1}
\end{pmatrix} %]]></script>

<script type="math/tex; mode=display">C_1 = A^{-1} + F^TB^{-1}F - F^TB^{-1}(B^{-1})^{-1}B^{-1}F = A^{-1}</script>

<script type="math/tex; mode=display">C_2 = B^{-1} - B^{-1}F(A^{-1} + F^TB^{-1}F)^{-1}F^TB^{-1} = (B + FAF^T )^{-1}</script>

<script type="math/tex; mode=display">(A^{-1} + F^TB^{-1}F)^{-1} = A -AF^T(B + FAF^T)^{-1}FA</script>

<script type="math/tex; mode=display">A_{21} = (B + FAF^T )B^{-1}F(A^{-1}+ F^TB^{-1}F)^{-1}</script>

<script type="math/tex; mode=display">A_{21} = (B + FAF^T )B^{-1}F(A -AF^T(B + FAF^T)^{-1}FA)</script>

<script type="math/tex; mode=display">A_{21} = (B + FAF^T )B^{-1}FA -(B + FAF^T )B^{-1}FAF^T(B + FAF^T)^{-1}FA</script>

<script type="math/tex; mode=display">A_{21} = \left[(B + FAF^T )B^{-1} -(B + FAF^T )B^{-1}FAF^T(B + FAF^T)^{-1}\right]FA</script>

<script type="math/tex; mode=display">A_{21} = \left[(B + FAF^T )B^{-1}\left\{I-FAF^T(B + FAF^T)^{-1}\right\} \right]FA</script>

<script type="math/tex; mode=display">A_{21} = (B + FAF^T )B^{-1}FA - BB^{-1}FAF^T(B + FAF^T)^{-1}FA + FAF^TB^{-1}FAF^T(B + FAF^T)^{-1}FA</script>

<script type="math/tex; mode=display">A_{21} = \left[(B + FAF^T )B^{-1} - FAF^T(B + FAF^T)^{-1} + FAF^TB^{-1}FAF^T(B + FAF^T)^{-1}\right]FA</script>

<script type="math/tex; mode=display">% <![CDATA[
\begin{pmatrix}
A^{+} + F^TB^{+}F 								& -F^TB^{+}\\
-B^{+}F 										& B^{+}
\end{pmatrix}^{+} 
= \begin{pmatrix}
C_1^{+} 										& (A^{+} + F^TB^{+}F)^{+} F^TB^{+}C_2^{+}\\
C_2^{+}B^{+}F(A^{+} + F^TB^{+}F)^{+} 			& C_2^{+}
\end{pmatrix} %]]></script>

<script type="math/tex; mode=display">C_1 = A^{+} + F^TB^{+}F  - F^TB^{+}BB^{+}F = A^{+} + F^TB^{+}F  - F^TB^{+}F =  A^{+}</script>

<script type="math/tex; mode=display">C_2 = B^{+} - B^{+}F(A^{+} + F^TB^{+}F)^{+}F^TB^{+} = (B + FAF^T )^{+}</script>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) \propto \exp\left(\begin{pmatrix}x - a\\ y - (b + Fa) \end{pmatrix}^T\begin{pmatrix}A & A^TF^T\\FA & B + FA^TF^T\end{pmatrix}^{-1}\begin{pmatrix}x-a\\y-(b + Fa) \end{pmatrix}\right) %]]></script>

<h1 id="joint-probability-of-gaussian-with-invertible-linear-function">Joint probability of Gaussian with invertible linear function</h1>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,0) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA &  FA^TF^T\end{matrix}\right) . %]]></script>

<h1 id="joint-probability-of-gaussian-with-degenerate-conditional-gaussian">Joint probability of Gaussian with degenerate conditional Gaussian</h1>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,B) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA & B + FA^TF^T\end{matrix}\right) . %]]></script>

<h1 id="joint-probability-of-gaussian-with-degenerate-linear-function">Joint probability of Gaussian with degenerate linear function</h1>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{N}(x|a,A)\mathcal{N}(y|b + Fx,0) = \mathcal{N}\left(\begin{matrix}x \\y\end{matrix}\middle|\begin{matrix}a\\b + Fa \end{matrix},\begin{matrix}A & A^TF^T\\FA &  FA^TF^T\end{matrix}\right) . %]]></script>

<h1 id="joint-probability-of-degenerate-gaussian-with-conditional-gaussian">Joint probability of Degenerate Gaussian with conditional Gaussian</h1>

<h1 id="joint-probability-of-degenerate-gaussian-with-invertible-linear-function">Joint probability of Degenerate Gaussian with invertible linear function</h1>

<h1 id="joint-probability-of-degenerate-gaussian-with-degenerate-conditional-gaussian">Joint probability of Degenerate Gaussian with degenerate conditional Gaussian</h1>

<h1 id="joint-probability-of-degenerate-gaussian-with-degenerate-linear-function">Joint probability of Degenerate Gaussian with degenerate linear function</h1>

<p><strong>prediction step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t+1|t} &=  A_t \hat x_{t|t} + B_tu_t \\ 
P_{t+1|t} &= A_t P_{t|t} A_t^T  \end{align} %]]></script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}\hat x_{t|t} &= \hat x_{t|t-1} + P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}(y_{t}-C_t\hat x_{t|t-1}) \\ 
P_{t|t} &= (I - P_{t|t-1}C_t^T (C_tP_{t|t-1}C_t^T)^{-1}C_t)P_{t|t-1}.  \end{align} %]]></script>

<p>Oblique projection</p>

<script type="math/tex; mode=display">P = A(B^TA)^-1B^T</script>

<script type="math/tex; mode=display">P = P_{t|t-1}C_t^T (C_tP_{t|t-1}C_t^T)^{-1}C_t</script>

<script type="math/tex; mode=display">A = P_{t|t-1}C_t^T</script>

<script type="math/tex; mode=display">B = C_t^T</script>

<p>Ortogonal projection with inner product space</p>

<script type="math/tex; mode=display">P = A(A^TDA)^{-1}A^TD</script>

<script type="math/tex; mode=display">P = P_{t|t-1}C_t^T (C_tP_{t|t-1}C_t^T)^{-1}C_t</script>

<script type="math/tex; mode=display">P = DA(A^TDA)^{-1}A^T</script>

<script type="math/tex; mode=display">D = P_{t|t-1}</script>

<script type="math/tex; mode=display">A = C_t^T</script>

<script type="math/tex; mode=display">\hat x_{t|t} = P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}y_{t} + (I - P_{t|t-1}C_t^T(C_tP_{t|t-1}C_t^T)^{-1}C_t)\hat x_{t|t-1}</script>


  </div><a class="u-url" href="/jekyll/update/2018/10/12/observability.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
