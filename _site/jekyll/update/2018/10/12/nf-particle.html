<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.5.0 -->
<title>Nonlinear filtering: Particle filter | Ikigai</title>
<meta name="generator" content="Jekyll v3.8.4" />
<meta property="og:title" content="Nonlinear filtering: Particle filter" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!" />
<meta property="og:description" content="A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!" />
<link rel="canonical" href="http://localhost:4000/jekyll/update/2018/10/12/nf-particle.html" />
<meta property="og:url" content="http://localhost:4000/jekyll/update/2018/10/12/nf-particle.html" />
<meta property="og:site_name" content="Ikigai" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2018-10-12T18:04:07+09:00" />
<script type="application/ld+json">
{"description":"A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!","@type":"BlogPosting","url":"http://localhost:4000/jekyll/update/2018/10/12/nf-particle.html","headline":"Nonlinear filtering: Particle filter","dateModified":"2018-10-12T18:04:07+09:00","datePublished":"2018-10-12T18:04:07+09:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/jekyll/update/2018/10/12/nf-particle.html"},"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css?1540373693911030744"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Ikigai" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Ikigai</a><a class="git-header" href="https://github.com/martinseilair"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">martinseilair</span></a></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Nonlinear filtering: Particle filter</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2018-10-12T18:04:07+09:00" itemprop="datePublished">Oct 12, 2018
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>A particle filter is a very helpful tool for tracking dynamic systems. This article is meant to be an introduction to particle filters with a strong focus on visual examples. In the course of this post we will think about the main idea of the particle filter, derive the corresponding algorithm and play around with examples on the way. In order to follow the steps in this post you should bring some basic knowledge of math, probability theory in particular. In the derivations and explanations, I tried to take as small steps as possible, to keep everyone on board. Let’s dive into it!
<!--more-->
<script src="https://d3js.org/d3.v5.min.js" charset="utf-8"></script>
<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_SVG"></script>
  <script src="https://cdn.plot.ly/plotly-latest.min.js"></script></p>

<script src="http://localhost:4000/assets/js/nonlinear_filter/particle_filter.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/race_car.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/race_track.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/util.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/plot.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/scene.js"></script>

<script src="http://localhost:4000/assets/js/nonlinear_filter/discrete_bayes_filter.js"></script>

<link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet" />

<link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/nonlinear_filter/style.css" />

<script type="text/javascript">


// mit keys oder button steuerbar
// strips ein und ausblendbar
// weights ein und ausblendbar
// update resample predict manuell oder langsam automatisch (weiter button)
// update resample predict button (hier macht input keinen sinn, außer man hat 3 button für predict)
// mit maus car position festlegen (geringster abstand)


// herangehensweise

// 1. auto fährt 
// 2. Vorstellung der system und beobachtungsfunktion (plot)
// 3. mit maus car position festlegen, entsprechende verteilung innen und außen anzeigen
// 3a. Bayes filter approximierung außen posterior innen beobachtung (update prediction weiter)
// 4. standbild: particle anzeigen
// 5. standbild: update step (5 sek vorher 5 sek nachher) (prob strip innen anzeigen)
// 6. standbild: resampling (5 sek vorher 5 sek nachher)
// 7. standbild: predict (5 sek vorher 5 sek nachher)
// 8. update resample predict manuell (weiter button)
// 9. update resample predict automatisch (geschwindigkeit einstellbar) (steuerung über pfeiltasten)
// 10. zwei trees

	// SITE NOT LOADED!!!

	// input modes
	// 0: Automatisch langsam; sequential
	// 1: Set input per  A = backward, S = no movement, D = forward; one step
	// 2: Set input per  A = backward, S = no movement, D = forward; sequential
	// 3: Mouse exploring
	// 4: No input

	// scene_flags

	scene = [];
	scenes = [];
	scenes_name = [];
	interval = null;
	loaded = false;
	var aa = 1;
	var fast_dur = 300;
	var slow_dur = 1000;
	var ani_step = 3;


	touch_id = null;





</script>

<div class="important_box">
  <p>The <strong>particle filter</strong> is a sample-based approximation of the Bayes filter. It is used to <strong>infer</strong> the current state of an arbitrary probabilistic state space model given all observations and inputs up to the current timestep and a prior distribution of the initial state.</p>
</div>

<h2 id="race-track">Race track</h2>

<p>In this section we will introduce our running example. The dynamic model is a car, that is driving on a race track. It has the discrete actions of moving forward, backward and standing still. The tree inside the race track serve as a natural landmark. We have a distance measurement device on board, which will give us a noisy observation of the trees distance to the car. After the introduction of the concepts of the Bayes filter, we will formulate the model of the car and of measurement device in a rigourous way.</p>

<svg id="race_track_intro" style="width:100%" onclick="ani()"></svg>

<script>
	// defines scenes
	n_scene = load_race_track("race_track_intro","http://localhost:4000");
	n_scene.mode = 0;
	n_scene.filter = null;
	n_scene.dur=fast_dur;
	n_scene.auto_start = true;

	n_scene.step= function(){
		this.rc.step(this.rc.current_input);
	}.bind(n_scene);

	scenes.push(n_scene);
</script>

<h2 id="bayes-filter">Bayes filter</h2>

<p><a href="/jekyll/update/2018/10/10/kalman_filter.html">Derivation of the Kalman filter</a></p>

<p>Now that we have a better understanding what Bayes filtering is. To demonstrate the process of Bayes filtering, let’s go back to our example and try to visualize it. First of all and most importantly, the process of Bayes filtering requires to solve integrals, that are in general intractable. Otherwise we we wouldn’t need approximations such as the particle filter. For the purpose of visualization we can discretize the probability distribution of model and output to compute the integrals numerically. This grid-based method is called Markov localization in the context of localization. Like all grid-based methods is in volatile to the curse of dimensionality.</p>

<p>But now focus again on our example. Let us first define the distribution of the model and obsrvations. In our case, we are lucky, because we know exactly the governing models of the simulation. In a real-life scenario, you would have to find a model. Either by empirical data or by engineering methods.</p>

<h1 id="system-dynamic">System dynamic</h1>

<p>The state \(x_t\) of the system consists only of the current position on the race track. In a more realistic setting you would also include the velocity of the car. We avoid this to simplify our example and for the sake of visualisation.</p>

<p>To obtain the position of the car in \((x,y)\) coordinates you have the mapping</p>

<script type="math/tex; mode=display">L(x_t) =  \begin{pmatrix}
    L_x(x_t) \\
    L_y(x_t) \\
    \end{pmatrix}</script>

<p>For the sake of simplicity our system dynamics are defined by a Gaussian distribution</p>

<script type="math/tex; mode=display">p(x_{t+1}|x_t,u_t) = \mathcal{N}(x_{t+1}|\mu_s(x_t, u_t) ,\sigma_s^2(x_t) )</script>

<p>with non-linear mean  \(\mu_s(x_t, u_t)\) and variance \(\sigma_s^2(x_t)\). To obtain the mean of the next state \(x_{t+1}\) the input \(u_t\), which is weighted by \(b(\kappa)\) is simply added to the current state \(x_t\):</p>

<script type="math/tex; mode=display">\mu_s(x_t, u_t) = x_t + b(\kappa)u_t</script>

<p>The weighting factor</p>

<script type="math/tex; mode=display">b(\kappa) = e^{- c\kappa},</script>

<p>with hand-tweaked parameter \(c\) is used to model a more realistic driving behaviour that depends on the curvature of the track</p>

<script type="math/tex; mode=display">\kappa(x_t) ={\frac {|L_x'(x_t)L_y''(x_t)-L_y'(x_t)L_x''(x_t)|}{\left(L_x'^2(x_t)+L_y'^2(x_t)\right)^{\frac {3}{2}}}}.</script>

<p>at position \(x_t\).</p>

<p>If the curvature is low, we drive faster. If the curvature is high, we drive faster.</p>

<p>The variance</p>

<script type="math/tex; mode=display">\sigma_s(x_t, u_t) = db(\kappa)\left|u_t\right|</script>

<p>depends similarly on the input \(u_t\) and weighting factor \(b(\kappa)\) with hand-tweaked parameter \(d\).</p>

<svg id="race_track_sys_dist" style="width:100%" onclick="ani()"></svg>

<script>


	// defines scenes
	n_scene = load_race_track("race_track_sys_dist", "http://localhost:4000");
	n_scene.mode = 3;
	n_scene.me_show_system = true;
	n_scene.me_show_observation_transposed = false;
	n_scene.me_show_observation = false;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	
	scenes.push(n_scene);
</script>

<h1 id="observation-model">Observation model</h1>

<p>To infer the position of the car, we will measure the distance to tree. The position of the tree is defined as \(T = (T_x, T_y)\). Again we model the uncertainty of the measurement with a Gaussian distribution</p>

<script type="math/tex; mode=display">p(y_t|x_t) = \mathcal{N}(y_t| \mu_o(x_t), \sigma_o^2(x_t)).</script>

<p>The mean</p>

<script type="math/tex; mode=display">\mu_o(x_t) = d(L(x_t),T) = \sqrt{(L_x(x_t)-T_x)^2 + (L_y(x_t)-T_y)^2}</script>

<p>is defined as the exact distance to the tree.</p>

<p>The variance</p>

<script type="math/tex; mode=display">\sigma_o(x_t) = ad(L(x_t),T)</script>

<p>changes corresponding to the distance. The farer we are away from the tree, the more noise will be present in the signal.</p>

<h1 id="discrete-bayes-filter">Discrete Bayes filter</h1>

<p>Now that we know our model a bit better, let’s look at the Bayes filter again.</p>

<svg id="race_track_obs_dist" style="width:100%" onclick="ani()"></svg>

<script>

	// defines scenes
	n_scene = load_race_track("race_track_obs_dist", "http://localhost:4000");
	n_scene.mode = 3;
	n_scene.me_show_system = false;
	n_scene.me_show_observation_transposed = true;
	n_scene.me_show_observation = true;
	n_scene.filter = null;
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;
	// define particle filter 


	scenes.push(n_scene);
</script>

<script>


	n_scene = load_race_track("race_track_mar_loc","http://localhost:4000");
	n_scene.mode = 2;
	n_scene.filter = "bayes";
	n_scene.dur=slow_dur;
	// define particle filter 

	n_scene.auto_start = false;

	n_scene.t = 1;


	n_scene.loaded = function(){
		//var ids = ["race_track_mar_loc_likelihood", "race_track_mar_loc_update","race_track_mar_loc_timestep", "race_track_mar_loc_predict" ];
		//for (var i=0; i<ids.length;i++){

		//	document.getElementById(ids[i]).style.display="none";
		//}
		document.getElementById("race_track_mar_loc_likelihood").style.display="block";
		this.rt.hide_strip("inner");
	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;
	var ids = ["race_track_mar_loc_likelihood", "race_track_mar_loc_update","race_track_mar_loc_timestep", "race_track_mar_loc_predict" ];
		for (var i=0; i<ids.length;i++){

			document.getElementById(ids[i]).style.display="none";
		}
		//document.getElementById(ids[this.t%3]).style.display="block";


		if(this.t % 4 == 0){
			this.rc.step(this.rc.current_input);
			document.getElementById("race_track_mar_loc_predict").style.display="block";
		}else if(this.t % 4 == 1){
			this.bf.predict(this.rc.current_input);
			this.rt.hide_strip("inner");
			this.rt.update_strip("outer", normalize_vector(this.bf.posterior));
			document.getElementById("race_track_mar_loc_likelihood").style.display="block";
		}else if(this.t % 4 == 2){
			this.rt.show_strip("inner");
			this.rt.update_strip("inner", get_output_dist_normalized(this.rc, this.rt, this.rc.state));
			document.getElementById("race_track_mar_loc_update").style.display="block";
		}else if(this.t % 4 == 3){
			var output = scene.rc.output_dist_sample(0);
	    	var y = scene.bf.cont_2_disc_output(output);
			this.bf.update(y);
			this.rt.update_strip("outer", normalize_vector(this.bf.posterior));
			document.getElementById("race_track_mar_loc_timestep").style.display="block";
		}



	}.bind(n_scene);





	scenes_name["race_track_mar_loc"] = n_scene;
	scenes.push(n_scene);

</script>

<svg id="race_track_mar_loc" style="width:100%" onclick="ani()"></svg>

<div id="race_track_mar_loc_timestep" class="button_set">
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=0;scenes_name['race_track_mar_loc'].step();">Backward</div>
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=1;scenes_name['race_track_mar_loc'].step();">No action</div>
<div class="bt3 bt" onclick="scenes_name['race_track_mar_loc'].rc.current_input=2;scenes_name['race_track_mar_loc'].step();">Forward</div>
 <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_predict" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_mar_loc'].step();">Predict step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_likelihood" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_mar_loc'].step();">Show likelihood</div>
  <span class="stretch"></span>
</div>

<div id="race_track_mar_loc_update" class="button_set" onclick="scenes_name['race_track_mar_loc'].step();">
<div class="bt1  bt">Update step</div>
  <span class="stretch"></span>
</div>

<h2 id="example">Example</h2>

<p>From the statistical and probabilistic point of view, particle filters can be interpreted as mean field particle interpretations of Feynman-Kac probability measures.</p>

<p>To design a particle filter we simply need to assume that we can sample the transitions \(X_{k-1}\to X_{k} \) of the Markov chain \(X_{k}\) , and to compute the likelihood function \(x_{k}\mapsto p(y_{k}|x_{k})\)</p>

<ol>
  <li>
    <p>Sample \(N\) samples from \(p(x_0)\)</p>
  </li>
  <li>
    <p>Selection-updating transition</p>
  </li>
</ol>

<script type="math/tex; mode=display">\sum _{i=1}^{N}{\frac {p(y_{k}|\xi _{k}^{i})}{\sum _{j=1}^{N}p(y_{k}|\xi _{k}^{j})}}\delta _{\xi _{k}^{i}}(dx_{k})</script>

<ol>
  <li>mutation-prediction transition</li>
</ol>

<script type="math/tex; mode=display">{\widehat {\xi }} _ {k}^{i}\longrightarrow \xi _ {k+1}^{i}\sim p(x _ {k+1}\|{\widehat {\xi }} _ {k}^{i}) ,\qquad i=1,\cdots ,N.</script>

<ol>
  <li>Initialize the distribution. The initial distribution can be anything. In this demo, a uniform distribution over a predefined range is used.</li>
  <li>Observe the system and find a (proportional) probability for each particle that the particle is an accurate representation of the system based on that observation. We refer to this value as a particle’s importance weight. In this demo, the selected function calculates the particle weight directly from</li>
  <li>Normalize the particle weights.</li>
  <li>Resample the distribution to get a new distribution. A particle is selected at a frequency proportional to its importance weight.</li>
  <li>Add noise to the filter. Because a particle may be resampled multiple times, we need to move some of the particles slightly make the distribution cover the space better. Add small random values to each parameter of each filter.</li>
  <li>If you have a prediction on how the system changes between time steps, you can update each particle in the filter according to the prediction.</li>
  <li>Repeat from step 2.</li>
</ol>

<script type="math/tex; mode=display">p(x) = \sum_{j=1}^J w^{[j]}\delta_{x_[j]}(x)</script>

<script type="math/tex; mode=display">\mathcal(X) = \left\{  \left(  x^{[j]},w^{[j]}  \right) \right\}</script>

<p>Fall 1 ich kann von \(X_{k-1}\to X_{k} \) samplen
Fall 2 ich kann es nicht</p>

<script>

		// defines scenes
	n_scene = load_race_track("race_track_particle", "http://localhost:4000");
	n_scene.mode = 2;
	n_scene.filter = "particle";
	n_scene.dur=slow_dur;
	n_scene.auto_start = false;




	n_scene.t = 1;


	n_scene.loaded = function(){
		document.getElementById("race_track_particle_likelihood").style.display="block";
		this.rt.hide_strip("inner");
	}.bind(n_scene)


	n_scene.step = function(){
		this.t++;

		var ids = ["race_track_particle_timestep", "race_track_particle_likelihood", "race_track_particle_update","race_track_particle_predict","race_track_particle_resampling" ];
		for (var i=0; i<ids.length;i++){

			document.getElementById(ids[i]).style.display="none";
		}
		


		if(this.t % 5 == 0){
			this.rc.step(scene.rc.current_input);
			document.getElementById("race_track_particle_predict").style.display="block";
		}if(this.t % 5 == 1){
	    	
			//this.rt.update_strip("outer", get_system_dist_normalized(scene.rc, scene.rt, scene.rc.state, scene.rc.current_input));

			this.pf.predict(scene.rc.current_input);
			document.getElementById("race_track_particle_likelihood").style.display="block";
		}else if(this.t % 5 == 2){
			this.rt.show_strip("inner");
			this.rt.update_strip("inner", get_output_dist_normalized(scene.rc, scene.rt, scene.rc.state));
			document.getElementById("race_track_particle_update").style.display="block";
		}else if(this.t % 5 == 3){
			var output = this.rc.output_dist_sample(0);
	    	this.pf.update(output, 0);
	    	document.getElementById("race_track_particle_resampling").style.display="block";
		}else if(this.t % 5 == 4){
			this.rt.hide_strip("inner");
			this.pf.ancestor_sampling();
			document.getElementById("race_track_particle_timestep").style.display="block";
		}
	}.bind(n_scene);

	scenes_name["race_track_particle"] = n_scene;
	scenes.push(n_scene);
</script>

<svg id="race_track_particle" style="width:100%" onclick="ani()"></svg>

<div id="race_track_particle_timestep" class="button_set">
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=0;scenes_name['race_track_particle'].step();">Backward</div>
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=1;scenes_name['race_track_particle'].step();">No action</div>
<div class="bt3 bt" onclick="scenes_name['race_track_particle'].rc.current_input=2;scenes_name['race_track_particle'].step();">Forward</div>
 <span class="stretch"></span>
</div>

<div id="race_track_particle_predict" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Predict step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_likelihood" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Show likelihood</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_update" class="button_set">
<div class="bt1  bt" onclick="scenes_name['race_track_particle'].step();">Update step</div>
  <span class="stretch"></span>
</div>

<div id="race_track_particle_resampling" class="button_set" onclick="scenes_name['race_track_particle'].step();">
<div class="bt1  bt">Resampling</div>
  <span class="stretch"></span>
</div>

<h2 id="particle-filter">Particle filter</h2>

<p>In this section we will develop the method of particle filtering from the Bayes filter and mean field particle methods.</p>

<p>We learned, that the equations of the Bayes filter are</p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})\frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} dx_{t}</script>

<p><strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .</script>

<p>and that these formula is in general is not tractable. The particle filter is approximating the Bayes filter by approximating the current belief \(p(x_{t+1}|y_{0:t},u_{0:t})\) with an empirical measure  \(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})\).</p>

<div class="important_box">
  <h1>Empirical measure</h1>
  <p>The empirical measure of \(p(x)\) is defined as</p>

  <script type="math/tex; mode=display">p_N(x) = \sum_{i=1}^{N} \delta_{x_i}(x),</script>

  <p>where \(\delta_{x_i}(x)\) is an abbreviation of the <a href="https://en.wikipedia.org/wiki/Dirac_delta_function">Dirac delta function</a> \(\delta(x-x_i)\) and \(x_{1:N}\) are \(N\) samples from \(p(x)\).
If the number of samples goes to infinity, the empirical measure will almost surely converge to the distribution \(p(x)\). The following figure shows the distribution \(p(x)\) in red and the emprical measure in black.</p>

  <div style="text-align:center; width:100%"><div id="dirac_plot" style="width:75%;display:inline-block;"></div></div>
  <script>
function load_em_meas(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = sample_gmm(mix, gs, n_sam, dom);
	for (var i = 0; i < n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#dirac_plot", samples, [dat], dom, 0.2, false, 0.7);
}
load_em_meas();
</script>

  <p>Please be aware that the peaks representing the samples are actually have infinitely high but it not possible to draw this. However, the area under the Dirac function is finite:</p>

  <p><script type="math/tex">\int_x \delta(x)dx = 1.</script></p>
</div>

<script type="math/tex; mode=display">\int p(x_{k+1}|x'_{k}){\frac {p(y_{k}|x_{k}'){\widehat {p}}(dx'_{k}|y_{0},\cdots ,y_{k-1})}{\int p(y_{k}|x''_{k}){\widehat {p}}(dx''_{k}|y_{0},\cdots ,y_{k-1})}}=\sum _{i=1}^{N}{\frac {p(y_{k}|\xi _{k}^{i})}{\sum _{i=1}^{N}p(y_{k}|\xi _{k}^{j})}}p(x_{k+1}|\xi _{k}^{i})=:{\widehat {q}}(x_{k+1}|y_{0},\cdots ,y_{k})</script>

<script type="math/tex; mode=display">\int p(x_{k+1}|x'_{k}){\frac {p(y_{k}|x_{k}'){\widehat {p}}(dx'_{k}|y_{0},\cdots ,y_{k-1})}{\int p(y_{k}|x''_{k}){\widehat {p}}(dx''_{k}|y_{0},\cdots ,y_{k-1})}}=\sum _{i=1}^{N}{\frac {p(y_{k}|\xi _{k}^{i})}{\sum _{i=1}^{N}p(y_{k}|\xi _{k}^{j})}}p(x_{k+1}|\xi _{k}^{i})=:{\widehat {q}}(x_{k+1}|y_{0},\cdots ,y_{k})</script>

<p>Let’s start with the update step and replace the belief distribution with a emprical measure \(\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) = \frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) \). This empirical measure could look like this</p>

<div style="text-align:center; width:100%"><div id="prior_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_prior_meas(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = sample_gmm(mix, gs, n_sam, dom);
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#prior_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas();
</script>

<p>The equation of the update step becomes</p>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1})}{\int_{x_t} p(y_t|x_t)\hat{p}(x_t|y_{0:t-1},u_{0:t-1}) dx_t} .</script>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t)}{\int_{x_t} p(y_t|x_t)\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) dx_t} .</script>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\delta_{\xi_t^i}(x_t)}{\frac{1}{N}\sum_{i=1}^N \int_{x_t} p(y_t|x_t)\delta_{\xi_t^i}(x_t) dx_t} .</script>

<script type="math/tex; mode=display">\hat{q}(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)\delta_{\xi_t^i}(x_t)}{\frac{1}{N}\sum_{i=1}^N p(y_t|x_t=\xi_t^i)} .</script>

<p>In the numerator we are multiplying the emipircial measure and the likelihood pointwise, the denominator serves as a normalizer. This operation can best understood graphically</p>

<div style="text-align:center; width:100%"><div id="prior_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_prior_meas_mal(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = sample_gmm(mix, gs, n_sam, dom);
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#prior_mal_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_prior_meas_mal();
</script>

<script type="math/tex; mode=display">*</script>

<div style="text-align:center; width:100%"><div id="likelihood_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_likelihood_meas_mal(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = [];
	for (var i = 0; i < n_plot; i++) {
		var x = dom[1]*i/n_plot;	
		gdata.push({x:x, y:gmm(x, mix, gs)});
	}
	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	
	create_dirac_plot("#likelihood_mal_plot", samples, [dat], dom, 0.4, false, 0.2);
}
load_likelihood_meas_mal();
</script>

<script type="math/tex; mode=display">=</script>

<div style="text-align:center; width:100%"><div id="posterior_mal_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_posterior_meas_mal(){
	var n_sam = 50;
	var mix = [0.8,0.2];
	var gs = [[1.5,0.6],[4.0,0.6]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var gdata= [];
	var samples = [];
	var samples = sample_gmm(mix, gs, n_sam, dom);

	var sw = samples.map((e)=>{return {x:e, w:gmm(e, mix, gs)};})

	var dat = [];
	dat.gdata = gdata;
	dat.color = "red";
	create_dirac_plot("#posterior_mal_plot", sw, [dat], dom, 0.2, false, 0.2);
}
load_posterior_meas_mal();
</script>

<p>As a result we have a weighted empirical measure. This leads us to the resampling step. This can be interpreted as creating sampling from our empirical measure to obtain a empirical measure of it. 
Great! Now we have performed the update step.</p>

<p>Let’s look at the predict step and replace the posterior with our empirical measure from the update step:</p>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})\hat{p}(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})\frac{1}{N}\sum_{i=1}^N \delta_{\xi_t^i}(x_t) dx_{t}</script>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N\int_{x_{t}} p(x_{t+1}|x_{t}, u_{t}) \delta_{\xi_t^i}(x_t) dx_{t}</script>

<script type="math/tex; mode=display">\hat{q}(x_{t+1}|y_{0:t},u_{0:t}) = \frac{1}{N}\sum_{i=1}^N p(x_{t+1}|x_{t} = \xi_t^i, u_{t}).</script>

<p>This is a weighted sum of continuous distributions.</p>

<div style="text-align:center; width:100%"><div id="pred_plot" style="width:75%;display:inline-block;"></div></div>
<script>
function load_pred_meas(){
	var n_sam = 50;
	var mix = [0.2, 0.2, 0.2, 0.2];
	var gs = [[1.5,0.6], [4.0,0.3], [2.5,0.3], [3.0,0.4]];
	var dom = [0.0, 5.0];
	var n_plot = 1000;
	var samples = [];
	var datdat = [];

	var sum_data = [...Array(n_plot)].map((e,i)=>{return {x:dom[1]*i/n_plot,y:0}});

	for (var j = 0; j < mix.length; j++) {
		var dat = [];
		var gdata= [];
		dat.color = "red";
		for (var i = 0; i < n_plot; i++) {
			var x = dom[1]*i/n_plot;	
			var v = mix[j]*gaussian(x,gs[j][0],gs[j][1] );
			gdata.push({x:x, y:v});
			sum_data[i].y+=v;
		}
		dat.gdata = gdata;
		datdat.push(dat);
	}

	var dat = [];
	dat.color="blue";
	dat.gdata=sum_data;
	datdat.push(dat);



	create_dirac_plot("#pred_plot", samples, datdat, dom, 0.2, false, 0.7);
}
load_pred_meas();
</script>

<p>If we hear the continuous, our alarm bells should ring. To avoid this we estimate this distribution by an empirical distribution again. How can we sample from it? First we sample from our posterior distribution. Based on this sample we can sample from the forward distribution to obtain a sample of a new state. These samples samples will be distributed like the posterior.</p>

<p>Well, thats essentially it.</p>

<p>But one thing seems a little bit weird, we are sampling from the weighted emprical measure to obtain an emprical measure, from which we are sampling again. We can get rid of the step in the middle! We directly sample from the weighted emprical distribution to use it for calculation of the next posterior.
Now we arrived at the algorithm of the particle filter.</p>

<h2 id="unscented-kalman-filter">Unscented Kalman filter</h2>

<p>The unscented Kalman filter is another approximation of the Bayes filter with Gaussian belief states.</p>

<p>When we are doing the <strong>prediction step</strong></p>

<script type="math/tex; mode=display">p(x_{t+1}|y_{0:t},u_{0:t}) = \int_{x_{t}} p(x_{t+1}|x_{t}, u_{t})p(x_{t}|y_{0:t},u_{0:t-1}) dx_{t}</script>

<p>with</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
p(x_{t+1}|x_{t}, u_{t})  &= \mathcal{N}(x_{t+1}|f(x_t, u_t), Q_t) \\
p(y_t|x_t) &=  \mathcal{N}(y_t|C_tx_t, R_t). 
 \end{align} %]]></script>

<p>with a Gaussian belief distribution xxx we will receive a non-Gaussian function as a result. The first idea of the unscented Kalman filter is, that we want a Gaussian posterior in the end. In other words we are only intrested in the mean and variance of the true posterior. We try to approximate the real posterior with a Gaussian. In this scenario, we have two problems:</p>

<ol>
  <li>The integral is intractable, because of the non-linear sytem dynamic \(f(x_t,u_t)\).</li>
  <li>It is intractable to calculate the mean and covariance of the resulting posterior</li>
</ol>

<p>How can we solve this problem?</p>

<p>One solution is sampling. We could simply obtain the empricial distribution by sampling from our Gaussian. Then we replace our belief with the emprical distribution in the predict step. The integral will become a sum. Please be aware, that the resulting function will still be a continuous function. Nonetheless, we know that this function is a mixture of gaussians, where each component is a Gaussian with mean \(f(\xi^i_t,u_t)\) and variance \(Q_t\). When we are forming our empirical distrobution our samples are weighted by \(w^i  = \frac{1}{N}\).</p>

<p>By assuming an arbitray weighting of the samples the resulting mean is</p>

<script type="math/tex; mode=display">\hat{\mu} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}[x_{t+1}]</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i p(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \int_{x_{t+1}} \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \sum_{i=1}^N w^i\int_{x_{t+1}}  w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}dx_{t+1}</script>

<script type="math/tex; mode=display">\hat{\mu} = \sum_{i=1}^N w^i f(\xi^i_t, u_t)</script>

<p>and the variance</p>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[(x_{t+1}-\hat{\mu})(x_{t+1}-\hat{\mu})^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T - x_{t+1}\hat{\mu}^T - \hat{\mu}x_{t+1}^T + \hat{\mu}\hat{\mu}^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}x_{t+1}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[x_{t+1}\hat{\mu}^T\right] - \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}x_{t+1}^T\right] + \mathbb{E}_{\hat{p}(x_{t+1}|y_{0:t},u_{0:t})}\left[\hat{\mu}\hat{\mu}^T\right]</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}} \hat{p}(x_{t+1}|y_{0:t},u_{0:t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T - \hat{\mu}\hat{\mu}^T + \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^ip(x_{t+1}|x_{t} = \xi_t^i, u_{t}) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \int_{x_{t+1}}  \sum_{i=1}^N w^i\mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i\int_{x_{t+1}}   \mathcal{N}(x_{t+1}|f(\xi^i_t, u_t), Q_t) x_{t+1}x_{t+1}^Tdx_{t+1} - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = \sum_{i=1}^N w^i(Q_t + f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T) - \hat{\mu}\hat{\mu}^T</script>

<script type="math/tex; mode=display">\hat{\Sigma} = Q_t + \sum_{i=1}^N w^i f(\xi^i_t, u_t)f(\xi^i_t, u_t)^T - \hat{\mu}\hat{\mu}^T</script>

<p>Because of this we can find the true mean and variance of the continuous posterior. In the limit with inifinitley many samples this procedure will procduce the correct mean and variance.</p>

<p>Great, we have a nice method! But there is another problem. Sampling can be very inefficient. Maybe there is a smarter way.</p>

<p>The second big idea of the unscented Kalman filter is, that we are replacing the our belief distribution with a surrogate distribution, which is distributoin containing weighted samples.</p>

<p>We choose this surrogate distribution in such a way, that it has the same mean and variance as the Gaussian belief distribution. This distribution is not unique.</p>

<p>For our samples we use</p>

<p><script type="math/tex">\xi_t^0 = \mu_t</script>
<script type="math/tex">\xi_t^i = \mu_t + \sqrt{n+\lambda}L i=1,..,n</script></p>

<script type="math/tex; mode=display">\xi_t^i = \mu_t - \sqrt{n+\lambda}L i=n+1, ..., 2n</script>

<p>with weights</p>

<script type="math/tex; mode=display">w_c^0 = \frac{\lambda}{n + \lambda}</script>

<script type="math/tex; mode=display">w_m^0 = \frac{\lambda}{n + \lambda} + (1-\alpha^2+\beta)</script>

<script type="math/tex; mode=display">w_m^i =   \frac{\lambda}{2(n + \lambda)}</script>

<p>Why do they choose another Wc0?????????</p>

<script type="math/tex; mode=display">D_{KL}(\hat{p}(x_{t+1}|y_{0:t},u_{0:t})||\mathcal{N}(x_{t+1}|\hat{\mu},\hat{\Sigma}))</script>

<p>and the <strong>update step</strong></p>

<script type="math/tex; mode=display">p(x_t|y_{0:t},u_{0:t-1}) = \frac{p(y_t|x_t)p(x_t|y_{0:t-1},u_{0:t-1})}{p(y_t|y_{0:t-1},u_{0:t-1})} .</script>

<p><a href="https://www.freepik.com/free-vector/flat-car-collection-with-side-view_1505022.htm"></a></p>

<div id="rad_to_s" style="width:100px"></div>
<div id="div1"></div>
<div id="div2"></div>
<!-- <div id="system_dist_approx"  style="width: 600px; height: 600px;"></div> -->
<!--<div id="output_dist_approx"  style="width: 600px; height: 600px;"></div>-->


  </div><a class="u-url" href="/jekyll/update/2018/10/12/nf-particle.html" hidden></a>
</article>

      </div>
    </main>

  </body>

</html>
